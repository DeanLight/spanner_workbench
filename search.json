[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Spannerlib",
    "section": "",
    "text": "Welcome to the spannerlib project.\nThe spannerlib is a framework for building programming languages that are a combination of imperative and declarative languages. This combination is based off of derivations of the document spanner model.\nCurrently, we implement a language called spannerlog over python. spannerlog is an extension of statically types datalog which allows users to define their own ie functions which can be used to derive new structured information from relations.\nThe spannerlog repl, shown bellow is served using the jupyter magic commands\nBellow, we will show you how to install and use spannerlog through Spannerlib",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Welcome to Spannerlib",
    "section": "Installation",
    "text": "Installation\n\nUnix\nTo download and install RGXLog run the following commands in your terminal:\ngit clone https://github.com/DeanLight/spannerlib\ncd spannerlib\n\npip install -e .\ndownload corenlp to spannerlib/rgxlog/\nfrom this link\n# verify everything worked\n# first time might take a couple of minutes since run time assets are being configured\npython nbdev_test.py\n\n\ndocker\ngit clone https://github.com/DeanLight/spannerlib\ncd spannerlib\ndownload corenlp to spannerlib/rgxlog/\nfrom this link\ndocker build . -t spannerlib_image\n\n# on windows, change `pwd to current working directory`\n# to get a bash terminal to the container\ndocker run --name swc --rm -it \\\n  -v `pwd`:/spannerlib:Z \\\n  spannerlib_image bash\n\n# to run an interactive notebook on host port 8891\ndocker run --name swc --rm -it \\\n  -v `pwd`:/spannerlib:Z \\\n  -p8891:8888 \\\n  spannerlib_image jupyter notebook --no-browser --allow-root\n\n#Verify tests inside the container\npython /spannerlib/nbdev_test.py",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "index.html#getting-started---tldr",
    "href": "index.html#getting-started---tldr",
    "title": "Welcome to Spannerlib",
    "section": "Getting started - TLDR",
    "text": "Getting started - TLDR\nHere is a TLDR intro, for a more comprehensive tutorial, please see the introduction section of the tutorials.\n\nimport spannerlib\nimport pandas as pd\n# get dynamic access to the session running through the jupyter magic system\nfrom spannerlib import magic_session\nsession = magic_session\n\nGet a dataframe\n\nlecturer_df = pd.DataFrame(\n    [[\"walter\",\"chemistry\"],\n     [\"linus\", \"operating_systems\"],\n     ['rick', 'physics']\n    ],columns=[\"name\",\"course\"])\nlecturer_df\n\n\n\n\n\n\n\n\n\nname\ncourse\n\n\n\n\n0\nwalter\nchemistry\n\n\n1\nlinus\noperating_systems\n\n\n2\nrick\nphysics\n\n\n\n\n\n\n\n\nOr a CSV\n\npd.read_csv('sample_data/example_students.csv',names=[\"name\",\"course\"])\n\n\n\n\n\n\n\n\n\nname\ncourse\n\n\n\n\n0\nabigail\nchemistry\n\n\n1\nabigail\noperation systems\n\n\n2\njordan\nchemistry\n\n\n3\ngale\noperation systems\n\n\n4\nhoward\nchemistry\n\n\n5\nhoward\nphysics\n\n\n\n\n\n\n\n\nImport them to the session\n\nsession.import_rel(lecturer_df, relation_name=\"lecturer\")\nsession.import_rel(\"sample_data/enrolled.csv\", relation_name=\"enrolled\", delimiter=\",\")\n\nThey can even be documents\n\ndocuments = pd.DataFrame([\n    [\"abigail is happy, but walter did not approve\"],\n    [\"howard is happy, gale is happy, but jordan is sad\"]\n])\nsession.import_rel(documents, relation_name=\"documents\")\n\n\n%%spannerlog\n?documents(X)\n\nprinting results for query 'documents(X)':\n                         X\n---------------------------------------------------\n   abigail is happy, but walter did not approve\n howard is happy, gale is happy, but jordan is sad\n\n\n\nDefine your own IE functions to extract information from relations\n\n# the function itself, writing it as a python generator makes your data processing lazy\ndef get_happy(text):\n    \"\"\"\n    get the names of people who are happy in `text`\n    \"\"\"\n    import re\n\n    compiled_rgx = re.compile(\"(\\w+) is happy\")\n    num_groups = compiled_rgx.groups\n    for match in re.finditer(compiled_rgx, text):\n        if num_groups == 0:\n            matched_strings = [match.group()]\n        else:\n            matched_strings = [group for group in match.groups()]\n        yield matched_strings\n\n# register the ie function with the session\nfrom spannerlib.primitive_types import DataTypes\nsession.register(ie_function=get_happy,\n                       ie_function_name = \"get_happy\",\n                       in_rel=[DataTypes.string],\n                       out_rel=[DataTypes.string])\n\nrgxlog supports relations over the following primitive types * strings * spans * integers\nWrite a rgxlog program (like datalog but you can use your own ie functions)\n\n%%spannerlog\n# you can also define data inline via a statically typed variant of datalog syntax\nnew sad_lecturers(str)\nsad_lecturers(\"walter\")\nsad_lecturers(\"linus\")\n\n# and include primitive variable\ngpa_doc = \"abigail 100 jordan 80 gale 79 howard 60\"\n\n# define datalog rules\nenrolled_in_chemistry(X) &lt;- enrolled(X, \"chemistry\")\nenrolled_in_physics_and_chemistry(X) &lt;- enrolled_in_chemistry(X), enrolled(X, \"physics\")\n\n# and query them inline (to print to screen)\n# ?enrolled_in_chemistry(\"jordan\") # returns empty tuple ()\n# ?enrolled_in_chemistry(\"gale\") # returns nothing\n# ?enrolled_in_chemistry(X) # returns \"abigail\", \"jordan\" and \"howard\"\n# ?enrolled_in_physics_and_chemistry(X) # returns \"howard\"\n\nlecturer_of(X,Z) &lt;- lecturer(X,Y), enrolled(Z,Y)\n\n# use ie functions in body clauses to extract structured data from unstructured data\n\n# standard ie functions like regex are already registered\nstudent_gpas(Student, Grade) &lt;- py_rgx_string(gpa_doc, \"(\\w+).*?(\\d+)\")-&gt;(Student, Grade)\n\n# and you can use your defined functions as well\nhappy_students_with_sad_lecturers_and_their_gpas(Student, Grade, Lecturer) &lt;- \\\n    documents(Doc), \\\n    get_happy(Doc)-&gt;(Student), \\\n    sad_lecturers(Lecturer), \\\n    lecturer_of(Lecturer,Student), \\\n    student_gpas(Student, Grade)\n\n\n# TODO\n# change lib name to spannerlib\n# check if you can inline the tests\n# start design for the aggregation functions\n# change functions to new_ie and new_agg (or register instaed of new)\n\nAnd query it\n\n%spannerlog ?happy_students_with_sad_lecturers_and_their_gpas(Stu,Gpa,Lec)\n\nprinting results for query 'happy_students_with_sad_lecturers_and_their_gpas(Stu, Gpa, Lec)':\n   Stu   |   Gpa |  Lec\n---------+-------+--------\n abigail |   100 | linus\n  gale   |    79 | linus\n howard  |    60 | walter\n\n\n\nYou can also get query results as Dataframes for downstream processing\n\ndf = magic_session.export(\n    \"?happy_students_with_sad_lecturers_and_their_gpas(Stu,Gpa,Lec)\")\ndf\n\n\n\n\n\n\n\n\n\nStu\nGpa\nLec\n\n\n\n\n0\nabigail\n100\nlinus\n\n\n1\ngale\n79\nlinus\n\n\n2\nhoward\n60\nwalter",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Welcome to Spannerlib",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSources of inspiration\n\nLogicblox repl\nLogicblox manual\nLogiQL is the language implemented in logicblox and is a Dialect of Datalog\n\n\n\nRelevant papers\n\nspannerlog\nRecursive RGXLog",
    "crumbs": [
      "Welcome to Spannerlib"
    ]
  },
  {
    "objectID": "live_repl.html",
    "href": "live_repl.html",
    "title": "Try it yourself!",
    "section": "",
    "text": "You don’t need to install anything to start using spannerlog; we offer a live REPL right here on this webpage! This feature allows you to interactively run code and see the results instantly. This is a great way to try out the library and get a feel for what it’s like to work with spannerlog.\n\n\nHere are a few example code snippets that you can run in the live REPL to get started:\n%%spannerlog\nenrolled = \"dave chemistry dave biology rem biology ram biology emilia physics roswaal chemistry roswaal biology roswaal physics\"\ngrades = \"dave 80 rem 66 ram 66 roswaal 100 emilia 88\"\n\nenrolled_in(Student, Course) &lt;- py_rgx_string(enrolled, \"(\\w+).*?(\\w+)\")-&gt;(Student, Course)\nstudent_grade(Student, Grade) &lt;- py_rgx_string(grades, \"(\\w+).*?(\\d+)\") -&gt; (Student, Grade)\ninteresting_student(X) &lt;- enrolled_in(X, \"biology\"), enrolled_in(X, \"chemistry\"), student_grade(X, \"80\")\n?interesting_student(X)\n%%spannerlog\nnew parent(str, str)\nparent(\"Liam\", \"Noah\")\nparent(\"Noah\", \"Oliver\")\n\nancestor(X,Y) &lt;- parent(X,Y)\nancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n\n?ancestor(Ancestor, Descendant)"
  },
  {
    "objectID": "live_repl.html#live-repl-for-spannerlog",
    "href": "live_repl.html#live-repl-for-spannerlog",
    "title": "Try it yourself!",
    "section": "",
    "text": "You don’t need to install anything to start using spannerlog; we offer a live REPL right here on this webpage! This feature allows you to interactively run code and see the results instantly. This is a great way to try out the library and get a feel for what it’s like to work with spannerlog.\n\n\nHere are a few example code snippets that you can run in the live REPL to get started:\n%%spannerlog\nenrolled = \"dave chemistry dave biology rem biology ram biology emilia physics roswaal chemistry roswaal biology roswaal physics\"\ngrades = \"dave 80 rem 66 ram 66 roswaal 100 emilia 88\"\n\nenrolled_in(Student, Course) &lt;- py_rgx_string(enrolled, \"(\\w+).*?(\\w+)\")-&gt;(Student, Course)\nstudent_grade(Student, Grade) &lt;- py_rgx_string(grades, \"(\\w+).*?(\\d+)\") -&gt; (Student, Grade)\ninteresting_student(X) &lt;- enrolled_in(X, \"biology\"), enrolled_in(X, \"chemistry\"), student_grade(X, \"80\")\n?interesting_student(X)\n%%spannerlog\nnew parent(str, str)\nparent(\"Liam\", \"Noah\")\nparent(\"Noah\", \"Oliver\")\n\nancestor(X,Y) &lt;- parent(X,Y)\nancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n\n?ancestor(Ancestor, Descendant)"
  },
  {
    "objectID": "expected_grammar.html",
    "href": "expected_grammar.html",
    "title": "Expected Grammar",
    "section": "",
    "text": "When dealing with spannerlog’s Abstract Syntax Tree (AST), it’s crucial to ensure that the node structure conforms to the expected grammar. The spannerlog_expected_children_names_lists data structure maps every node type in the AST to its expected list(s) of children node names. Below is a comprehensive guide on how to safely modify the spannerlog grammar and update the code to accommodate the changes.\n\n\nFor each node in the AST, spannerlog_expected_children_names_lists contains a list of its expected children node names. Each node type maps to a list of lists, where each internal list represents a valid set of children node names for that particular node type.\n\n\n\n\n\n\nNote\n\n\n\nSome nodes in spannerlog can have variable-length children lists (e.g., term_list). Such nodes are not included in spannerlog_expected_children_names_lists.\n\n\n\n\n\n\n\n\nBefore any modifications, assert that each AST node retains its original, expected structure using spannerlog_expected_children_names_lists.\n# Example usage\nlark_passes_utils.assert_expected_node_structure(node, spannerlog_expected_children_names_lists)\n\n\n\nGo ahead and make the changes to the spannerlog grammar file.\n\n\n\nExecute a spannerlog program that uses a variety of different statements to ensure broad test coverage. Observe where your program crashes due to failed node structure assertions. Modify the code to work with the new grammar and temporarily comment out the node structure assertion(s).\nRepeat this step until no crashes occur.\n# Temporarily comment this line\n# lark_passes_utils.assert_expected_node_structure(node, spannerlog_expected_children_names_lists)\n\n\n\nOnce you are sure the program doesn’t crash with the new grammar, uncomment the node structure assertion(s).\n# Uncomment this line\nlark_passes_utils.assert_expected_node_structure(node, spannerlog_expected_children_names_lists)\n\n\n\nFinally, update the spannerlog_expected_children_names_lists data structure to reflect your new grammar.\n\nBy following this strategy, you ensure that the new grammar is functional and that the code that interacts with the AST is updated to accommodate the changes.",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Expected Grammar"
    ]
  },
  {
    "objectID": "expected_grammar.html#expected-children-structure-for-ast-nodes",
    "href": "expected_grammar.html#expected-children-structure-for-ast-nodes",
    "title": "Expected Grammar",
    "section": "",
    "text": "For each node in the AST, spannerlog_expected_children_names_lists contains a list of its expected children node names. Each node type maps to a list of lists, where each internal list represents a valid set of children node names for that particular node type.\n\n\n\n\n\n\nNote\n\n\n\nSome nodes in spannerlog can have variable-length children lists (e.g., term_list). Such nodes are not included in spannerlog_expected_children_names_lists.",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Expected Grammar"
    ]
  },
  {
    "objectID": "expected_grammar.html#strategy-for-modifying-spannerlog-grammar",
    "href": "expected_grammar.html#strategy-for-modifying-spannerlog-grammar",
    "title": "Expected Grammar",
    "section": "",
    "text": "Before any modifications, assert that each AST node retains its original, expected structure using spannerlog_expected_children_names_lists.\n# Example usage\nlark_passes_utils.assert_expected_node_structure(node, spannerlog_expected_children_names_lists)\n\n\n\nGo ahead and make the changes to the spannerlog grammar file.\n\n\n\nExecute a spannerlog program that uses a variety of different statements to ensure broad test coverage. Observe where your program crashes due to failed node structure assertions. Modify the code to work with the new grammar and temporarily comment out the node structure assertion(s).\nRepeat this step until no crashes occur.\n# Temporarily comment this line\n# lark_passes_utils.assert_expected_node_structure(node, spannerlog_expected_children_names_lists)\n\n\n\nOnce you are sure the program doesn’t crash with the new grammar, uncomment the node structure assertion(s).\n# Uncomment this line\nlark_passes_utils.assert_expected_node_structure(node, spannerlog_expected_children_names_lists)\n\n\n\nFinally, update the spannerlog_expected_children_names_lists data structure to reflect your new grammar.\n\nBy following this strategy, you ensure that the new grammar is functional and that the code that interacts with the AST is updated to accommodate the changes.",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Expected Grammar"
    ]
  },
  {
    "objectID": "tests/05l_test_export_import.html",
    "href": "tests/05l_test_export_import.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import tempfile\nfrom pathlib import Path\n\nfrom pandas import DataFrame\n \nfrom spannerlib.primitive_types import Span\nfrom spannerlib.execution import FREE_VAR_PREFIX\nfrom spannerlib.session import Session\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import (run_test, is_equal_stripped_sorted_tables, is_equal_dataframes_ignore_order, run_commands_into_csv_test, TEMP_FILE_NAME)\n\n\nim_ex_session = Session()\n\n\ndef test_import_csv1() -&gt; None:\n    # im_ex_session = Session()\n    example_relation = (\n        '\"aoi\";[0,3);8\\n'\n        '\"aoi\";[1,2);16\\n'\n        '\"ano sora\";[42,69);24\\n')\n\n    expected_result_string = f\"\"\"{QUERY_RESULT_PREFIX}'csv_rel(X, Y, Z)':\n                                        X     |    Y     |   Z\n                                    ----------+----------+-----\n                                     ano sora | [42, 69) |  24\n                                       aoi    |  [1, 2)  |  16\n                                       aoi    |  [0, 3)  |   8\n                                    \"\"\"\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        example_relation_csv = Path(temp_dir) / TEMP_FILE_NAME\n        with open(example_relation_csv, \"w\") as f:\n            f.write(example_relation)\n\n        im_ex_session.import_rel(example_relation_csv, relation_name=\"csv_rel\")\n        query = \"?csv_rel(X,Y,Z)\"\n        run_test(query, expected_result_string, session=im_ex_session)\n\ntest_import_csv1()\n\nprinting results for query 'csv_rel(X, Y, Z)':\n    X     |    Y     |   Z\n----------+----------+-----\n   aoi    |  [0, 3)  |   8\n   aoi    |  [1, 2)  |  16\n ano sora | [42, 69) |  24\n\n\n\n\ndef test_import_csv2() -&gt; None:\n    example_relation_two = (\n        \"a\\n\"\n        \"b\\n\"\n        \"c\\n\")\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        example_relation2_csv = Path(temp_dir) / TEMP_FILE_NAME\n        with open(example_relation2_csv, \"w\") as f:\n            f.write(example_relation_two)\n\n        im_ex_session.import_rel(example_relation2_csv, relation_name=\"csv_rel2\")\n\n        expected_result_string = f\"\"\"{QUERY_RESULT_PREFIX}'csv_rel2(X)':\n                                      X\n                                    -----\n                                      c\n                                      b\n                                      a\n                                    \"\"\"\n\n        query = \"?csv_rel2(X)\"\n        run_test(query, expected_result_string, session=im_ex_session)\n\ntest_import_csv2()\n\nprinting results for query 'csv_rel2(X)':\n  X\n-----\n  a\n  b\n  c\n\n\n\n\nfrom spannerlib.session import _infer_relation_type,_verify_relation_types\n\n\ndf = DataFrame([[\"a\", \"[1,2)\"], [\"b\", Span(6, 8)], [\"c\", \"[2,10)\"]], columns=[\"str\", \"span\"])\n\ndf.values.tolist()\n\n[['a', '[1,2)'], ['b', [6, 8)], ['c', '[2,10)']]\n\n\n\nl = df.values.tolist()[1]\nl\n\n['b', [6, 8)]\n\n\n\nisinstance(l[1],Span)\n\nTrue\n\n\n\n_infer_relation_type(l[1:])\n\n[&lt;DataTypes.span: 1&gt;]\n\n\n\nfor l in df.values.tolist():\n    print(l)\n    print(_infer_relation_type(l))\n\n['a', '[1,2)']\n[&lt;DataTypes.string: 0&gt;, &lt;DataTypes.span: 1&gt;]\n['b', [6, 8)]\n[&lt;DataTypes.string: 0&gt;, &lt;DataTypes.span: 1&gt;]\n['c', '[2,10)']\n[&lt;DataTypes.string: 0&gt;, &lt;DataTypes.span: 1&gt;]\n\n\n\ndf = DataFrame([[\"a\", \"[1,2)\"], [\"b\", Span(6, 8)], [\"c\", \"[2,10)\"]], columns=[\"str\", \"span\"])\nquery = \"?df_rel(X,Y)\"\nexpected_result_string = f\"\"\"{QUERY_RESULT_PREFIX}'df_rel(X, Y)':\n          X  |    Y\n        -----+---------\n          c  | [2, 10)\n          b  | [6, 8)\n          a  | [1, 2)\"\"\"\n\n\nim_ex_session.import_rel(df, relation_name=\"df_rel\")\n_ = run_test(query, expected_result_string, session=im_ex_session)\n\nprinting results for query 'df_rel(X, Y)':\n  X  |    Y\n-----+---------\n  a  | [1, 2)\n  b  | [6, 8)\n  c  | [2, 10)\n\n\n\n\ndef test_commands_into_csv_basic() -&gt; None:\n    commands = \"\"\"new basic_rel(str)\n            basic_rel(\"stardew\")\n            basic_rel(\"valley\")\"\"\"\n\n    expected_rel = (\n        \"X\\n\"\n        \"valley\\n\"\n        \"stardew\\n\")\n\n    query_for_csv = '?basic_rel(X)'\n\n    run_commands_into_csv_test(expected_rel, im_ex_session, commands, query_for_csv)\n\ntest_commands_into_csv_basic()\n\n\ndef test_commands_into_csv_long() -&gt; None:\n    commands = \"\"\"new longrel(str,span,int)\n            longrel(\"ano sora\",[42, 69),24)\n            longrel(\"aoi\",[1, 2),16)\n            longrel(\"aoi\",[0, 3),8)\"\"\"\n\n    expected_longrel = (\n        \"X;Y;Z\\n\"\n        \"aoi;[0, 3);8\\n\"\n        \"aoi;[1, 2);16\\n\"\n        \"ano sora;[42, 69);24\\n\")\n\n    query_for_csv = \"?longrel(X,Y,Z)\"\n\n    run_commands_into_csv_test(expected_longrel, im_ex_session, commands, query_for_csv)\n\ntest_commands_into_csv_long()\n\n\ndef test_export_relation_into_csv() -&gt; None:\n    relation_name = \"hotdoge\"\n    commands = f\"\"\"\n            new {relation_name}(str, int)\n            {relation_name}(\"wow\",42)\n            {relation_name}(\"such summer\", 420)\n            {relation_name}(\"much heat\", 42)\"\"\"\n\n    expected_export_rel = f\"\"\"\n        {FREE_VAR_PREFIX}0:{FREE_VAR_PREFIX}1\n        wow:42\n        such summer:420\n        much heat:42\"\"\"\n\n    im_ex_session.run_commands(commands, print_results=False)\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        temp_csv = Path(temp_dir) / TEMP_FILE_NAME\n        im_ex_session.export(relation_name=relation_name,csv_path=temp_csv, delimiter=\":\")\n        assert Path(temp_csv).is_file(), \"file was not created\"\n\n        with open(temp_csv) as f_temp:\n            assert is_equal_stripped_sorted_tables(f_temp.read(), expected_export_rel), \"file was not written properly\"\n\ntest_export_relation_into_csv()\n\n\ndef test_commands_into_df() -&gt; None:\n    test_df = DataFrame([\"king\", \"jump\"], columns=[\"X\"])\n    # create new relation\n    commands = \"\"\"\n        new df_query_rel(str)\n        df_query_rel(\"jump\")\n        df_query_rel(\"king\")\"\"\"\n\n    im_ex_session.run_commands(commands, print_results=False)\n\n    query_for_df = \"?df_query_rel(X)\"\n\n    # send commands into df and compare\n    temp_df = im_ex_session.send_commands_result_into_df(query_for_df)\n    assert is_equal_dataframes_ignore_order(temp_df, test_df), \"the dataframes are not equal\"\n\ntest_commands_into_df()\n\n\ndef test_export_relation_into_df() -&gt; None:\n    column_names = [f\"{FREE_VAR_PREFIX}0\", f\"{FREE_VAR_PREFIX}1\"]\n\n    relation_name = \"export_df_rel\"\n    commands = f\"\"\"\n        new {relation_name}(span, str)\n        {relation_name}([1,3), \"aa\")\n        {relation_name}([2,4), \"bb\")\"\"\"\n\n    expected_df = DataFrame([[Span(1, 3), \"aa\"], [Span(2, 4), \"bb\"]], columns=column_names)\n\n    im_ex_session.run_commands(commands)\n    result_df = im_ex_session.export(relation_name = relation_name)\n\n    assert is_equal_dataframes_ignore_order(result_df, expected_df)\n\ntest_export_relation_into_df()"
  },
  {
    "objectID": "tests/05m_test_magic_client.html",
    "href": "tests/05m_test_magic_client.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\n\n\ndef test_magic_session_basic() -&gt; None:\n    commands = \"\"\"\n        new uncle(str, str)\n        uncle(\"bob\", \"greg\")\n        ?uncle(X,Y)\n        \"\"\"\n\n    expected_result_intro = f\"\"\"{QUERY_RESULT_PREFIX}'uncle(X, Y)':\n              X  |  Y\n            -----+------\n             bob | greg\n            \"\"\"\n\n    run_test(commands, expected_result_intro)\n\ntest_magic_session_basic()\n\nprinting results for query 'uncle(X, Y)':\n  X  |  Y\n-----+------\n bob | greg"
  },
  {
    "objectID": "tests/05d_test_non_default_ie_functions.html",
    "href": "tests/05d_test_non_default_ie_functions.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from typing import Tuple, Iterable\n\nfrom spannerlib.primitive_types import DataTypes, Span\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\n\n\ndef test_range_int_no_tuple() -&gt; None:\n    def yield_range_int_no_tuple(num: int) -&gt; Iterable[int]:\n        for i in range(num):\n            yield i\n\n    yield_range_int_no_tuple_in_types = [DataTypes.integer]\n    yield_range_int_no_tuple_out_types = [DataTypes.integer]\n    yield_range_dict = {\"ie_function\": yield_range_int_no_tuple,\n                        \"ie_function_name\": \"yield_range_int_no_tuple\",\n                        \"in_rel\": yield_range_int_no_tuple_in_types,\n                        \"out_rel\": yield_range_int_no_tuple_out_types}\n\n    commands = \"\"\"\n        test_range_int_no_tuple(X) &lt;- yield_range_int_no_tuple(5) -&gt; (X)\n        ?test_range_int_no_tuple(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'test_range_int_no_tuple(X)':\n                            X\n                            -----\n                            4\n                            3\n                            2\n                            1\n                            0\"\"\"\n\n    run_test(commands, expected_result, functions_to_import=[yield_range_dict])\n\ntest_range_int_no_tuple()\n\nprinting results for query 'test_range_int_no_tuple(X)':\n   X\n-----\n   0\n   1\n   2\n   3\n   4\n\n\n\n\ndef test_range_span_no_tuple() -&gt; None:\n    def yield_range_span_no_tuple(num: int) -&gt; Iterable[Span]:\n        for i in range(num):\n            yield Span(i, i)\n\n    yield_range_span_no_tuple_in_types = [DataTypes.integer]\n    yield_range_span_no_tuple_out_types = [DataTypes.span]\n    yield_range_span_dict = {\"ie_function\": yield_range_span_no_tuple,\n                             \"ie_function_name\": \"yield_range_span_no_tuple\",\n                             \"in_rel\": yield_range_span_no_tuple_in_types,\n                             \"out_rel\": yield_range_span_no_tuple_out_types}\n\n    commands = \"\"\"\n        test_range_span_no_tuple(X) &lt;- yield_range_span_no_tuple(5) -&gt; (X)\n        ?test_range_span_no_tuple(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'test_range_span_no_tuple(X)':\n                           X\n                        --------\n                         [4, 4)\n                         [3, 3)\n                         [2, 2)\n                         [1, 1)\n                         [0, 0)\"\"\"\n\n    run_test(commands, expected_result, functions_to_import=[yield_range_span_dict])\n\ntest_range_span_no_tuple()\n\nprinting results for query 'test_range_span_no_tuple(X)':\n   X\n--------\n [0, 0)\n [1, 1)\n [2, 2)\n [3, 3)\n [4, 4)\n\n\n\n\ndef test_range_str_no_tuple() -&gt; None:\n    def yield_range_str_no_tuple(num: int) -&gt; Iterable[str]:\n        for i in range(num):\n            yield \"string\" + str(i)\n\n    yield_range_str_no_tuple_in_types = [DataTypes.integer]\n    yield_range_str_no_tuple_out_types = [DataTypes.string]\n    yield_range_str_dict = {\"ie_function\": yield_range_str_no_tuple,\n                            \"ie_function_name\": \"yield_range_str_no_tuple\",\n                            \"in_rel\": yield_range_str_no_tuple_in_types,\n                            \"out_rel\": yield_range_str_no_tuple_out_types}\n\n    commands = \"\"\"\n        test_range_str_no_tuple(X) &lt;- yield_range_str_no_tuple(5) -&gt; (X)\n        ?test_range_str_no_tuple(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'test_range_str_no_tuple(X)':\n                            X\n                        ---------\n                         string4\n                         string3\n                         string2\n                         string1\n                         string0\"\"\"\n\n    run_test(commands, expected_result, functions_to_import=[yield_range_str_dict])\n\ntest_range_str_no_tuple()\n\nprinting results for query 'test_range_str_no_tuple(X)':\n    X\n---------\n string0\n string1\n string2\n string3\n string4\n\n\n\n\ndef test_range_int_with_tuple() -&gt; None:\n    \"\"\"\n    in this test, a tuple of an integer is treated as an integer\n    @return:\n    \"\"\"\n\n    def yield_range_int_with_tuple(num: int) -&gt; Iterable[Tuple]:\n        for i in range(num):\n            yield i,\n\n    yield_range_int_with_tuple_in_types = [DataTypes.integer]\n    yield_range_int_with_tuple_out_types = [DataTypes.integer]\n    yield_range_dict = {\"ie_function\": yield_range_int_with_tuple,\n                        \"ie_function_name\": \"yield_range_int_with_tuple\",\n                        \"in_rel\": yield_range_int_with_tuple_in_types,\n                        \"out_rel\": yield_range_int_with_tuple_out_types}\n\n    commands = \"\"\"\n        test_range_int_with_tuple(X) &lt;- yield_range_int_with_tuple(5) -&gt; (X)\n        ?test_range_int_with_tuple(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'test_range_int_with_tuple(X)':\n                            X\n                            -----\n                            4\n                            3\n                            2\n                            1\n                            0\"\"\"\n\n    run_test(commands, expected_result, functions_to_import=[yield_range_dict])\n\ntest_range_int_with_tuple()\n\nprinting results for query 'test_range_int_with_tuple(X)':\n   X\n-----\n   0\n   1\n   2\n   3\n   4"
  },
  {
    "objectID": "tests/05h_test_introduction_tutorial.html",
    "href": "tests/05h_test_introduction_tutorial.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from typing import Iterable, Any, Tuple\nfrom spannerlib.primitive_types import DataTypes\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\n\n\ndef test_strings_query() -&gt; None:\n    commands = \"\"\"\n    new uncle(str, str)\n    uncle(\"bob\", \"greg\")\n    ?uncle(X,Y)\n    \"\"\"\n\n    expected_result_intro = f\"\"\"{QUERY_RESULT_PREFIX}'uncle(X, Y)':\n          X  |  Y\n        -----+------\n         bob | greg\n        \"\"\"\n\n    run_test(commands, expected_result_intro)\n\ntest_strings_query()\n\nprinting results for query 'uncle(X, Y)':\n  X  |  Y\n-----+------\n bob | greg\n\n\n\n\ndef test_basic_queries() -&gt; None:\n    commands = '''\n            new lecturer(str, str)\n            lecturer(\"walter\", \"chemistry\")\n            lecturer(\"linus\", \"operation systems\")\n            lecturer(\"rick\", \"physics\")\n\n            new enrolled(str, str)\n            enrolled(\"abigail\", \"chemistry\")\n            enrolled(\"abigail\", \"operation systems\")\n            enrolled(\"jordan\", \"chemistry\")\n            enrolled(\"gale\", \"operation systems\")\n            enrolled(\"howard\", \"chemistry\")\n            enrolled(\"howard\", \"physics\")\n\n            enrolled_in_chemistry(X) &lt;- enrolled(X, \"chemistry\")\n            ?enrolled_in_chemistry(\"jordan\")\n            ?enrolled_in_chemistry(\"gale\")\n            ?enrolled_in_chemistry(X)\n\n            enrolled_in_physics_and_chemistry(X) &lt;- enrolled(X, \"chemistry\"), enrolled(X, \"physics\")\n            ?enrolled_in_physics_and_chemistry(X)\n\n            lecturer_of(X, Z) &lt;- lecturer(X, Y), enrolled(Z, Y)\n            ?lecturer_of(X, \"abigail\")\n            '''\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'enrolled_in_chemistry(\"jordan\")':\n        [()]\n\n        {QUERY_RESULT_PREFIX}'enrolled_in_chemistry(\"gale\")':\n        []\n\n        {QUERY_RESULT_PREFIX}'enrolled_in_chemistry(X)':\n            X\n        ---------\n         howard\n         jordan\n         abigail\n\n        {QUERY_RESULT_PREFIX}'enrolled_in_physics_and_chemistry(X)':\n           X\n        --------\n         howard\n\n        {QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n           X\n        --------\n         linus\n         walter\n        \"\"\"\n\n    session = run_test(commands, expected_result)\n\n    commands2 = (r\"\"\"gpa_str = \"abigail 100 jordan 80 gale 79 howard 60\"\n                gpa_of_chemistry_students(Student, Grade) &lt;- py_rgx_string(gpa_str, \"(\\w+).*?(\\d+)\")\"\"\"\n                 r\"\"\"-&gt;(Student, Grade), enrolled_in_chemistry(Student)\n               ?gpa_of_chemistry_students(X, \"100\")\"\"\")\n\n    expected_result2 = f\"\"\"{QUERY_RESULT_PREFIX}'gpa_of_chemistry_students(X, \"100\")':\n            X\n        ---------\n         abigail\n        \"\"\"\n\n    run_test(commands2, expected_result2, session=session)\n\ntest_basic_queries()\n\nprinting results for query 'enrolled_in_chemistry(\"jordan\")':\n[()]\n\nprinting results for query 'enrolled_in_chemistry(\"gale\")':\n[]\n\nprinting results for query 'enrolled_in_chemistry(X)':\n    X\n---------\n abigail\n jordan\n howard\n\nprinting results for query 'enrolled_in_physics_and_chemistry(X)':\n   X\n--------\n howard\n\nprinting results for query 'lecturer_of(X, \"abigail\")':\n   X\n--------\n walter\n linus\n\nprinting results for query 'gpa_of_chemistry_students(X, \"100\")':\n    X\n---------\n abigail\n\n\n\n\ndef test_json_path() -&gt; None:\n    commands = \"\"\"\n                jsonpath_simple_1 = \"foo[*].baz\"\n                json_ds_simple_1  = \"{'foo': [{'baz': 1}, {'baz': 2}]}\"\n                simple_1(X) &lt;- JsonPath(json_ds_simple_1, jsonpath_simple_1) -&gt; (X)\n                ?simple_1(X)\n\n                jsonpath_simple_2 = \"a.*.b.`parent`.c\"\n                json_ds_simple_2 = \"{'a': {'x': {'b': 1, 'c': 'number one'}, 'y': {'b': 2, 'c': 'number two'}}}\"\n\n                simple_2(X) &lt;- JsonPath(json_ds_simple_2, jsonpath_simple_2) -&gt; (X)\n                ?simple_2(X)\n\n                json_ds_advanced  = \"{'foo': [{'baz': 1}, {'baz': {'foo': [{'baz': 1}, {'baz': 2}]}}]}\"\n                advanced(X) &lt;- JsonPath(json_ds_advanced, jsonpath_simple_1) -&gt; (X)\n                ?advanced(X)\n            \"\"\"\n\n    expected_result = (\n        f\"\"\"{QUERY_RESULT_PREFIX}'simple_1(X)':\n           X\n        -----\n           2\n           1\n\n        {QUERY_RESULT_PREFIX}'simple_2(X)':\n             X\n        ------------\n         number two\n         number one\n\n        {QUERY_RESULT_PREFIX}'advanced(X)':\"\"\"\n        \"\"\"\n                             X\n            -----------------------------------\n             {'foo': [{'baz': 1}, {'baz': 2}]}\n                             1\n        \"\"\")\n\n    run_test(commands, expected_result)\n\ntest_json_path()\n\nprinting results for query 'simple_1(X)':\n   X\n-----\n   1\n   2\n\nprinting results for query 'simple_2(X)':\n     X\n------------\n number one\n number two\n\nprinting results for query 'advanced(X)':\n                 X\n-----------------------------------\n                 1\n {'foo': [{'baz': 1}, {'baz': 2}]}\n\n\n\n\ndef test_remove_rule() -&gt; None:\n    commands = \"\"\"\n           new parent(str, str)\n           new grandparent(str, str)\n           parent(\"Liam\", \"Noah\")\n           parent(\"Noah\", \"Oliver\")\n           parent(\"James\", \"Lucas\")\n           parent(\"Noah\", \"Benjamin\")\n           parent(\"Benjamin\", \"Mason\")\n           grandparent(\"Tom\", \"Avi\")\n           ancestor(X,Y) &lt;- parent(X,Y)\n           ancestor(X,Y) &lt;- grandparent(X,Y)\n           ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n\n           tmp(X, Y) &lt;- ancestor(X,Y)\n           tmp(X, Y) &lt;- parent(X,Y)\n           \"\"\"\n\n    session = run_test(commands)\n\n    session.remove_rule(\"ancestor(X, Y) &lt;- parent(X, Y)\")\n    commands = \"\"\"\n            ?ancestor(X, Y)\n            ?tmp(X, Y)\n          \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'ancestor(X, Y)':\n              X  |  Y\n            -----+-----\n             Tom | Avi\n\n            {QUERY_RESULT_PREFIX}'tmp(X, Y)':\n                X     |    Y\n            ----------+----------\n             Benjamin |  Mason\n               Noah   | Benjamin\n              James   |  Lucas\n               Noah   |  Oliver\n               Liam   |   Noah\n               Tom    |   Avi\n            \"\"\"\n\n    run_test(commands, expected_result, session=session)\n\ntest_remove_rule()\n\nprinting results for query 'ancestor(X, Y)':\n  X  |  Y\n-----+-----\n Tom | Avi\n\nprinting results for query 'tmp(X, Y)':\n    X     |    Y\n----------+----------\n Benjamin |  Mason\n  James   |  Lucas\n   Liam   |   Noah\n   Noah   | Benjamin\n   Noah   |  Oliver\n   Tom    |   Avi\n\n\n\n\ndef test_issue_80_len() -&gt; None:\n    def length(string: str) -&gt; Iterable[int]:\n        # here we append the input to the output inside the ie function!\n        yield len(string)\n\n    length_dict = dict(ie_function=length,\n                       ie_function_name='Length',\n                       in_rel=[DataTypes.string],\n                       out_rel=[DataTypes.integer])\n\n    commands = \"\"\"new string(str)\n            string(\"a\")\n            string(\"d\")\n            string(\"a\")\n            string(\"ab\")\n            string(\"abc\")\n            string(\"abcd\")\n\n            string_length(Str, Len) &lt;- string(Str), Length(Str) -&gt; (Len)\n            ?string_length(Str, Len)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'string_length(Str, Len)':\n          Str  |   Len\n        -------+-------\n           a   |     1\n           d   |     1\n          ab   |     2\n          abc  |     3\n         abcd  |     4\n        \"\"\"\n\n    run_test(commands, expected_result, [length_dict])\n\ntest_issue_80_len()\n\nprinting results for query 'string_length(Str, Len)':\n  Str  |   Len\n-------+-------\n   a   |     1\n   d   |     1\n  ab   |     2\n  abc  |     3\n abcd  |     4\n\n\n\n\ndef test_issue_80_1() -&gt; None:\n    def which_century(year) -&gt; Iterable[int]:\n        yield int(year / 100) + 1\n\n    in_out_types = [DataTypes.integer]\n\n    which_century_dict = dict(ie_function=which_century,\n                              ie_function_name='which_century',\n                              in_rel=in_out_types,\n                              out_rel=in_out_types)\n\n    def which_era(cet) -&gt; Iterable[str]:\n        if 1 &lt;= cet &lt; 4:\n            yield \"Targerian Regime\"\n        elif 4 &lt;= cet &lt; 8:\n            yield \"Lanister Regime\"\n        elif 8 &lt;= cet &lt; 12:\n            yield \"Stark Regime\"\n        elif 12 &lt;= cet &lt; 16:\n            yield \"Barathion Regime\"\n        elif cet &gt;= 16:\n            yield \"Long Winter\"\n\n    which_era_dict = dict(ie_function=which_era,\n                          ie_function_name='which_era',\n                          in_rel=[DataTypes.integer],\n                          out_rel=[DataTypes.string])\n\n    commands = \"\"\"new event(str, int)\n                        event(\"First Dragon\", 250)\n                        event(\"Mad king\", 390)\n                        event(\"Winter came\", 1750)\n                        event(\"Hodor\", 999)\n                        event(\"Joffery died\", 799)\n                        \n                        new important_year(int)\n                        important_year(999)\n                        important_year(1750)\n                        important_year(250)\n                        \n                        \n                        important_events(EVE, Y) &lt;- event(EVE, Y), important_year(Y)\n                        \n                        important_events_per_cet(EVE, CET) &lt;- important_events(EVE, Y), which_century(Y) -&gt; (CET)\n                        ?important_events_per_cet(EVE, CET)\n            \"\"\"\n    commands2 = \"\"\"\n                        important_events_per_era(EVE, ERA) &lt;- important_events_per_cet(EVE, CET), which_era(CET) -&gt; (ERA)\n                        ?important_events_per_era(EVE, ERA)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'important_events_per_cet(EVE, CET)':\n                         EVE      |   CET\n                    --------------+-------\n                     First Dragon |     3\n                     Winter came  |    18\n                        Hodor     |    10\n         \"\"\"\n\n    expected_result2 = f\"\"\"{QUERY_RESULT_PREFIX}'important_events_per_era(EVE, ERA)':\n                         EVE      |       ERA\n                    --------------+------------------\n                     First Dragon | Targerian Regime\n                     Winter came  |   Long Winter \n                        Hodor     |   Stark Regime\n        \"\"\"\n\n    session = run_test(commands, expected_result, [which_century_dict])\n\n    run_test(commands2, expected_result2, [which_era_dict], session=session)\n\ntest_issue_80_1()\n\nprinting results for query 'important_events_per_cet(EVE, CET)':\n     EVE      |   CET\n--------------+-------\n    Hodor     |    10\n Winter came  |    18\n First Dragon |     3\n\nprinting results for query 'important_events_per_era(EVE, ERA)':\n     EVE      |       ERA\n--------------+------------------\n    Hodor     |   Stark Regime\n Winter came  |   Long Winter\n First Dragon | Targerian Regime\n\n\n\n\ndef test_issue_80_2() -&gt; None:\n    def multiple_highest_2(x, y, z) -&gt; Iterable[Tuple[int, int]]:\n        if (x &lt;= y &lt;= z) or (x &lt;= z &lt;= y):\n            yield y * z, x\n        elif (y &lt;= x &lt;= z) or (y &lt;= z &lt;= x):\n            yield x * z, y\n        elif (z &lt;= x &lt;= y) or (z &lt;= y &lt;= x):\n            yield y * x, z\n\n    in_types = [DataTypes.integer, DataTypes.integer, DataTypes.integer]\n    out_types = [DataTypes.integer, DataTypes.integer]\n\n    multiple_highest_2_dict = dict(ie_function=multiple_highest_2,\n                                   ie_function_name='multiple_highest_2',\n                                   in_rel=in_types,\n                                   out_rel=out_types)\n\n    def multiple_by_2_the_highest(x, y) -&gt; Iterable[Tuple[int, int]]:\n        if x &lt; y:\n            yield y * 2, x\n        else:\n            yield x * 2, y\n\n    in_out_types = [DataTypes.integer, DataTypes.integer]\n\n    multiple_by_2_the_highest_dict = dict(ie_function=multiple_by_2_the_highest,\n                                          ie_function_name='multiple_by_2_the_highest',\n                                          in_rel=in_out_types,\n                                          out_rel=in_out_types)\n\n    commands = \"\"\"new trio(int, int, int)\n\n                trio(5, 6 ,7)\n                trio(4, 4, 7)\n                trio(10, 8 ,2)\n                trio(4, 4, 4)\n                trio(8, 40, 12)\n\n                new pair(int, int)\n                pair(2, 4)\n                pair(5, 10)\n                pair(3, 3)\n                pair(10, 14)\n\n                multiple_highest_2_from_trio(MUL, MIN) &lt;- trio(X, Y, Z), multiple_highest_2(X, Y, Z) -&gt; (MUL, MIN)\n                ?multiple_highest_2_from_trio(MUL, MIN)\n            \"\"\"\n    commands2 = \"\"\"\n                multiple_by_2_the_highest_from_pairs(MUL2, MIN) &lt;- pair(X, Y), multiple_by_2_the_highest(X, Y) -&gt; (MUL2, MIN)\n                min_is_the_same(MUL, MUL2, MIN) &lt;- multiple_highest_2_from_trio(MUL, MIN), multiple_by_2_the_highest_from_pairs(MUL2, MIN)\n                ?min_is_the_same(MUL, MUL2, MIN)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'multiple_highest_2_from_trio(MUL, MIN)':\n                MUL |   MIN\n             -------+-------\n                 42 |     5\n                 28 |     4\n                 80 |     2\n                 16 |     4\n                480 |     8\n         \"\"\"\n\n    expected_result2 = f\"\"\"{QUERY_RESULT_PREFIX}'min_is_the_same(MUL, MUL2, MIN)':\n               MUL |   MUL2 |   MIN\n            -------+--------+-------\n                80 |      8 |     2\n                42 |     20 |     5\n        \"\"\"\n\n    session = run_test(commands, expected_result, [multiple_highest_2_dict])\n\n    run_test(commands2, expected_result2, [multiple_by_2_the_highest_dict], session=session)\n\ntest_issue_80_2()\n\nprinting results for query 'multiple_highest_2_from_trio(MUL, MIN)':\n   MUL |   MIN\n-------+-------\n    42 |     5\n    28 |     4\n    80 |     2\n    16 |     4\n   480 |     8\n\nprinting results for query 'min_is_the_same(MUL, MUL2, MIN)':\n   MUL |   MUL2 |   MIN\n-------+--------+-------\n    80 |      8 |     2\n    42 |     20 |     5\n\n\n\n\ndef test_neq() -&gt; None:\n    def neq(x: Any, y: Any) -&gt; Iterable:\n        if x == y:\n            # return false\n            yield tuple()\n        else:\n            # return true\n            yield x, y\n\n    in_out_types = [DataTypes.string, DataTypes.string]\n    neq_dict = dict(ie_function=neq,\n                    ie_function_name='NEQ',\n                    in_rel=in_out_types,\n                    out_rel=in_out_types)\n    commands = \"\"\"new pair(str, str)\n                pair(\"Dan\", \"Tom\")\n                pair(\"Cat\", \"Dog\")\n                pair(\"Apple\", \"Apple\")\n                pair(\"Cow\", \"Cow\")\n                pair(\"123\", \"321\")\n\n                unique_pair(X, Y) &lt;- pair(First, Second), NEQ(First, Second) -&gt; (X, Y)\n                ?unique_pair(X, Y)\n                \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'unique_pair(X, Y)':\n          X  |  Y\n        -----+-----\n         Dan | Tom\n         Cat | Dog\n         123 | 321\n        \"\"\"\n\n    run_test(commands, expected_result, [neq_dict])\n\ntest_neq()\n\nprinting results for query 'unique_pair(X, Y)':\n  X  |  Y\n-----+-----\n Dan | Tom\n Cat | Dog\n 123 | 321\n\n\n\n\ndef test_span_constant() -&gt; None:\n    commands = '''\n            new verb(str, span)\n            verb(\"Ron eats quickly.\", [4,8))\n            verb(\"You write neatly.\", [4,9))\n            ?verb(X,[4,9)) # returns \"You write neatly.\"\n            '''\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'verb(X, [4, 9))':\n                                 X\n                        -------------------\n                         You write neatly.\"\"\"\n\n    run_test(commands, expected_result)\ntest_span_constant()\n\nprinting results for query 'verb(X, [4, 9))':\n         X\n-------------------\n You write neatly."
  },
  {
    "objectID": "tests/05f_test_lark_passes.html",
    "href": "tests/05f_test_lark_passes.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import pytest\nfrom spannerlib.session import Session\nfrom spannerlib.ast_node_types import RelationDeclaration, AddFact, Query\nfrom spannerlib.primitive_types import DataTypes, Span\n\nInstallation NLP failed\n\n\n\n\ndef test_CheckReservedRelationNames()-&gt;None:\n    with pytest.raises(Exception) as exc_info:\n        my_session = Session()\n        my_session.run_commands(\"\"\"\n                    new age__spannerlog__(str,int)\n                    age__spannerlog__(\"Allie\",23)\n                \"\"\")\n    assert str(exc_info.value) == \"encountered relation name: age__spannerlog__. names containing __spannerlog__ are reserved\"\n    \ntest_CheckReservedRelationNames()\n\n\ndef test_FixStrings()-&gt;None:\n    my_session = Session()\n    output = my_session.run_commands(\"\"\"\n                new age(str,int)\n                age(\"\\\\\\nAllie\",23)\n                age(\"\\\\nAllie\",23)\n                ?age(X,Y)\n            \"\"\")\n    expected_output = r\"[(age(X, Y), [('Allie', 23), ('\\\\nAllie', 23)])]\"\n    assert str(output) == expected_output\n    \ntest_FixStrings()\n\nprinting results for query 'age(X, Y)':\n    X    |   Y\n---------+-----\n  Allie  |  23\n \\nAllie |  23\n\n\n\n\ndef test_ConvertSpanNodesToSpanInstances():\n    my_session = Session()\n    output = my_session.run_commands(\"\"\"\n                new price(str,span)\n                price(\"milk\",[5,10))\n                ?price(X,Y)\n            \"\"\")\n    relation_name, fact = output[0]\n    product_name, product_price_span = fact[0]\n    \n    expected_product_name = \"milk\"\n    expected_product_price_span = Span(5, 10)\n    \n    assert product_name == expected_product_name\n    assert product_price_span == expected_product_price_span\n\ntest_ConvertSpanNodesToSpanInstances()\n\nprinting results for query 'price(X, Y)':\n  X   |    Y\n------+---------\n milk | [5, 10)\n\n\n\n\ndef test_CheckDefinedReferencedVariables():\n    test_cases = [\n        \"\"\"\n        new sisters(str,str)\n        sisters(\"Sara\",\"Kyra\")\n        sisters(\"Allie\",f)\n        \"\"\",\n        \"\"\"\n        new sisters(str,str)\n        sisters(\"Sara\",\"Kyra\")\n        sisters_of_Sara(X) &lt;- sisters(f,X)\n        \"\"\",\n        \"\"\"\n        new sisters(str,str)\n        sisters(\"Sara\",\"Kyra\")\n        ?sisters(f,X)\n        \"\"\"\n    ]\n    \n    expected_error = 'variable \"f\" is not defined'\n    \n    for idx, commands in enumerate(test_cases, start=1):\n        with pytest.raises(Exception) as exc_info:\n            my_session = Session()\n            my_session.run_commands(commands)\n        \n        assert str(exc_info.value) == expected_error, f\"Test case {idx} failed\"\n\ntest_CheckDefinedReferencedVariables()\n\n\ndef test_CheckDefinedReferencedVariables4():\n    my_session = Session()\n    output = my_session.run_commands(\"\"\"\n                    new sisters(str,str)\n                    sisters(\"Sara\",\"Kyra\") &lt;- True\n                    sisters(\"Sara\",\"Kyra\") &lt;- False\n                    ?sisters(X,Y)\n                \"\"\")\n    assert str(output) == '''[(sisters(X, Y), [])]'''\ntest_CheckDefinedReferencedVariables4()\n\nprinting results for query 'sisters(X, Y)':\n[]\n\n\n\n\ndef test_CheckReferencedRelationsExistenceAndArity():\n    test_cases = [\n        (\n            \"\"\"\n            new sisters(str, str)\n            sisters(\"Allie\")\n            \"\"\",\n            '''relation \"sisters\" was referenced with an incorrect arity: 1. The correct arity is: 2'''\n        ),\n        (\n            \"\"\"\n            new sisters(str, str)\n            sisters(\"Allie\", \"Sara\")\n            brothers_or_sisters(X, Y) &lt;- sisters(X, Y)\n            brothers_or_sisters(X, Y) &lt;- brothers(X, Y)\n            \"\"\",\n            '''relation \"brothers\" is not defined'''\n        ),\n        (\n            \"\"\"\n            new sisters(str, str)\n            sisters(\"Allie\", \"Sara\")\n            brothers_or_sisters(X, Y) &lt;- sisters(X, Y)\n            ?brothers_or_sisters(X)\n            \"\"\",\n            '''relation \"brothers_or_sisters\" was referenced with an incorrect arity: 1. The correct arity is: 2'''\n        ),\n        (\n            \"\"\"\n            new sisters(str, str)\n            sisters(\"Allie\", \"Sara\")\n            new brothers(str, str)\n            brothers(\"Sam\", \"Noah\")\n            brothers_or_sisters(X, Y) &lt;- sisters(X, Y)\n            brothers_or_sisters(X, Y) &lt;- brothers(X, Y, Z)\n            \"\"\",\n            '''relation \"brothers\" was referenced with an incorrect arity: 3. The correct arity is: 2'''\n        )\n    ]\n    \n    for idx, (commands, expected) in enumerate(test_cases, start=1):\n        with pytest.raises(Exception) as exc_info:\n            my_session = Session()\n            my_session.run_commands(commands)\n        \n        assert str(exc_info.value) == expected, f\"Test case {idx} failed\"\n\ntest_CheckReferencedRelationsExistenceAndArity()\n\n\ndef test_CheckRuleSafety1():\n    with pytest.raises(Exception) as exc_info:\n        my_session = Session()\n        output = my_session.run_commands(\"\"\"\n                    new daughter(str)\n                    daughter(\"Allie\")\n                    mom(X,Y) &lt;- daughter(Y)\n                \"\"\")\n    expected = '''The rule \"mom(X, Y) &lt;- daughter(Y)\" \\nis not safe because the following free variables appear in the rule head but not as output terms in the rule body:\\n{'X'}'''\n    assert str(exc_info.value) == expected\n\ntest_CheckRuleSafety1()\n\n\ndef test_TypeCheckRelations1():\n    with pytest.raises(Exception) as exc_info:\n        my_session = Session()\n        output = my_session.run_commands(\"\"\"\n                    new daughter(str)\n                    daughter(1)\n                \"\"\")\n    expected = '''type check failed for fact: \"daughter(1)\"'''\n    assert str(exc_info.value) == expected\n\ntest_TypeCheckRelations1()\n\n\ndef test_TypeCheckRelations2():\n    with pytest.raises(Exception) as exc_info:\n        my_session = Session()\n        output = my_session.run_commands(\"\"\"\n                    new age_str(str,str)\n                    age_str(\"Allie\",\"23\")\n                    new age_int(str,int)\n                    age_int(\"Sam\",23)\n                    same_age(X)&lt;- age_str(X,Y), age_int(X,Y)\n                \"\"\")\n    expected = '''type check failed for rule \"same_age(X) &lt;- age_str(X, Y), age_int(X, Y)\"\\nbecause the following free variables have conflicting types:\\n{'Y'}'''\n    assert str(exc_info.value) == expected\n\ntest_TypeCheckRelations2()\n\n\ndef test_SaveDeclaredRelationsSchemas():\n    commands = \"\"\"\n                new parent(str, str)\n                person(X) &lt;- parent(X, Y)\n                person(Y) &lt;- parent(X, Y)\n                allie = \"Allie\"\n                jackie = \"Jackie\"\n                parent(jackie, allie)\n                ?person(X)\n            \"\"\"\n    expected_output = '''[(person(X), [('Allie',), ('Jackie',)])]'''\n    \n    my_session = Session()\n    output = my_session.run_commands(commands)\n    \n    all_relations = my_session._symbol_table.get_all_relations()\n    all_vars = my_session._symbol_table.get_all_variables()\n    \n    assert (\n        len(all_relations) == 2 and\n        len(all_vars) == 2 and\n        str(output) == expected_output\n    )\n\n\ntest_SaveDeclaredRelationsSchemas()\n\nprinting results for query 'person(X)':\n   X\n--------\n Allie\n Jackie\n\n\n\n\ndef test_ExecuteAssignments():\n    commands = \"\"\"\n                allie = \"Allie\"\n                jackie = \"Jackie\"\n                a = 1\n            \"\"\"\n    expected_variables = [('allie', 'Allie'), ('jackie', 'Jackie'), ('a', 1)]\n    \n    my_session = Session()\n    output = my_session.run_commands(commands)\n    all_vars = my_session._symbol_table.get_all_variables()\n\n    assert len(all_vars) == len(expected_variables)\n\n    for var_info, expected_info in zip(all_vars, expected_variables):\n        assert var_info[0] == expected_info[0]\n        assert var_info[2] == expected_info[1]\n\ntest_ExecuteAssignments()\n\n\ndef test_AddStatementsToNetxParseGraph():\n    my_session = Session()\n    output = my_session.run_commands(\"\"\"\n                        new parent(str,str)\n                        person(X) &lt;- parent(X,Y)\n                        person(Y) &lt;- parent(X,Y)\n                        allie = \"Allie\"\n                        jackie = \"Jackie\"\n                        parent(jackie,allie)\n                        ?person(X)\n\n            \"\"\")\n    num_of_commands = len(my_session._parse_graph.get_children(my_session._parse_graph._root_id))\n    assert num_of_commands == 5\n\ntest_AddStatementsToNetxParseGraph()\n\nprinting results for query 'person(X)':\n   X\n--------\n Allie\n Jackie\n\n\n\n\ndef test_AddRulesToTermGraph():\n    my_session = Session()\n    output = my_session.run_commands(\"\"\"\n                        new parent(str,str)\n                        parent(\"Taylor\",\"Sam\")\n                        parent(\"Elijah\",\"Taylor\")\n                        grandparent(X,Z)&lt;-parent(X,Y) , parent(Y,Z)\n                        ?grandparent(X,Y)\n                        new sisters(str,str)\n                        sisters(\"Sam\",\"Kristen\")\n                        sisters(\"Allie\",\"Jackie\")\n                        person(X)&lt;-sisters(X,Y)\n                        person(X)&lt;-sisters(Y,X)\n                        ?person(X)\n\n            \"\"\")\n    assert (my_session._term_graph.get_children(my_session._term_graph.get_root_id())) == ['grandparent', 'person']\n\ntest_AddRulesToTermGraph()\n\nprinting results for query 'grandparent(X, Y)':\n   X    |  Y\n--------+-----\n Elijah | Sam\n\nprinting results for query 'person(X)':\n    X\n---------\n  Allie\n Jackie\n Kristen\n   Sam"
  },
  {
    "objectID": "tests/tests_utils.html",
    "href": "tests/tests_utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\nis_equal_stripped_sorted_tables\n\n is_equal_stripped_sorted_tables (result_text:str, expected_text:str)\n\n*Compares all lines in between two strings, ignoring the order of the lines.\n@param result_text: first string to compare, usually the output of a test. @param expected_text: second string to compare, usually the expected output of a test. @return: True if equal, else False.*\n\nsource\n\n\nis_equal_dataframes_ignore_order\n\n is_equal_dataframes_ignore_order (result_df:pandas.core.frame.DataFrame,\n                                   expected_df:pandas.core.frame.DataFrame\n                                   )\n\n*Similarly to is_equal_stripped_sorted_tables, compares two dataframes while ignoring the order of the rows.\n@param result_df: first dataframe to compare. @param expected_df: second dataframe to compare. @return: True if equal, else False.*\n\nsource\n\n\ntable_to_query_free_vars_tuples\n\n table_to_query_free_vars_tuples (table:str)\n\n*Parses the string table into a nicer format.\n@param table: the string that represents a table. @return: the clean format (see comments above return statements).*\n\nsource\n\n\nsplit_to_tables\n\n split_to_tables (result:str)\n\n@param result: spannerlog’s output. @return: List of strings, each string represents a table.\n\nsource\n\n\ncompare_strings\n\n compare_strings (expected:str, output:str)\n\n@param expected: expected output. @param output: actual output. @return: True if output and expected represent the same result, False otherwise.\n\nsource\n\n\nrun_test\n\n run_test (commands:str, expected_output:Union[str,NoneType]=None,\n           functions_to_import:Iterable[Dict]=(),\n           session:Union[spannerlib.session.Session,NoneType]=None)\n\n*A function that executes a test.\n@param commands: the commands to run. @param expected_output: the expected output of the commands. if it has value of None, than we won’t check the output. @param functions_to_import: an iterable of functions we want to import to the session. @param session: the session in which we run the commands. @return: the session it created or got as an argument.*\n\nsource\n\n\nrun_test_df_compare\n\n run_test_df_compare (commands:str,\n                      expected_output:Union[pandas.core.frame.DataFrame,No\n                      neType]=None, functions_to_import:Iterable[Dict]=(),\n                      session:Union[spannerlib.session.Session,NoneType]=N\n                      one)\n\n*A function that executes a test.\n@param commands: the commands to run. @param expected_output: the expected output of the commands (in a format of pd.DataFrame). if it has value of None, than we won’t check the output. @param functions_to_import: an iterable of functions we want to import to the session. @param session: the session in which we run the commands. @return: the session it created or got as an argument.*\n\nsource\n\n\ncheck_unordered_dataframes_equal\n\n check_unordered_dataframes_equal (df1:pandas.core.frame.DataFrame,\n                                   df2:pandas.core.frame.DataFrame)\n\n\nsource\n\n\nget_session_with_optimizations\n\n get_session_with_optimizations (parse_graph_optimization_passes:Iterable[\n                                 Type[spannerlib.lark_passes.GenericPass]]\n                                 =(), term_graph_optimization_passes:Itera\n                                 ble[Type[spannerlib.lark_passes.GenericPa\n                                 ss]]=())\n\nCreates a session and adds optimization passes to the pass stack. @param parse_graph_optimization_passes: optimization passes that will be added before AddRulesToComputationTermGraph pass. @param term_graph_optimization_passes: optimization passes that will be added after AddRulesToComputationTermGraph pass @return: the session.\n\nsource\n\n\nrun_commands_into_csv_test\n\n run_commands_into_csv_test (expected_longrel:str,\n                             im_ex_session:spannerlib.session.Session,\n                             commands:str, query_for_csv:str)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\nget_git_root\n\n get_git_root (path='.')\n\n\nsource\n\n\nget_lib_name\n\n get_lib_name ()\n\n\nsource\n\n\nget_base_file_path\n\n get_base_file_path ()\n\n\nsource\n\n\npatch_method\n\n patch_method (func:Callable, *args, **kwargs)\n\nApplies fastcore’s patch decorator and removes func from cls.__abstractsmethods__ in case  func is an abstractmethods\n\nsource\n\n\nkill_process_and_children\n\n kill_process_and_children (process:subprocess.Popen)\n\n\nsource\n\n\nrun_cli_command\n\n run_cli_command (command:str, stderr:bool=False, shell:bool=False,\n                  timeout:float=-1)\n\nThis utility can be used to run any cli command, and iterate over the output.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncommand\nstr\n\na single command string\n\n\nstderr\nbool\nFalse\nif true, suppress stderr output. default: False\n\n\nshell\nbool\nFalse\nif true, spawn shell process (e.g. /bin/sh), which allows using system variables (e.g. $HOME),but is considered a security risk (see: https://docs.python.org/3/library/subprocess.html#security-considerations)\n\n\ntimeout\nfloat\n-1\nif positive, kill the process after timeout seconds. default: -1\n\n\nReturns\ntyping.Iterable[str]\n\nstring iterator\n\n\n\n\nsource\n\n\ndownload_file_from_google_drive\n\n download_file_from_google_drive (file_id:str, destination:pathlib.Path)\n\nDownloads a file from Google Drive\n\n\n\n\nType\nDetails\n\n\n\n\nfile_id\nstr\nthe id of the file to download\n\n\ndestination\nPath\nthe path to which the file will be downloaded\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\ndf_to_list\n\n df_to_list (df)"
  },
  {
    "objectID": "optimizations_passes.html",
    "href": "optimizations_passes.html",
    "title": "Optimizations Passes",
    "section": "",
    "text": "Note\n\n\n\nNote that these passes depend on the structure of the term graph!\n\n\n\nsource\n\nPruneUnnecessaryProjectNodes\n\n PruneUnnecessaryProjectNodes (term_graph:TermGraphBase, **kwargs:Any)\n\n*This class prunes project nodes that gets a relation with one column (therefore, the project is redundant). For example: the rule A(X) &lt;- B(X) will yield the following term graph:\n    rule_rel node (of A)\n        union node\n            project node (on X)\n               get_rel node (get B)\nsince we project a relation with one column, after this pass the term graph will be: prolog     rule_rel node (of A)         union node             get_rel node (get B)*\n\nsource\n\n\nRemoveUselessRelationsFromRule\n\n RemoveUselessRelationsFromRule (parse_graph:GraphBase, **kwargs:Any)\n\nThis pass removes duplicated relations from a rule.  For example, the rule A(X) &lt;- B(X), C(Y) contains a redundant relation (C(Y)).  After this pass the rule will be A(X) &lt;- B(X).\n\n\n\n\n\n\nNote\n\n\n\nIn the rule A(X) &lt;- B(X, Y), C(Y), C(Y) is not redundant!",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Optimizations Passes"
    ]
  },
  {
    "objectID": "session.html",
    "href": "session.html",
    "title": "Session",
    "section": "",
    "text": "assert _infer_relation_type([1, 2, 3]) == [ DataTypes.integer,DataTypes.integer,DataTypes.integer]\nassert _infer_relation_type([1, 'a']) == [ DataTypes.integer,DataTypes.string]\nassert _infer_relation_type(['[0,1)','[0, 1)',Span(1,3)]) == [DataTypes.span,DataTypes.span,DataTypes.span]\n\n\nsource\n\nformat_query_results\n\n format_query_results (query:spannerlib.ast_node_types.Query,\n                       query_results:List)\n\nFormats a single result from the engine into a usable format.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nquery\nQuery\nthe query that was executed, and outputted query_results\n\n\nquery_results\nList\nthe results after executing the aforementioned query\n\n\nReturns\nUnion[DataFrame, List]\na false value, a true value, or a dataframe representing the query + its results\n\n\n\n\nsource\n\n\ntabulate_result\n\n tabulate_result (result:Union[pandas.core.frame.DataFrame,List])\n\n*Organizes a query result in a table  for example: \n{QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n   X\n-------\n linus\n walter\nThere are two cases in which a table won’t be printed:\n\nQuery returned no results: This will result in an output of [].\nQuery returned a single empty tuple: The output will be [()].*\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresult\nUnion[DataFrame, List]\nthe query result (free variable names are the dataframe’s column names)\n\n\nReturns\nstr\na tabulated string\n\n\n\n\nsource\n\n\nqueries_to_string\n\n queries_to_string\n                    (query_results:List[Tuple[spannerlib.ast_node_types.Qu\n                    ery,List]])\n\n*Takes in a list of results from the engine and converts them into a single string, which contains either a table, a false value (=[]), or a true value (=[tuple()]), for each result.\nfor example:\n{QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n   X\n-------\n linus\n walter\n```*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| query_results | List[Tuple[Query, List]] | List[the Query object used in execution, the execution's results (from engine)] |\n| **Returns** | **str** | **a tabulated string** |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/session.py#L218){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### Session\n\n&gt;      Session (symbol_table:Optional[SymbolTableBase]=None,\n&gt;               parse_graph:Optional[GraphBase]=None,\n&gt;               term_graph:Optional[TermGraphBase]=None)\n\n*A class that serves as the central connection point between various modules in the system.\n\nThis class takes input data and coordinates communication between different modules by sending the relevant parts\nof the input to each module. It also orchestrates the execution of micro passes and handles engine-related tasks. &lt;br&gt;\nFinally, it formats the results before presenting them to the user.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| symbol_table | Optional[SymbolTableBase] | None | symbol table to help with all semantic checks |\n| parse_graph | Optional[GraphBase] | None | an AST that contains nodes which represent commands |\n| term_graph | Optional[TermGraphBase] | None | a graph that holds all the connection between the relations |\n\n\n---\n\n### Session.get_pass_stack\n\n&gt;      Session.get_pass_stack ()\n\n*@return: the current pass stack.*\n\n\n---\n\n### Session.set_pass_stack\n\n&gt;      Session.set_pass_stack\n&gt;                              (user_stack:List[Type[spannerlib.lark_passes.Gene\n&gt;                              ricPass]])\n\n*Sets a new pass stack instead of the current one.*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| user_stack | List[Type[GenericPass]] | a user supplied pass stack |\n| **Returns** | **List[Type[GenericPass]]** | **success message with the new pass stack** |\n\n\n---\n\n### Session.export\n\n&gt;      Session.export (query=None, relation_name:str=None, csv_path=None,\n&gt;                      delimiter:str=';')\n\n*Exports the given query or relation to a csv file or a dataframe.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| query | NoneType | None | query string to export |\n| relation_name | str | None | whether to export an entire relation (either extrinsic or intrinsic), cant be used together with query parameter |\n| csv_path | NoneType | None | whether to export to csv, by default returns as a dataframe |\n| delimiter | str | ; | the delimeter to use in the csv file |\n| **Returns** | **Union[DataFrame, List]** |  |  |\n\n\n---\n\n### Session.run_commands\n\n&gt;      Session.run_commands (query:str, print_results:bool=True,\n&gt;                            format_results:bool=False)\n\n*Generates an AST and passes it through the pass stack.*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| query | str |  | The user's input |\n| print_results | bool | True | whether to print the results to stdout or not |\n| format_results | bool | False | if this is true, return the formatted result instead of the `[Query, List]` pair |\n| **Returns** | **Union[List[Union[List, List[Tuple], DataFrame]], List[Tuple[Query, List]]]** |  | **the results of every query, in a list** |\n\n\n::: {.callout-note collapse=\"true\"}\n\n##### Example\n\n::: {#cell-12 .cell}\n``` {.python .cell-code}\ncommands = \"\"\"\n    new Parent(str, str)\n    Parent(\"Sam\", \"Noah\")\n    Parent(\"Noah\", \"Austin\")\n    Parent(\"Austin\", \"Stephen\")\n\n    GrandParent(G, C) &lt;- Parent(G, M), Parent(M, C)\n    ?GrandParent(X, \"Austin\")\n    \"\"\"\nsession = Session()\noutput = session.run_commands(commands)\n\nprinting results for query 'GrandParent(X, \"Austin\")':\n  X\n-----\n Sam\n\n\n:::\n\noutput = session.export(query='?GrandParent(X, \"Austin\")')\nassert output.to_dict(orient='records') == [{'X': 'Sam'}]\noutput\n\n\n\n\n\n\n\n\n\nX\n\n\n\n\n0\nSam\n\n\n\n\n\n\n\n\n\noutput = session.export(relation_name='GrandParent')\nassert output.to_dict(orient='records') == [{'COL0': 'Sam', 'COL1': 'Austin'}, {'COL0': 'Noah', 'COL1': 'Stephen'}]\noutput\n\n\n\n\n\n\n\n\n\nCOL0\nCOL1\n\n\n\n\n0\nSam\nAustin\n\n\n1\nNoah\nStephen\n\n\n\n\n\n\n\n\n:::\n\n\n\nSession.register\n\n Session.register (ie_function:Callable, ie_function_name:str,\n                   in_rel:List[spannerlib.primitive_types.DataTypes], out_\n                   rel:Union[List[spannerlib.primitive_types.DataTypes],Ca\n                   llable[[int],Sequence[spannerlib.primitive_types.DataTy\n                   pes]]])\n\n*Registers an ie function.\n@see params in IEFunction’s init.*\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ndef length(string: str) -&gt; Iterable[int]:\n        # here we append the input to the output inside the ie function!\n        yield len(string)\n\nlength_dict = dict(ie_function=length,\n                ie_function_name='Length',\n                in_rel=[DataTypes.string],\n                out_rel=[DataTypes.integer])\n\nsession = Session()\nsession.register(**length_dict)\ncommands = \"\"\"new string(str)\n            string(\"a\")\n            string(\"d\")\n            string(\"a\")\n            string(\"ab\")\n            string(\"abc\")\n            string(\"abcd\")\n\n            string_length(Str, Len) &lt;- string(Str), Length(Str) -&gt; (Len)\n            ?string_length(Str, Len)\n            \"\"\"\noutput = session.run_commands(commands)\n\nprinting results for query 'string_length(Str, Len)':\n  Str  |   Len\n-------+-------\n   a   |     1\n   d   |     1\n  ab   |     2\n  abc  |     3\n abcd  |     4\n\n\n\n\n\n\n\n\n\nSession.remove_rule\n\n Session.remove_rule (rule:str)\n\nRemove a rule from the spannerlog’s engine.\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nstr\nThe rule to be removed\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ncommands = \"\"\"\n    new parent(str, str)\n    new grandparent(str, str)\n    parent(\"Liam\", \"Noah\")\n    parent(\"Noah\", \"Oliver\")\n    parent(\"James\", \"Lucas\")\n    parent(\"Noah\", \"Benjamin\")\n    parent(\"Benjamin\", \"Mason\")\n    grandparent(\"Tom\", \"Avi\")\n    ancestor(X,Y) &lt;- parent(X,Y)\n    ancestor(X,Y) &lt;- grandparent(X,Y)\n    ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n    \"\"\"\nsession = Session()\noutput = session.run_commands(commands)\nsession.print_all_rules()\n\nPrinting all the rules:\n    1. ancestor(X, Y) &lt;- parent(X, Y)\n    2. ancestor(X, Y) &lt;- grandparent(X, Y)\n    3. ancestor(X, Y) &lt;- parent(X, Z), ancestor(Z, Y)\n\n\nafter removing first rule:\n\nsession.remove_rule(\"ancestor(X, Y) &lt;- parent(X, Y)\")\nsession.print_all_rules()\n\nPrinting all the rules:\n    1. ancestor(X, Y) &lt;- grandparent(X, Y)\n    2. ancestor(X, Y) &lt;- parent(X, Z), ancestor(Z, Y)\n\n\n\n\n\n\n\n\nSession.remove_all_rules\n\n Session.remove_all_rules (rule_head:Union[str,NoneType]=None)\n\nRemoves all rules from the engine.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrule_head\nOptional[str]\nNone\nif rule head is not none we remove all rules with rule_head\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ncommands = \"\"\"\n    new parent(str, str)\n    new grandparent(str, str)\n    parent(\"Liam\", \"Noah\")\n    grandparent(\"Tom\", \"Avi\")\n    ancestor(X,Y) &lt;- parent(X,Y)\n    ancestor(X,Y) &lt;- grandparent(X,Y)\n    ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n    \"\"\"\nsession = Session()\noutput = session.run_commands(commands)\nsession.print_all_rules()\n\nPrinting all the rules:\n    1. ancestor(X, Y) &lt;- parent(X, Y)\n    2. ancestor(X, Y) &lt;- grandparent(X, Y)\n    3. ancestor(X, Y) &lt;- parent(X, Z), ancestor(Z, Y)\n\n\nafter removing all rules:\n\nsession.remove_all_rules()\nsession.print_all_rules()\n\nPrinting all the rules:\n\n\n\n\n\n\n\n\nSession.clear_relation\n\n Session.clear_relation (relation_name:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nThe name of the relation to clear\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ncommands = \"\"\"\n    new parent(str, str)\n    parent(\"Liam\", \"Noah\")\n    parent(\"Noah\", \"Oliver\")\n    ?parent(X,Y)\n    \"\"\"\nsession = Session()\noutput = session.run_commands(commands)\n\nprinting results for query 'parent(X, Y)':\n  X   |   Y\n------+--------\n Liam |  Noah\n Noah | Oliver\n\n\n\nafter clearing parent relation:\n\nsession.clear_relation(\"parent\")\ncommands = \"\"\"\n    ?parent(X,Y)\n    \"\"\"\noutput = session.run_commands(commands)\n\nprinting results for query 'parent(X, Y)':\n[]\n\n\n\n\n\n\n\n\n\nSession.send_commands_result_into_csv\n\n Session.send_commands_result_into_csv (commands:str,\n                                        csv_file_name:pathlib.Path,\n                                        delimiter:str=';')\n\nrun commands as usual and output their formatted results into a csv file (the commands should contain a query)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncommands\nstr\n\nthe commands to run\n\n\ncsv_file_name\nPath\n\nthe file into which the output will be written\n\n\ndelimiter\nstr\n;\na csv separator between values\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\nSession.print_registered_ie_functions\n\n Session.print_registered_ie_functions ()\n\nPrints information about the registered ie functions.\n\n\n\nSession.remove_ie_function\n\n Session.remove_ie_function (name:str)\n\nRemoves a function from the symbol table.\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nthe name of the ie function to remove\n\n\nReturns\nNone\n\n\n\n\n\n\n\nSession.remove_all_ie_functions\n\n Session.remove_all_ie_functions ()\n\nRemoves all the ie functions from the symbol table.\n\n\n\nSession.print_all_rules\n\n Session.print_all_rules (head:Union[str,NoneType]=None)\n\nPrints all the rules that are registered.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhead\nOptional[str]\nNone\nif specified it will print only rules with the given head relation name\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ncommands = \"\"\"\n    new parent(str, str)\n    new grandparent(str, str)\n    ancestor(X,Y) &lt;- parent(X,Y)\n    ancestor(X,Y) &lt;- grandparent(X,Y)\n    ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n    \"\"\"\nsession = Session()\noutput = session.run_commands(commands)\nsession.print_all_rules()\n\nPrinting all the rules:\n    1. ancestor(X, Y) &lt;- parent(X, Y)\n    2. ancestor(X, Y) &lt;- grandparent(X, Y)\n    3. ancestor(X, Y) &lt;- parent(X, Z), ancestor(Z, Y)\n\n\n\n\n\n\n\n\nSession.import_rel\n\n Session.import_rel (data:Union[pandas.core.frame.DataFrame,pathlib.Path],\n                     relation_name:str=None, delimiter:str=None)\n\nImports a relation into the current session, either from a dataframe or from a csv file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nUnion[DataFrame, Path]\n\nEither a dataframe or a path to a csv file to import.\n\n\nrelation_name\nstr\nNone\nThe name of the relation. If not provided when importing a csv, it will be derived from the file name.\n\n\ndelimiter\nstr\nNone\nThe delimiter used when parsing a csv file, defaults to ‘;’\n\n\nReturns\nNone\n\n\n\n\n\n\nsession = Session()\nsession.import_rel(\"./sample_data/enrolled.csv\", relation_name=\"enrolled\", delimiter=\",\")\ncommands = \"\"\"\n    enrolled(\"abigail\", \"chemistry\")\ngpa_str = \"abigail 100 jordan 80 gale 79 howard 60\"\n\ngpa(Student,Grade) &lt;- py_rgx_string(gpa_str, \"(\\w+).*?(\\d+)\")-&gt;(Student, Grade),enrolled(Student,X)\n\n?gpa(X,Y)\n\"\"\"\nx = session.run_commands(commands)\n\nprinting results for query 'gpa(X, Y)':\n    X    |   Y\n---------+-----\n abigail | 100\n jordan  |  80\n  gale   |  79\n howard  |  60\n\n\n\n\nsession = Session()\nlecturer_df = DataFrame(([[\"walter\",\"chemistry\"], [\"linus\", \"operating_systems\"]]))\nsession.import_rel(lecturer_df, relation_name=\"lecturer\")\ncommands = \"\"\" \n?lecturer(X,Y)\n\"\"\"\noutput = session.run_commands(commands)\n\nprinting results for query 'lecturer(X, Y)':\n   X    |         Y\n--------+-------------------\n walter |     chemistry\n linus  | operating_systems\n\n\n\n\noutput\n\n[(lecturer(X, Y), [('walter', 'chemistry'), ('linus', 'operating_systems')])]\n\n\n\n# #| export\n# #| hide\n# @patch_method\n# def import_relation_from_csv(self: Session, csv_file_name: Path, #The path to the CSV file that is being imported\n#                              relation_name: str = None, #The name of the relation. If not provided, it will be derived from the CSV file name\n#                              delimiter: str = CSV_DELIMITER #The delimiter used in the CSV file\n#                              )-&gt; None: \n#     if not Path(csv_file_name).is_file():\n#         raise IOError(\"csv file does not exist\")\n\n#     if os.stat(csv_file_name).st_size == 0:\n#         raise IOError(\"csv file is empty\")\n\n#     # the relation_name is either an argument or the file's name\n#     if relation_name is None:\n#         relation_name = Path(csv_file_name).stem\n\n#     with open(csv_file_name) as fh:\n#         reader = csv.reader(fh, delimiter=delimiter)\n\n#         # read first line and go back to start of file - make sure there is no empty line!\n#         relation_types = _infer_relation_type(next(reader))\n#         fh.seek(0)\n\n#         self._add_imported_relation_to_engine(reader, relation_name, relation_types)\n\n\n# show_doc(Session.import_relation_from_csv)\n\n\n\nSession.import_relation_from_csv\n\n Session.import_relation_from_csv (csv_file_name:pathlib.Path,\n                                   relation_name:str=None,\n                                   delimiter:str=';')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncsv_file_name\nPath\n\nThe path to the CSV file that is being imported\n\n\nrelation_name\nstr\nNone\nThe name of the relation. If not provided, it will be derived from the CSV file name\n\n\ndelimiter\nstr\n;\nThe delimiter used in the CSV file\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n# session = Session()\n# session.import_relation_from_csv(\"./sample_data/enrolled.csv\", relation_name=\"enrolled\", delimiter=\",\")\n# commands = \"\"\"\n#     enrolled(\"abigail\", \"chemistry\")\n# gpa_str = \"abigail 100 jordan 80 gale 79 howard 60\"\n\n# gpa(Student,Grade) &lt;- py_rgx_string(gpa_str, \"(\\w+).*?(\\d+)\")-&gt;(Student, Grade),enrolled(Student,X)\n\n# ?gpa(X,Y)\n# \"\"\"\n# x = session.run_commands(commands)\n\nprinting results for query 'gpa(X, Y)':\n    X    |   Y\n---------+-----\n abigail | 100\n jordan  |  80\n  gale   |  79\n howard  |  60\n\n\n\n\n\n\n\n# #| export\n# #| hide\n# @patch_method\n# def import_relation_from_df(self: Session, relation_df: DataFrame, #The DataFrame containing the data to be imported\n#                             relation_name: str #The name to be assigned to the relation. It can be an existing relation or a new one\n#                             ) -&gt; None:\n#     data = relation_df.values.tolist()\n\n#     if not isinstance(data, list):\n#         raise Exception(\"dataframe could not be converted to list\")\n\n#     if len(data) &lt; 1:\n#         raise Exception(\"dataframe is empty\")\n\n#     relation_types = _infer_relation_type(data[0])\n\n#     self._add_imported_relation_to_engine(data, relation_name, relation_types)\n\n\n# show_doc(Session.import_relation_from_df)\n\n\n\nSession.import_relation_from_df\n\n Session.import_relation_from_df (relation_df:pandas.core.frame.DataFrame,\n                                  relation_name:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_df\nDataFrame\nThe DataFrame containing the data to be imported\n\n\nrelation_name\nstr\nThe name to be assigned to the relation. It can be an existing relation or a new one\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n# session = Session()\n# lecturer_df = DataFrame(([[\"walter\",\"chemistry\"], [\"linus\", \"operating_systems\"]]))\n# session.import_relation_from_df(lecturer_df, relation_name=\"lecturer\")\n# commands = \"\"\" \n# ?lecturer(X,Y)\n# \"\"\"\n# output = session.run_commands(commands)\n\nprinting results for query 'lecturer(X, Y)':\n   X    |         Y\n--------+-------------------\n walter |     chemistry\n linus  | operating_systems",
    "crumbs": [
      "src",
      "User-Interface",
      "Session"
    ]
  },
  {
    "objectID": "ast_node_types.html",
    "href": "ast_node_types.html",
    "title": "Ast Nodes Types",
    "section": "",
    "text": "for each statement node in the spannerlog grammar, this module contains a matching class that can represent that statement in the abstract syntax tree. classes representations for relations are also included.\nthese classes are useful as they represent a statement with a single instance, instead of a lark tree, thus simplifying the code required for semantic checks and manipulations of the statement.\n\nsource\n\nget_term_list_string\n\n get_term_list_string\n                       (term_list:Sequence[Union[str,spannerlib.primitive_\n                       types.Span,int]], type_list:Sequence[spannerlib.pri\n                       mitive_types.DataTypes])\n\nreturns a string representation of the term list. quotes are added to string terms so they will not be confused with variables.  @raise Exception: if length of term list doesn’t match the length of type list.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nterm_list\nSequence[DataTypeMapping.term]\nthe term list to be turned into a string\n\n\ntype_list\nSequence[DataTypes]\nthe types of the terms in term_list\n\n\nReturns\nstr\na string representation of the term list\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nterm_list = [\"X\", \"jack\", 1]\ntype_list = [DataTypes.free_var_name, DataTypes.string, DataTypes.integer]\nprint(get_term_list_string(term_list,type_list))\n\nX, \"jack\", 1\n\n\n\n\n\n\nsource\n\n\nRelationDeclaration\n\n RelationDeclaration (relation_name:str, type_list:Sequence[DataTypes])\n\na representation of a relation_declaration statement\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the relation.\n\n\ntype_list\nSequence[DataTypes]\na list of the types of the terms in the relation’s tuples.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nrel_decl = RelationDeclaration(\"parent\", [DataTypes.string, DataTypes.string])\nprint(rel_decl)\n\nparent(str, str)\n\n\n\n\n\n\nsource\n\n\nRelation\n\n Relation (relation_name:str, term_list:Sequence[DataTypeMapping.term],\n           type_list:Sequence[DataTypes])\n\na representation of a normal relation\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the relation\n\n\nterm_list\nSequence[DataTypeMapping.term]\na list of the relation terms\n\n\ntype_list\nSequence[DataTypes]\na list of the relation term types\n\n\n\n\nsource\n\n\nRelation.get_select_cols_values_and_types\n\n Relation.get_select_cols_values_and_types ()\n\n\nsource\n\n\nRelation.has_same_terms_and_types\n\n Relation.has_same_terms_and_types (other:Relation)\n\nChecks only term list and type list equivalence.\n\n\n\n\nType\nDetails\n\n\n\n\nother\n‘Relation’\nThe other relation to compare with\n\n\nReturns\nbool\nTrue if same, false otherwise\n\n\n\n\nsource\n\n\nRelation.get_index_of_free_var\n\n Relation.get_index_of_free_var (free_var:&lt;DataTypes.free_var_name:3&gt;)\n\n@raise Exception: if free_var doesn’t exist in term_list.\n\n\n\n\nType\nDetails\n\n\n\n\nfree_var\nDataTypes.free_var_name\nthe free var to search for\n\n\nReturns\nint\nthe index of free_var in term_list.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nrel1 = Relation(\"parent\", [\"Michael\", \"Jack\"], [DataTypes.string, DataTypes.string])\nprint(rel1)\n\nparent(\"Michael\", \"Jack\")\n\n\n\n\n\n\nsource\n\n\nIERelation\n\n IERelation (relation_name:str,\n             input_term_list:List[DataTypeMapping.term],\n             input_type_list:List[DataTypes],\n             output_term_list:List[DataTypeMapping.term],\n             output_type_list:List[DataTypes])\n\n*a representation of an information extraction (ie) relation. An information extraction relation is different than a normal relation as it is constructed from the results of a call to an information extraction function.\nThe ie relation instructs us on how to construct it: * its name is the ie function we need to call * its input term list represents a relation where each tuple is an argument list to call the ie function with. * its output term list represents a relation that filters the tuples that are returned from the ie function calls, and matches the values inside the tuples to free variables.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the information extraction relation\n\n\ninput_term_list\nList[DataTypeMapping.term]\na list of the input terms for the ie functiom, must be either literal values or free variables\n\n\ninput_type_list\nList[DataTypes]\na list of the term types in input_term_list\n\n\noutput_term_list\nList[DataTypeMapping.term]\na list of the output terms for the ie function, must be either literal values or free variables\n\n\noutput_type_list\nList[DataTypes]\na list of the term types in output_term_list\n\n\n\n\nsource\n\n\nIERelation.has_same_terms_and_types\n\n IERelation.has_same_terms_and_types (other:__main__.Relation)\n\nChecks that everything besides names is equivalent.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nother\nRelation\nOther relation to compare with\n\n\nReturns\nbool\nTrue if everything equivalent besides name, false otherwise\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nie_rel = IERelation(\"get_parents\", [\"X\"], [DataTypes.free_var_name], [\"Y\"], [DataTypes.free_var_name])\nprint(ie_rel)\n\nget_parents(X) -&gt; (Y)\n\n\n\n\n\n\nsource\n\n\nAddFact\n\n AddFact (relation_name:str, term_list:List[DataTypeMapping.term],\n          type_list:Sequence[DataTypes])\n\na representation of an add_fact statement inherits from relation as a fact can be defined by a relation.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the relation\n\n\nterm_list\nList[DataTypeMapping.term]\na list of the relation terms\n\n\ntype_list\nSequence[DataTypes]\na list of the relation term types\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nfact = AddFact(\"parent\", [\"Michael\", \"Jack\"], [DataTypes.string, DataTypes.string])\nprint(fact)\n\nparent(\"Michael\", \"Jack\")\n\n\n\n\n\n\nsource\n\n\nRemoveFact\n\n RemoveFact (relation_name:str, term_list:List[DataTypeMapping.term],\n             type_list:Sequence[DataTypes])\n\na representation of a remove_fact statement inherits from relation as a fact can be defined by a relation.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the relation\n\n\nterm_list\nList[DataTypeMapping.term]\na list of the relation terms\n\n\ntype_list\nSequence[DataTypes]\na list of the relation term types\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nfact = RemoveFact(\"parent\", [\"Michael\", \"Jack\"], [DataTypes.string, DataTypes.string])\nprint(fact)\n\nparent(\"Michael\", \"Jack\")\n\n\n\n\n\n\nsource\n\n\nQuery\n\n Query (relation_name:str, term_list:List[DataTypeMapping.term],\n        type_list:Sequence[DataTypes])\n\na representation of a query statement inherits from relation as a query can be defined by a relation\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the relation\n\n\nterm_list\nList[DataTypeMapping.term]\na list of the relation terms\n\n\ntype_list\nSequence[DataTypes]\na list of the relation term types\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nquery = Query(\"parent\", [\"X\", \"Y\"], [DataTypes.free_var_name, DataTypes.free_var_name])\nprint(query)\n\nparent(X, Y)\n\n\n\n\n\n\nsource\n\n\nRule\n\n Rule (head_relation:Relation,\n       body_relation_list:List[Union[Relation,IERelation]],\n       body_relation_type_list:List[str])\n\na representation of a rule statement.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nhead_relation\nRelation\nthe rule head, which is represented by a single relation\n\n\nbody_relation_list\nList[Union[Relation, IERelation]]\na list of the rule body relations\n\n\nbody_relation_type_list\nList[str]\na list of the rule body relations types (e.g. “relation”, “ie_relation”)\n\n\n\n\nsource\n\n\nRule.get_relations_by_type\n\n Rule.get_relations_by_type ()\n\ncategorizes relations into two sets based on their types, distinguishing between regular relations and information-extraction relations.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nrule = Rule(\n        \"rule\",\n        [Relation(\"Person\", [\"X\", \"Y\"], [DataTypes.free_var_name, DataTypes.free_var_name])],\n        [\"Relation\"]\n    )\nprint(rule)\n\nrule &lt;- Person(X, Y)\n\n\n\n\n\n\nsource\n\n\nAssignment\n\n Assignment (var_name:str, value:DataTypeMapping.term,\n             value_type:DataTypes)\n\na representation of an assignment statement.\n\n\n\n\nType\nDetails\n\n\n\n\nvar_name\nstr\nthe variable name to be assigned a value\n\n\nvalue\nDataTypeMapping.term\nthe assigned value\n\n\nvalue_type\nDataTypes\nthe assigned value’s type\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nassignment = Assignment(\"var1\",10,int)\nprint(assignment)\n\nvar1 = 10\n\n\n\n\n\n\nsource\n\n\nReadAssignment\n\n ReadAssignment (var_name:str, read_arg:str, read_arg_type:type)\n\na representation of a read_assignment statement.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nvar_name\nstr\nthe variable name to be assigned a value\n\n\nread_arg\nstr\nthe argument that is passed to the read() function (e.g. “some_file” in s = read(\"some_file\"))\n\n\nread_arg_type\ntype\nthe type of the argument that is passed to the read function\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nread_assignment = ReadAssignment(\"s\", \"file_name\", DataTypes.string)\nprint(read_assignment)\n\ns = read(\"file_name\")",
    "crumbs": [
      "src",
      "Graphs",
      "Ast Nodes Types"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Note\n\n\n\nthis project is built with nbdev, which is a full literate programming environment built on Jupyter Notebooks. That means that every piece of documentation, including the page you’re reading now, can be accessed as interactive Jupyter notebook.",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#using-spannerlog",
    "href": "introduction.html#using-spannerlog",
    "title": "Introduction",
    "section": "Using spannerlog",
    "text": "Using spannerlog\n\nInstallation\nprerequisites:\n\nHave Python version 3.8 or above installed\n\nTo download and install spannerlog run the following commands in your terminal:\ngit clone https://github.com/DeanLight/spannerlib\ncd spannerlib\npip install . \nMake sure you are calling the pip version of your current python environment. To install with another python interpreter, run\n&lt;path_to_python_interpreter&gt; -m pip install .\nYou can also install spannerlog in the current Jupyter kernel: \n!git clone https://github.com/DeanLight/spannerlib\n!pip install spannerlib\nIn order to use spannerlog in jupyter notebooks, you must first load it:\n\nimport spannerlib\n\nImporting the spannerlog library automatically loads the %spannerlog and %%spannerlog cell magics which accepts spannerlog queries as shown below.\nuse %spannerlog to run a single line, and %%spannerlog to run a block of code:\n\n%spannerlog new relation(str)\nprint(\"this is python code\")\n\nthis is python code\n\n\n\n%%spannerlog\nnew uncle(str, str)\nuncle(\"bob\", \"greg\")\n?uncle(X,Y)\n\nprinting results for query 'uncle(X, Y)':\n  X  |  Y\n-----+------\n bob | greg",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#functional-regex-formulas",
    "href": "introduction.html#functional-regex-formulas",
    "title": "Introduction",
    "section": "Functional regex formulas",
    "text": "Functional regex formulas\nspannerlog contains IE functions which are registered by default. Let’s go over a couple regex IE functions:\nrgx_span(regex_input ,regex_formula)-&gt;(x_1, x_2, ...,x_n)\nand\nrgx_string(regex_input ,regex_formula)-&gt;(x_1, x_2, ...,x_n)\nwhere: * regex_input is the string that the regex operation will be performed on * regex_formula is either a string literal or a string variable that represents your regular expression. * x_1, x_2, … x_n can be either constant terms or free variable terms. They’re used to construct the tuples of the resulting relation. the number of terms has to be the same as the number of capture groups used in regex_formula. If not capture groups are used, then each returned tuple includes a single, whole regex match, so only one term should be used.\nThe only difference between the rgx_span and rgx_string ie functions, is that rgx_string returns strings, while rgx_span returns the spans of those strings. This also means that if you want to use constant terms as return values, they have to be spans if you use rgx_span, and strings if you use rgx_string\nFor example consider the following spannerlog code:\n\n%%spannerlog\ninput_string = \"John Doe: 35 years old, Jane Smith: 28 years old\"\nregex_pattern = \"(\\w+\\s\\w+):\\s(\\d+)\"\n\nage(X,Y) &lt;- py_rgx_string(input_string, regex_pattern) -&gt; (X,Y)\nage_span(X,Y) &lt;- py_rgx_span(input_string, regex_pattern) -&gt; (X,Y)\n?age(X,Y)\n?age_span(X,Y)\n\nprinting results for query 'age(X, Y)':\n     X      |   Y\n------------+-----\n  John Doe  |  35\n Jane Smith |  28\n\nprinting results for query 'age_span(X, Y)':\n    X     |    Y\n----------+----------\n  [0, 8)  | [10, 12)\n [24, 34) | [36, 38)\n\n\n\nThe variables X,Y in the output of the above ie functions are the matches of the capture groups used in the regex_pattern.  capture groups allow us to extract specific parts of a matched pattern in a text using regular expressions.  When you define a regular expression pattern with parentheses (), you create a capturing group",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#creating-and-registering-a-new-ie-function",
    "href": "introduction.html#creating-and-registering-a-new-ie-function",
    "title": "Introduction",
    "section": "Creating and Registering a New IE Function",
    "text": "Creating and Registering a New IE Function\nUsing regex is nice, but what if you want to define your own IE function?  spannerlog allows you to define and use your own information extraction functions. You can use them only in rule bodies in the current version. The following is the syntax for custom IE functions:\nfunc(term_1,term_2,...term_n)-&gt;(x_1, x_2, ..., x_n)\nwhere: * func is a IE function that was previously defined and registered * term_1,term_2,…,term_n are the parameters for func * x_1, … x_n could be any type of terms, and are used to construct tuples of the resulting relation\nFor example:\n\nIE function get_happy\n\nimport re\nfrom spannerlib.primitive_types import DataTypes\n\n# the function itself, which should yield an iterable of primitive types\ndef get_happy(text):\n    \"\"\"\n    get the names of people who are happy in `text`\n    \"\"\"\n    compiled_rgx = re.compile(\"(\\w+) is happy\")\n    num_groups = compiled_rgx.groups\n    for match in re.finditer(compiled_rgx, text):\n        if num_groups == 0:\n            matched_strings = [match.group()]\n        else:\n            matched_strings = [group for group in match.groups()]\n        yield matched_strings\n\n# the input types, a list of primitive types\nget_happy_in_types = [DataTypes.string]\n\n# the output types, either a list of primitive types or \n# a method which expects an arity and computes the desired types based on it\nget_happy_out_types = lambda arity : arity * [DataTypes.string]\n# or: `get_happy_out_types = [DataTypes.string]`s\n\n# finally, register the function\nmagic_session.register(ie_function=get_happy,\n                       ie_function_name = \"get_happy\",\n                       in_rel=get_happy_in_types,\n                       out_rel=get_happy_out_types)\n\nYou may have noticed that when we register a custom ie function, we use yield instead of return,  and that is because part of making spanner based database systems more performant and memory efficient is to do lazy evaluation,  since building iterators in python is very simple using the generator pattern, we made the ie functions into generators to allow ie functions to also be as lazy as their author desires.\n\n\ncustom IE using get_happy\n\n%%spannerlog\nnew grandmother(str, str)\ngrandmother(\"rin\", \"alice\")\ngrandmother(\"denna\", \"joel\")\nsentence = \"rin is happy, denna is sad.\"\n# note that this statement will fail as 'get_happy' is not registered as an ie_function\nhappy_grandmother(X) &lt;- grandmother(X,Z),get_happy(sentence)-&gt;(X)\n?happy_grandmother(X) # assuming get_happy returned \"rin\", also returns \"rin\"\n\nprinting results for query 'happy_grandmother(X)':\n  X\n-----\n rin",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#more-information-about-ie-functions",
    "href": "introduction.html#more-information-about-ie-functions",
    "title": "Introduction",
    "section": "More information about IE functions",
    "text": "More information about IE functions\n\nYou can remove an IE function via the session:\n\nmagic_session.remove_ie_function(ie_function_name)\n\nIf you want to remove all the registered ie functions:\n\nmagic_session.remove_all_ie_functions()\n\nIf you register an IE function with a name that was already registered before, the old IE function will be overwitten by the new one. \nYou can inspect all the registered IE functions using the following command:\n\nmagic_session.print_registered_ie_functions()\n# first, let's print all functions:\nmagic_session.print_registered_ie_functions()\nanother tremendous triumph! Coref was deleted from the registered functions",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#matching-outputs",
    "href": "introduction.html#matching-outputs",
    "title": "Introduction",
    "section": "Matching Outputs:",
    "text": "Matching Outputs:\nLet’s write a spannerlog program that gets a table in which each row is a single string - string(str).  The program will create a new table in which each row is a string and its length.\n\nFirst try:\n\n# Step 1: implement an IE function\ndef length(string):\n    #here we append the input to the output inside the ie function!\n    yield len(string), string\n\nmagic_session.register(length, \"Length\", [DataTypes.string], [DataTypes.integer, DataTypes.string])\n\n\n%%spannerlog\n# Let's test this solution:\nnew string(str)\nstring(\"a\")\nstring(\"ab\")\nstring(\"abc\")\nstring(\"abcd\")\n\nstring_length(Str, Len) &lt;- string(Str), Length(Str) -&gt; (Len, Str)\n?string_length(Str, Len)\n\nprinting results for query 'string_length(Str, Len)':\n  Str  |   Len\n-------+-------\n   a   |     1\n  ab   |     2\n  abc  |     3\n abcd  |     4\n\n\n\n\n\nIt works\nOur first IE function yield the input in addition to the output. This will ensure that we will get the right output to his input. But, is this really necessary? Let’s try another solution:\n\n#here we don't append the input to the output inside the ie function!\ndef length2(string):\n   yield len(string),\n\n# Step 2: register the function\nmagic_session.register(length2, \"Length2\", [DataTypes.string], [DataTypes.integer])\n\n\n%%spannerlog\n# Let's test this solution:\nnew rel(str)\nrel(\"a\")\nrel(\"ab\")\nrel(\"abc\")\nrel(\"abcd\")\n\nstring_length(Str, Len) &lt;- rel(Str), Length2(Str) -&gt; (Len)\n?string_length(Str, Len)\n\nprinting results for query 'string_length(Str, Len)':\n  Str  |   Len\n-------+-------\n   a   |     1\n  ab   |     2\n  abc  |     3\n abcd  |     4\n\n\n\n\n\nIt looks good, but why?\nFirst we can see that the IE function yield only an output without any trace to the input. In addition, spannerlog stores all the inputs of each IE function in an input table and all the outputs in an output table. Then it’s joining the input table with the output table. So, why we still got the right solution? This thanks to the fact that spannerlog stores the input bounded to it’s output deductively.",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#logical-operators",
    "href": "introduction.html#logical-operators",
    "title": "Introduction",
    "section": "Logical Operators:",
    "text": "Logical Operators:\nSuppose we have a table in which each row contains two strings - pair(str, str). Our goal is to filter all the rows that contain the same value twice.  In other words, we want to implement the relation not equals (NEQ).\nWe would like to have a rule such as:  unique_pair(X, Y) &lt;- pair(X, Y), X != Y  Unfortunately spannerlog doesn’t support True/False values. Therefore, we can’t use X != Y.  Our solution to this problem is to create an ie function that implements NEQ relation:\n\ndef NEQ(x, y):\n    if x == y:\n        # return false (empty tuple represents false)\n        yield tuple() \n    else:\n        #return true\n        yield x, y\n\nin_out_types = [DataTypes.string, DataTypes.string]\nmagic_session.register(NEQ, \"NEQ\", in_out_types, in_out_types)\n\n\n%%spannerlog\n#Lets test this solution\nnew pair(str, str)\npair(\"Dan\", \"Tom\")\npair(\"Cat\", \"Dog\")\npair(\"Apple\", \"Apple\")\npair(\"Cow\", \"Cow\")\npair(\"123\", \"321\")\n\nunique_pair(X, Y) &lt;- pair(X, Y), NEQ(X, Y) -&gt; (X, Y)\n?unique_pair(X, Y)\n\nprinting results for query 'unique_pair(X, Y)':\n  X  |  Y\n-----+-----\n Dan | Tom\n Cat | Dog\n 123 | 321",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#python",
    "href": "introduction.html#python",
    "title": "Introduction",
    "section": "python",
    "text": "python\n\nimport re\nenrolled = \"dave chemistry dave biology rem biology ram biology emilia physics roswaal chemistry roswaal biology roswaal physics\"\ngrades = \"dave 80 rem 66 ram 66 roswaal 100 emilia 88\"\n\nenrolled_pairs = re.findall(r\"(\\w+).*?(\\w+)\", enrolled)\ngrade_pairs = re.findall(r\"(\\w+).*?(\\d+)\", grades)\nfor student1, course1 in enrolled_pairs:\n    for student2, course2 in enrolled_pairs:\n        for student3, grade in grade_pairs:\n            if (student1 == student2 == student3):\n                if (course1 == \"biology\" and course2 == \"chemistry\" and int(grade) == 80):\n                    print(student1)\n\ndave",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#spannerlog",
    "href": "introduction.html#spannerlog",
    "title": "Introduction",
    "section": "spannerlog",
    "text": "spannerlog\n\n%%spannerlog\nenrolled = \"dave chemistry dave biology rem biology ram biology emilia physics roswaal chemistry roswaal biology roswaal physics\"\ngrades = \"dave 80 rem 66 ram 66 roswaal 100 emilia 88\"\n\nenrolled_in(Student, Course) &lt;- py_rgx_string(enrolled, \"(\\w+).*?(\\w+)\")-&gt;(Student, Course)\nstudent_grade(Student, Grade) &lt;- py_rgx_string(grades, \"(\\w+).*?(\\d+)\") -&gt; (Student, Grade)\ninteresting_student(X) &lt;- enrolled_in(X, \"biology\"), enrolled_in(X, \"chemistry\"), student_grade(X, \"80\")\n?interesting_student(X)\n\nprinting results for query 'interesting_student(X)':\n  X\n------\n dave\n\n\n\nin this case, the python implementation was long and unnatural. on the other hand, the spannerlog implementation was cleaner and allowed us to express our intentions directly, rather than dealing with annoying programming logic.",
    "crumbs": [
      "Tutorials",
      "Introduction"
    ]
  },
  {
    "objectID": "advanced_usage.html",
    "href": "advanced_usage.html",
    "title": "Advanced Usage",
    "section": "",
    "text": "When spannerlog is loaded, a default session (spannerlog.magic_session) is created behind the scenes. This is the session that %%spannerlog uses.\nUsing a session manually enables one to dynamically generate queries, facts, and rules\n\nimport spannerlib\nsession = spannerlib.magic_session\n\n\nresult = session.run_commands('''\n    new uncle(str, str)\n    uncle(\"benjen\", \"jon\")\n                              ''')\n\n\nfor maybe_uncle in ['ned', 'robb', 'benjen']:\n    result = session.run_commands(f'?uncle(\"{maybe_uncle}\",Y)')\n\nprinting results for query 'uncle(\"ned\", Y)':\n[]\n\nprinting results for query 'uncle(\"robb\", Y)':\n[]\n\nprinting results for query 'uncle(\"benjen\", Y)':\n  Y\n-----\n jon",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "advanced_usage.html#default-session",
    "href": "advanced_usage.html#default-session",
    "title": "Advanced Usage",
    "section": "",
    "text": "When spannerlog is loaded, a default session (spannerlog.magic_session) is created behind the scenes. This is the session that %%spannerlog uses.\nUsing a session manually enables one to dynamically generate queries, facts, and rules\n\nimport spannerlib\nsession = spannerlib.magic_session\n\n\nresult = session.run_commands('''\n    new uncle(str, str)\n    uncle(\"benjen\", \"jon\")\n                              ''')\n\n\nfor maybe_uncle in ['ned', 'robb', 'benjen']:\n    result = session.run_commands(f'?uncle(\"{maybe_uncle}\",Y)')\n\nprinting results for query 'uncle(\"ned\", Y)':\n[]\n\nprinting results for query 'uncle(\"robb\", Y)':\n[]\n\nprinting results for query 'uncle(\"benjen\", Y)':\n  Y\n-----\n jon",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "advanced_usage.html#changing-the-session-of-the-magic-cells",
    "href": "advanced_usage.html#changing-the-session-of-the-magic-cells",
    "title": "Advanced Usage",
    "section": "Changing the session of the magic cells",
    "text": "Changing the session of the magic cells\nIn cases where you want to work with a custom session, but still make use of the magic system, you can overide the session used by the magic system\n\nimport spannerlib  # default session starts here\nfrom spannerlib import Session\n\nanother_session=Session()\nold_magic_session = spannerlib.magic_session\nspannerlib.magic_session = another_session\n\n\n%%spannerlog\n# we're now using the new session\nnew uncle(str, str)\nuncle(\"bob\", \"greg\")\n?uncle(X,Y)\n\nprinting results for query 'uncle(X, Y)':\n  X  |  Y\n-----+------\n bob | greg\n\n\n\n\n# back to the old session\nspannerlib.magic_session = old_magic_session\n%spannerlog uncle(\"jim\", \"dwight\")\n\n\nprint(spannerlib.magic_session._parse_graph)\nprint(another_session._parse_graph)\n\n(__spannerlog_root) (computed) root\n    (0) (computed) relation_declaration: uncle(str, str)\n    (1) (computed) add_fact: uncle(\"benjen\", \"jon\")\n    (2) (computed) query: uncle(\"ned\", Y)\n    (3) (computed) query: uncle(\"robb\", Y)\n    (4) (computed) query: uncle(\"benjen\", Y)\n    (5) (computed) add_fact: uncle(\"jim\", \"dwight\")\n\n(__spannerlog_root) (computed) root\n    (0) (computed) relation_declaration: uncle(str, str)\n    (1) (computed) add_fact: uncle(\"bob\", \"greg\")\n    (2) (computed) query: uncle(X, Y)",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "advanced_usage.html#mixing-magics-with-dynamic-session-calls",
    "href": "advanced_usage.html#mixing-magics-with-dynamic-session-calls",
    "title": "Advanced Usage",
    "section": "Mixing magics with dynamic session calls",
    "text": "Mixing magics with dynamic session calls\nLets take the GPA example from the introductory tutorial. What if we want to have multiple rules each looking for GPAs of students in different classes. We wouldnt want to manually write a rule for every single subject.\n\npython-spannerlog interface functions\nwe can either write our data manually, or import it from a csv/dataframe:\n\n%%spannerlog\nnew lecturer(str, str)\nlecturer(\"rick\", \"physics\")\n\n\nfrom pandas import DataFrame\nlecturer_df = DataFrame(([[\"walter\",\"chemistry\"], [\"linus\", \"operating_systems\"]]))\nsession.import_rel(lecturer_df, relation_name=\"lecturer\")\n\n\nsession.import_rel(\"sample_data/enrolled.csv\", relation_name=\"enrolled\", delimiter=\",\")\n\n\n%%spannerlog\nenrolled(\"abigail\", \"chemistry\")\ngpa_str = \"abigail 100 jordan 80 gale 79 howard 60\"\n\ngpa(Student,Grade) &lt;- py_rgx_string(gpa_str, \"(\\w+).*?(\\d+)\")-&gt;(Student, Grade),enrolled(Student,X)\n\n?gpa(X,Y)\n\nprinting results for query 'gpa(X, Y)':\n    X    |   Y\n---------+-----\n abigail | 100\n jordan  |  80\n  gale   |  79\n howard  |  60\n\n\n\n\n\nusing spannerlog in python loops\nNow we are going to define the rules using a for loop\n\nsubjects = [\n    \"chemistry\",\n    \"physics\",\n    \"operation_systems\",\n    \"magic\",\n]\n\nfor subject in subjects:\n    rule = f\"\"\"\n    gpa_of_{subject}_students(Student, Grade) &lt;- gpa(Student, Grade), enrolled(Student, \"{subject}\")\n    \"\"\"\n    session.run_commands(rule)\n    print(rule)  # we print the rule here to show you what strings are sent to the session\n\n\n    gpa_of_chemistry_students(Student, Grade) &lt;- gpa(Student, Grade), enrolled(Student, \"chemistry\")\n    \n\n    gpa_of_physics_students(Student, Grade) &lt;- gpa(Student, Grade), enrolled(Student, \"physics\")\n    \n\n    gpa_of_operation_systems_students(Student, Grade) &lt;- gpa(Student, Grade), enrolled(Student, \"operation_systems\")\n    \n\n    gpa_of_magic_students(Student, Grade) &lt;- gpa(Student, Grade), enrolled(Student, \"magic\")\n    \n\n\nAs you can see, we can use the dynamically defined rules in a magic cell\n\n%%spannerlog\n?gpa_of_operation_systems_students(X,Y)\n\nprinting results for query 'gpa_of_operation_systems_students(X, Y)':\n[]\n\n\n\nAnd we can also query dynamically\n\nsubjects = [\n    \"chemistry\",\n    \"physics\",\n    \"operation_systems\",\n    \"magic\",\n]\n\nfor subject in subjects:\n    query = f\"\"\"\n    ?gpa_of_{subject}_students(Student, Grade)\n    \"\"\"\n    session.run_commands(query)\n\nprinting results for query 'gpa_of_chemistry_students(Student, Grade)':\n  Student  |   Grade\n-----------+---------\n  abigail  |     100\n  jordan   |      80\n  howard   |      60\n\nprinting results for query 'gpa_of_physics_students(Student, Grade)':\n  Student  |   Grade\n-----------+---------\n  howard   |      60\n\nprinting results for query 'gpa_of_operation_systems_students(Student, Grade)':\n[]\n\nprinting results for query 'gpa_of_magic_students(Student, Grade)':\n[]",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "advanced_usage.html#creating-rules-dynamically",
    "href": "advanced_usage.html#creating-rules-dynamically",
    "title": "Advanced Usage",
    "section": "Creating rules Dynamically",
    "text": "Creating rules Dynamically\nhere’s a more complicated example where we create spannerlog code dynamically:\n\nfrom spannerlib import magic_session\n\n%spannerlog new sibling(str, str)\n%spannerlog new parent(str, str)\n%spannerlog parent(\"jonathan\", \"george\")\n%spannerlog parent(\"george\", \"joseph\")\n%spannerlog parent(\"joseph\", \"holy\")\n%spannerlog parent(\"holy\", \"jotaro\")\n%spannerlog sibling(\"dio\", \"jonathan\")\n\na = [\"parent\", \"uncle_aunt\", \"grandparent\", \"sibling\"]\nd = {\"uncle_aunt\": [\"sibling\", \"parent\"], \"grandparent\": [\"parent\", \"parent\"], \"great_aunt_uncle\": [\"sibling\", \"parent\", \"parent\"]}\n\nfor key, steps in d.items():\n    # add the start of the rule\n    result = key + \"(A,Z) &lt;- \"\n    for num, step in enumerate(steps):\n        # for every step in the list, add the condition: step(letter, next letter).\n        #  the first letter is always `A`, and the last is always `Z`\n        curr_letter = chr(num + ord(\"A\"))\n        result += step + \"(\" + curr_letter + \",\"\n        if (num == len(steps) - 1):\n            result += \"Z)\"\n        else:\n            result += chr(1 + ord(curr_letter)) + \"), \"\n    print(\"running:\", result)\n    magic_session.run_commands(result)\n    magic_session.run_commands(f\"?{key}(X,Y)\")\n\nrunning: uncle_aunt(A,Z) &lt;- sibling(A,B), parent(B,Z)\nprinting results for query 'uncle_aunt(X, Y)':\n  X  |   Y\n-----+--------\n dio | george\n\nrunning: grandparent(A,Z) &lt;- parent(A,B), parent(B,Z)\nprinting results for query 'grandparent(X, Y)':\n    X     |   Y\n----------+--------\n jonathan | joseph\n  george  |  holy\n  joseph  | jotaro\n\nrunning: great_aunt_uncle(A,Z) &lt;- sibling(A,B), parent(B,C), parent(C,Z)\nprinting results for query 'great_aunt_uncle(X, Y)':\n  X  |   Y\n-----+--------\n dio | joseph",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "advanced_usage.html#processing-the-result-of-a-query-in-python-and-using-the-result-in-a-new-query",
    "href": "advanced_usage.html#processing-the-result-of-a-query-in-python-and-using-the-result-in-a-new-query",
    "title": "Advanced Usage",
    "section": "Processing the result of a query in python and using the result in a new query",
    "text": "Processing the result of a query in python and using the result in a new query\nwe can add format_results=True to run_statements to get the output as one of the following: 1. [], if the result is false, 2. [tuple()], if the result if true (the tuple is empty), or 3. pandas.DataFrame, otherwise-\n\nresults = session.run_commands(f'''\n    new friends(str, str, str)\n    friends(\"bob\", \"greg\", \"clyde\")\n    friends(\"steven\", \"benny\", \"horace\")\n    friends(\"lenny\", \"homer\", \"toby\")\n    ?friends(X,Y,Z)''', print_results=False, format_results=True)\n\n# now we'll showcase processing the result with native python...\n# lets filter our tuples with some predicate:\nres = results[0].values.tolist()\nfiltered = tuple(filter(lambda friends: 'bob' in friends or 'lenny' in friends, res))\n\n# and feed the matching tuples into a new query:\nsession.run_commands('new buddies(str, str)')\n\nfor first, second, _ in filtered:\n    session.run_commands(f'buddies(\"{first}\", \"{second}\")')\n\nresult = session.run_commands(\"?buddies(First, Second)\")\n\nprinting results for query 'buddies(First, Second)':\n  First  |  Second\n---------+----------\n   bob   |   greg\n  lenny  |  homer",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "advanced_usage.html#import-a-relation-from-a-dataframe",
    "href": "advanced_usage.html#import-a-relation-from-a-dataframe",
    "title": "Advanced Usage",
    "section": "Import a relation from a DataFrame",
    "text": "Import a relation from a DataFrame\nBy default, non-boolean query results are saved as a DataFrame. A relation can also be imported from a DataFrame, like this:\n\nfrom pandas import DataFrame\n\ndf = DataFrame([[\"Shrek\",42], [\"Fiona\", 1337]], columns=[\"name\", \"number\"])\nsession.import_rel(df, relation_name=\"ogres\")\n%spannerlog ?ogres(X,Y)\n\nprinting results for query 'ogres(X, Y)':\n   X   |    Y\n-------+------\n Shrek |   42\n Fiona | 1337",
    "crumbs": [
      "Tutorials",
      "Advanced Usage"
    ]
  },
  {
    "objectID": "ie_func/rust_spanner_regex.html",
    "href": "ie_func/rust_spanner_regex.html",
    "title": "Rust spanner regex",
    "section": "",
    "text": "source\n\nrgx\n\n rgx (regex_pattern:str, out_type:str, text:Union[str,NoneType]=None,\n      text_file:Union[str,NoneType]=None)\n\nAn IE function which runs regex using rust’s enum-spanner-rs and yields tuples of strings/spans (not both).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nregex_pattern\nstr\n\nthe pattern to run\n\n\nout_type\nstr\n\nstring/span - decides which one will be returned\n\n\ntext\nOptional[str]\nNone\nthe string on which regex is run\n\n\ntext_file\nOptional[str]\nNone\nuse text from this file instead of text. default: None\n\n\nReturns\nIterable[Iterable[Union[str, Span]]]\n\na tuple of strings/spans\n\n\n\n\nsource\n\n\nrgx_span\n\n rgx_span (text:str, regex_pattern:str)\n\n*computes the spans of all the matches of the regex pattern in the text\nbased on the enum-spanner-rs rust package*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe input text for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\nIterable[Iterable[Union[str, Span]]]\ntuples of spans that represents the results\n\n\n\n\nsource\n\n\nrgx_string\n\n rgx_string (text:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe input text for the regex operation\n\n\nregex_pattern\nstr\nhe pattern of the regex operation\n\n\nReturns\nIterable[Iterable[Union[str, Span]]]\ntuples of strings that represents the results\n\n\n\n\nsource\n\n\nrgx_span_from_file\n\n rgx_span_from_file (text_file:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext_file\nstr\nThe input file for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\nIterable[Iterable[Union[str, Span]]]\ntuples of spans that represents the results\n\n\n\n\nsource\n\n\nrgx_string_from_file\n\n rgx_string_from_file (text_file:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext_file\nstr\nThe input file for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\nIterable[Iterable[Union[str, Span]]]\ntuples of strings that represents the results"
  },
  {
    "objectID": "ie_func/nlp.html",
    "href": "ie_func/nlp.html",
    "title": "nlp",
    "section": "",
    "text": "source\n\ndownload_and_install_nlp\n\n download_and_install_nlp ()\n\n\nsource\n\n\ntokenize_wrapper\n\n tokenize_wrapper (sentence:str)\n\n\nsource\n\n\nssplit_wrapper\n\n ssplit_wrapper (sentence:str)\n\n\nsource\n\n\npos_wrapper\n\n pos_wrapper (sentence:str)\n\n\nsource\n\n\nlemma_wrapper\n\n lemma_wrapper (sentence:str)\n\n\nsource\n\n\nner_wrapper\n\n ner_wrapper (sentence:str)\n\n\nsource\n\n\nentitymentions_wrapper\n\n entitymentions_wrapper (sentence:str)\n\n\nsource\n\n\nregexner_wrapper\n\n regexner_wrapper (sentence:str, pattern:str)\n\n\nsource\n\n\ntokensregex_wrapper\n\n tokensregex_wrapper (sentence:str, pattern:str)\n\n\nsource\n\n\ncleanxml_wrapper\n\n cleanxml_wrapper (sentence:str)\n\n\nsource\n\n\nparse_wrapper\n\n parse_wrapper (sentence:str)\n\n\nsource\n\n\ndependency_parse_wrapper\n\n dependency_parse_wrapper (sentence:str)\n\n\nsource\n\n\ncoref_wrapper\n\n coref_wrapper (sentence:str)\n\n\nsource\n\n\nopenie_wrapper\n\n openie_wrapper (sentence:str)\n\n\nsource\n\n\nkbp_wrapper\n\n kbp_wrapper (sentence:str)\n\n\nsource\n\n\nquote_wrapper\n\n quote_wrapper (sentence:str)\n\n\nsource\n\n\nsentiment_wrapper\n\n sentiment_wrapper (sentence:str)\n\n\nsource\n\n\ntruecase_wrapper\n\n truecase_wrapper (sentence:str)\n\n\nsource\n\n\nudfeats_wrapper\n\n udfeats_wrapper (sentence:str)"
  },
  {
    "objectID": "stdlib.html",
    "href": "stdlib.html",
    "title": "Standard library ie functions",
    "section": "",
    "text": "Here are the current standard ie functions that are registered in spannerlog sessions. This section is still a WIP.\n\n\n\nsource\n\npy_rgx\n\n py_rgx (text:str, regex_pattern:str)\n\nAn IE function which runs regex using python’s re and yields tuples of spans.\n@param text: The input text for the regex operation. @param regex_pattern: the pattern of the regex operation. @return: tuples of spans that represents the results.\n\n\n\n\nsource\n\npy_rgx_string\n\n py_rgx_string (text:str, regex_pattern:str)\n\nAn IE function which runs regex using python’s re and yields tuples of strings.\n@param text: The input text for the regex operation. @param regex_pattern: the pattern of the regex operation. @return: tuples of strings that represents the results.\n\n\n\n\nsource\n\nrgx_span\n\n rgx_span (text:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe input text for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\ntyping.Iterable[typing.Iterable[typing.Union[str, spannerlib.primitive_types.Span]]]\ntuples of spans that represents the results\n\n\n\n\n\n\n\nsource\n\nrgx_string\n\n rgx_string (text:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe input text for the regex operation\n\n\nregex_pattern\nstr\nhe pattern of the regex operation\n\n\nReturns\ntyping.Iterable[typing.Iterable[typing.Union[str, spannerlib.primitive_types.Span]]]\ntuples of strings that represents the results\n\n\n\n\n\n\n\nsource\n\nrgx_span_from_file\n\n rgx_span_from_file (text_file:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext_file\nstr\nThe input file for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\ntyping.Iterable[typing.Iterable[typing.Union[str, spannerlib.primitive_types.Span]]]\ntuples of spans that represents the results\n\n\n\n\n\n\n\nsource\n\nrgx_string_from_file\n\n rgx_string_from_file (text_file:str, regex_pattern:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntext_file\nstr\nThe input file for the regex operation\n\n\nregex_pattern\nstr\nThe pattern of the regex operation\n\n\nReturns\ntyping.Iterable[typing.Iterable[typing.Union[str, spannerlib.primitive_types.Span]]]\ntuples of strings that represents the results\n\n\n\n\n\n\n\nsource\n\njson_path\n\n json_path (json_document:str, path_expression:str)\n\n@param json_document: The document on which we will run the path expression. @param path_expression: The query to execute. @return: json documents.\n\n\n\n\nsource\n\njson_path_full\n\n json_path_full (json_document:str, path_expression:str)\n\n@param json_document: The document on which we will run the path expression. @param path_expression: The query to execute. @return: json documents with the full results paths.\n\n\n\n\nsource\n\ntokenize_wrapper\n\n tokenize_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nssplit_wrapper\n\n ssplit_wrapper (sentence:str)\n\n\n\n\n\nsource\n\npos_wrapper\n\n pos_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nlemma_wrapper\n\n lemma_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nner_wrapper\n\n ner_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nentitymentions_wrapper\n\n entitymentions_wrapper (sentence:str)\n\n\n\n\n\nsource\n\ncleanxml_wrapper\n\n cleanxml_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nparse_wrapper\n\n parse_wrapper (sentence:str)\n\n\n\n\n\nsource\n\ndependency_parse_wrapper\n\n dependency_parse_wrapper (sentence:str)\n\n\n\n\n\nsource\n\ncoref_wrapper\n\n coref_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nopenie_wrapper\n\n openie_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nkbp_wrapper\n\n kbp_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nquote_wrapper\n\n quote_wrapper (sentence:str)\n\n\n\n\n\nsource\n\nsentiment_wrapper\n\n sentiment_wrapper (sentence:str)\n\n\n\n\n\nsource\n\ntruecase_wrapper\n\n truecase_wrapper (sentence:str)",
    "crumbs": [
      "Tutorials",
      "Standard library ie functions"
    ]
  },
  {
    "objectID": "primitive_types.html",
    "href": "primitive_types.html",
    "title": "Primitive Types",
    "section": "",
    "text": "this module contains the ‘DataTypes’ enum that is used to represents the types of variables or terms in the ast, term graph and symbol table\nthis module also contains class representations of complex datatypes (e.g. Span which is represented by two numbers)\n\nsource\n\nDataTypes\n\n DataTypes (value, names=None, module=None, qualname=None, type=None,\n            start=1)\n\nAn enum class that represents the types of spannerlog\nThe current supported types are:  string  span: A Span identifies a substring of a string by specifying its bounding indices. It is constructed from two integers. [a,b)  integer  free_var_name  var_name \n\nsource\n\n\nSpan\n\n Span (span_start:int, span_end:int)\n\nA representation of a span\n\n\n\n\nType\nDetails\n\n\n\n\nspan_start\nint\nthe first (included) index of the span.\n\n\nspan_end\nint\nthe last (excluded) index of the span.\n\n\n\n\nsource\n\n\nDataTypeMapping\n\n DataTypeMapping ()\n\nThe DataTypeMapping class Maps between the supported DataTypes to their object type",
    "crumbs": [
      "src",
      "Generic",
      "Primitive Types"
    ]
  },
  {
    "objectID": "covid-nlp/covid_pipeline.html",
    "href": "covid-nlp/covid_pipeline.html",
    "title": "Covid-19 NLP Pipeline",
    "section": "",
    "text": "In this tutorial, we will guide you through the process of re-writing an existing data pipeline into the spannerlog framework, allowing you to witness a real-world example of the framework’s benefits. And to offer further tutorials on advanced applications of the spannerlog framework. We’ve chosen to adapt a pipeline from the field of NLP, specifically the Covid-19 NLP pipeline, which was a part of a published paper in 2020.\n\n\nThe pipline repository link.\nThe primary objective of the NLP pipeline is to identify individuals who have been positively diagnosed with COVID-19 by extracting pertinent information from unstructured free-text narratives found within the Electronic Health Record (EHR) of the Department of Veterans Affairs (VA). By automating this process, the pipeline streamlines the screening of a substantial volume of clinical text, significantly reducing the time and effort required for identification. The pipeline is built on medSpacy framework, and defines a new UI to use.\n\n\n\n\n\n\npipeline_flowchart\n\n\n\npreprocessor: Modifies the preprocessed text\nconcept tagger: Assigns a semantic tag in a custom attribute to each Token, which helps with concept extraction and normalization.\ntarget matcher: Extracts spans using rules, based on the concept tagger.\nsectionizer: Identifies note section headers in the text and assigns section titles to entities and tokens contained in that section.\ncontext: Identifies semantic modifiers of entities and asserts attributes such as positive status, negation, and other experiencier.\npostprocessor: Modifies or removes the entity based on business logic. This handles special cases or complex logic using the results of earlier stages.\ndocument classifier: Assigns a label of “POS”, “UNK”, or “NEG” to each file, A document will be classified as positive if it has at least one positive, non-excluded entity We will explain about each stage in more details later on.\n\nAt each stage, we encountered unique challenges. Each stage involved working with different classes and medSpacy framework attributes. Our approach was to first understand the original implementation, seek opportunities to simplify it, and then rewrite it in spannerlog. What was possible we wrote it declaratively using rules, facts, and queries. Additionally, if any essential functionalities were missing in the library, we built IE functions. As we mentioned the pipline uses medSpacy framework so first we need to import some libraries and install some requirements.",
    "crumbs": [
      "Tutorials",
      "Covid-19 NLP Pipeline"
    ]
  },
  {
    "objectID": "covid-nlp/covid_pipeline.html#tutorial-introduction",
    "href": "covid-nlp/covid_pipeline.html#tutorial-introduction",
    "title": "Covid-19 NLP Pipeline",
    "section": "",
    "text": "In this tutorial, we will guide you through the process of re-writing an existing data pipeline into the spannerlog framework, allowing you to witness a real-world example of the framework’s benefits. And to offer further tutorials on advanced applications of the spannerlog framework. We’ve chosen to adapt a pipeline from the field of NLP, specifically the Covid-19 NLP pipeline, which was a part of a published paper in 2020.\n\n\nThe pipline repository link.\nThe primary objective of the NLP pipeline is to identify individuals who have been positively diagnosed with COVID-19 by extracting pertinent information from unstructured free-text narratives found within the Electronic Health Record (EHR) of the Department of Veterans Affairs (VA). By automating this process, the pipeline streamlines the screening of a substantial volume of clinical text, significantly reducing the time and effort required for identification. The pipeline is built on medSpacy framework, and defines a new UI to use.\n\n\n\n\n\n\npipeline_flowchart\n\n\n\npreprocessor: Modifies the preprocessed text\nconcept tagger: Assigns a semantic tag in a custom attribute to each Token, which helps with concept extraction and normalization.\ntarget matcher: Extracts spans using rules, based on the concept tagger.\nsectionizer: Identifies note section headers in the text and assigns section titles to entities and tokens contained in that section.\ncontext: Identifies semantic modifiers of entities and asserts attributes such as positive status, negation, and other experiencier.\npostprocessor: Modifies or removes the entity based on business logic. This handles special cases or complex logic using the results of earlier stages.\ndocument classifier: Assigns a label of “POS”, “UNK”, or “NEG” to each file, A document will be classified as positive if it has at least one positive, non-excluded entity We will explain about each stage in more details later on.\n\nAt each stage, we encountered unique challenges. Each stage involved working with different classes and medSpacy framework attributes. Our approach was to first understand the original implementation, seek opportunities to simplify it, and then rewrite it in spannerlog. What was possible we wrote it declaratively using rules, facts, and queries. Additionally, if any essential functionalities were missing in the library, we built IE functions. As we mentioned the pipline uses medSpacy framework so first we need to import some libraries and install some requirements.",
    "crumbs": [
      "Tutorials",
      "Covid-19 NLP Pipeline"
    ]
  },
  {
    "objectID": "covid-nlp/covid_pipeline.html#implementation---step-by-step",
    "href": "covid-nlp/covid_pipeline.html#implementation---step-by-step",
    "title": "Covid-19 NLP Pipeline",
    "section": "Implementation - step by step:",
    "text": "Implementation - step by step:\n\nSetting Up The Environment:\nWe need to install some requirements to work with medspacy framework\n\n! pip install spacy\n! python -m spacy download en_core_web_sm\n\n\nimport spacy\n\nInstalling and importing primitives from the framework\n! pip install spannerlib\n\nimport re\nimport csv\nimport pandas as pd\nfrom pandas import DataFrame\nimport spannerlib\nfrom spannerlib.session import Session\nfrom spannerlib.primitive_types import Span, DataTypes\nfrom spannerlib import magic_session\n\nDefining some generic ie functions that will be used in every stage of the pipline:\n\ndef read_from_file(text_path):\n    \"\"\"\n    Reads from file and return it's content.\n\n    Parameters:\n        text_path (str): The path to the text file to read from.\n\n    Returns:\n        str: The content of the file.\n    \"\"\"\n    with open(f\"{text_path}\", 'r') as file:\n        content = file.read()\n    yield content\n\nmagic_session.register(ie_function=read_from_file,\n                       ie_function_name = \"read_from_file\",\n                       in_rel=[DataTypes.string],\n                       out_rel=[DataTypes.string])\n\n\n# usage example\nfor value in read_from_file(\"sample1.txt\"):\n    print(value)\n\npatient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive . \n\n\n\ndef print_csv_file(file_path):\n    \"\"\"\n    Print the contents of a CSV file in a human-readable format.\n\n    Parameters:\n        file_path (str): The path to the CSV file.\n    \"\"\"\n    with open(file_path, 'r', newline='') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        for row in csv_reader:\n            print(','.join(row))\n\n\n# usage example\n\ndata_to_write = [\n    ['Name', 'Age', 'Occupation'],\n    ['John', '25', 'Engineer'],\n    ['Jane', '30', 'Doctor'],\n    ['Bob', '28', 'Teacher'],\n    ['Alice', '22', 'Student']\n]\n\nwith open('example.csv', 'w', newline='') as csv_file:\n    csv_writer = csv.writer(csv_file)\n    csv_writer.writerows(data_to_write)\n\nprint_csv_file('example.csv')\n\nName,Age,Occupation\nJohn,25,Engineer\nJane,30,Doctor\nBob,28,Teacher\nAlice,22,Student\n\n\n\ndef select_containing_span(spans):\n    \"\"\"\n    This function takes a list of spans, where each span is represented\n    as a list containing a label and a span (interval). It resolves overlaps\n    by selecting the containing span, favoring the larger span in case of conflicts.\n\n    Parameters:\n    spans (list of lists): A list of spans, where each span is represented\n        as a list [label, span].\n\n    Returns:\n    list of lists: A list of resolved spans, where each span is a list\n        [label, span], with conflicts resolved by selecting the containing span.\n    \"\"\"\n    # Sort the replacements by the size of the spans in descending order\n    spans.sort(key=lambda x: x[1].span_end - x[1].span_start, reverse=True)\n\n    # Initialize a list to keep track of intervals that have been replaced\n    resolved_spans = []\n    \n    for label, span in spans:\n        conflict = False\n\n        for _, existing_span in resolved_spans:\n            existing_start = existing_span.span_start\n            existing_end = existing_span.span_end\n\n            if not (span.span_end &lt;= existing_start or span.span_start &gt;= existing_end):\n                conflict = True\n                break\n\n        if not conflict:\n            resolved_spans.append([label, span])\n\n    return resolved_spans\n\n\n# usage example\nspans = [ \n     ['Label1', Span(2, 8)],\n     ['Label2', Span(5, 8)]\n ]\nresolved_spans = select_containing_span(spans)\nfor label, span in resolved_spans:\n    print(f\"Label: {label}, Span: {span.span_start}-{span.span_end}\")\n\nLabel: Label1, Span: 2-8\n\n\n\ndef replace_spans(spans_table, paths_table, session):\n    \"\"\"\n    This function takes tables a spans tables and path table for the files paths,\n    it generate queries for the tables, executes the queries using the given session, processes the results, \n    and replaces specific spans in a text with the corresponding labels, it first\n    resolve spans overlapping conflicts for each giving path.\n\n    Parameters:\n    spans_table (str): A string representing the spans table to process, table columns are formated as (Label, Span, Path).\n    paths_table (str): A string representing the paths table to process, table columns are formated as (Path)\n    session: the session in the spannerlog to run the queries at.\n\n    Returns:\n    str: The adjusted text string with the new labels.\n    \"\"\"\n    # Get a list of all the paths\n    paths = session.run_commands(f\"?{paths_table}(Path)\", print_results=False, format_results=True)\n    paths = paths[0].values.tolist()\n    for path_list in paths:\n        path = path_list[0]\n\n        # Generate a spans query for each path, the query will be formates as (Label, Span, Path)\n        results = magic_session.run_commands(f'?{spans_table}(Label, Span, \"{path}\")', print_results=True, format_results=True)\n        if len(results[0]) == 0:\n            continue\n        # replacments is list of lists where each list is a [Label, Span]\n        replacements = results[0].values.tolist()\n        \n        with open(f\"{path}\", 'r') as file:\n            adjusted_string = file.read()\n    \n        # Resolve spans conflicts\n        resolved_replacements = select_containing_span(replacements)\n    \n        # Sort the resolved replacements by the starting index of each span in descending order\n        resolved_replacements.sort(key=lambda x: x[1].span_start, reverse=True)\n    \n        # iterate over the resolved query results and replace the space with the corresponding label\n        for i in range(len(resolved_replacements)):\n            replace_string, span = resolved_replacements[i]\n            replace_length = len(replace_string)\n            adjusted_string = adjusted_string[:span.span_start] + replace_string + adjusted_string[span.span_end:]\n    \n        with open(f\"{path}\", 'w') as file:\n            file.writelines(adjusted_string)\n\n\n# usage example\nwith open('example.txt', 'w') as file:\n    file.write('The boy has novel coronavirus')\n%spannerlog new samplePaths(str)\n%spannerlog samplePaths(\"example.txt\")\n%spannerlog new sampleMatches(str, span, str)\n%spannerlog sampleMatches(\"Covid-19\", [12,29), \"example.txt\")\nreplace_spans('sampleMatches', 'samplePaths', magic_session)\nfor value in read_from_file(\"example.txt\"):\n    print(value)\n\nprinting results for query 'sampleMatches(Label, Span, \"example.txt\")':\n  Label   |   Span\n----------+----------\n Covid-19 | [12, 29)\n\nThe boy has Covid-19\n\n\n\ndef is_span_contained(span1, span2):\n    \"\"\"\n    Checks if one span is contained within the other span and returns the smaller span if yes.\n\n    Parameters:\n        span1 (span)\n        span2 (span)\n\n    Returns:\n        span: span1 if contained within span2 or vice versa, or None if not contained.\n    \"\"\"\n    start1, end1 = span1.span_start, span1.span_end\n    start2, end2 = span2.span_start, span2.span_end\n    \n    if start2 &lt;= start1 and end1 &lt;= end2:\n        yield span1\n        \n    elif start1 &lt;= start2 and end2 &lt;= end1:\n        yield span2\n\nmagic_session.register(is_span_contained, \"is_span_contained\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])\n\n\n# usage example\nspan1 = Span(2, 12)\nspan2 = Span(8, 9)\nfor span in is_span_contained(span1, span2):\n    print(f\" Span: {span.span_start}-{span.span_end}\")\n\n Span: 8-9\n\n\n\ndef get_relative_span(span1, span2):\n    \"\"\"\n    Computes the relative position of the conatined span within the other span.\n\n    Parameters:\n        span1 (Span): The first span object.\n        span2 (Span): The second span object.\n\n    Returns:\n        Span: The new relative span of the contained one.\n        None: If there's no span contained within the other.\n    \"\"\"\n    start1, end1 = span1.span_start, span1.span_end\n    start2, end2 = span2.span_start, span2.span_end\n    \n    if start2 &lt;= start1 and end1 &lt;= end2:\n        yield Span(span1.span_start - span2.span_start, span1.span_end - span2.span_start)\n        \n    elif start1 &lt;= start2 and end2 &lt;= end1:\n        yield Span(span2.span_start - span1.span_start, span2.span_end - span1.span_start)\n\nmagic_session.register(get_relative_span, \"get_relative_span\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])\n\n\n# usage example \nspan1 = Span(2, 12)\nspan2 = Span(2, 5)\nfor span in get_relative_span(span1, span2):\n    print(f\" Span: {span.span_start}-{span.span_end}\")\n\n Span: 0-3\n\n\n\ndef sent_tokenization(text_path):\n    \"\"\"\n    This function reads a text file, processes its content using spaCy's English language model,\n    tokenizing it into sentences and returns each individual sentence in the processed text using a generator.\n    \n    Parameters:\n        text_path (str): The path to the text file to be annotated.\n\n    Returns:\n        str: Individual sentences extracted from the input text.\n    \"\"\"\n    with open(text_path, 'r') as file:\n        contents = file.read()\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    for sentence in doc.sents:\n        yield sentence.text\n\nmagic_session.register(ie_function=sent_tokenization, ie_function_name = \"sent_tokenization\", in_rel=[DataTypes.string], out_rel=[DataTypes.string])\n\n\n# usage example \nfor sentence in sent_tokenization(\"sample1.txt\"):\n    print(sentence)\n\npatient presents to be tested for COVID-19 .\nHis family recently tested positive for COVID-19 .\nCOVID-19 results came back positive .\n\n\n\n\nPipeline Input:\nThe paths of the text files to be classified should be written in “files_paths.csv” file\n\nprint_csv_file('files_paths.csv')\n\nsample1.txt\nsample2.txt\nsample3.txt\nsample4.txt\nsample5.txt\nsample6.txt\nsample7.txt\n\n\n\nmagic_session.import_rel(\"files_paths.csv\", relation_name=\"FilesPaths\", delimiter=\",\")\n\nThe initial files contents:\n\n%%spannerlog\nFilesContent(Path, Content) &lt;- FilesPaths(Path), read_from_file(Path) -&gt; (Content)\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\nBefore we continue we have to do some pre processing to help ease the next stages, we will certain words in each list to it’s lemma forms, here a list of the words that we want to lemmatize\n\nprint_csv_file('lemma_words.txt')\n\nbe\nman\nwoman\nhave\ndo\nemergency\nepidemic\noutbreak\ncrisis\nbreakout\npandemic\nspread\nconfirm\nperson\npatient\nveteran\nlimit\nreduce\nfactor\ncontact\ncase\nlower\nminimize\nrisk\nchance\npossibility\ncare\nclean\ndesire\nflight\ntrip\nplan\nreschedule\npostpone\nbarrier\n\n\nWe will define a helper method to do that:\n\ndef lemmatize_text(text_path, lemma_words_path):\n    \"\"\"\n    This function reads a text file, lemmatizes its content using spaCy's English language model,\n    and replaces certain words with their lemmas the rest will remain the same. The updated text is then written back to the same file.\n\n    Parameters:\n        text_path (str): The path to the text file to be lemmatized.\n        lemma_words_path(str): The path that contains the list of words to be lemmatized\n\n    Returns:\n        str: The lemmatized text.\n    \"\"\"\n    # Define a list of words to be lemmatized\n    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n\n    with open(text_path, 'r') as file:\n        contents = file.read()\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    lemmatized_text = \"\"\n    for token in doc:\n        if token.lemma_ in lemma_words:\n            lemmatized_text += token.lemma_\n        elif token.like_num:\n            lemmatized_text += \"like_num\"\n        else:\n            lemmatized_text += token.text\n        lemmatized_text += \" \"\n\n    # Write the lemmatized text back to the same file\n    with open(text_path, 'w') as file:\n        file.writelines(lemmatized_text)\n\n    return lemmatized_text\n\n\n# usage example\nwith open('example.txt', 'w') as file:\n    file.write('The boy was sick')\nlemmatized_text = lemmatize_text('example.txt', 'lemma_words.txt')\nprint(lemmatized_text)\n\nThe boy be sick \n\n\nIterate over the texts to lemmatize them\n\nwith open('files_paths.csv', 'r') as file:\n    # Create a CSV reader object\n    csv_reader = csv.reader(file)\n\n    # Iterate through each row in the CSV file\n    for row in csv_reader:\n        path = row[0]\n        lemmatize_text(path, 'lemma_words.txt')\n\nAs we can see for example in sample2.txt, was has changed to be.\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n\nConcept Tagger:\nConcept tag rules, also known as pattern-based rules or custom rules, are a way to specify and define patterns that an NLP (Natural Language Processing) system should recognize within text data. These rules are used to identify specific concepts or entities within text documents. In the context of MedSpaCy and medical NLP, concept tag rules are often used to identify medical entities and concepts accurately.\nIn the orginal project they used the TargetRule class which defines a rule for identifying a specific concept or entity in text. each concept Target Rule looks like this:\nTargetRule( literal=“coronavirus”, category=“COVID-19”, pattern=[{“LOWER”: {“REGEX”: “coronavirus|hcov|ncov$”}}], )\nLiteral : This specifies the literal text or word that this rule is targeting.\nCategory : This specifies the category or label associated with the identified entity.\nPattern : This defines the pattern or conditions under which the entity should be recognized. It’s a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\nInstead what we did is to define regex patterns, we have added these pattern in concept_target_rules.csv file, there are two types of these patterns lemma and pos, that we will implement each later on. Each rule in the csv file is like this : regexPattern, label, type\n\nprint_csv_file('concept_tags_rules.csv')\n\n(?i)(?:hcov|covid(?:(?:-)?(?:\\s)?19|10)?|2019-cov|cov2|ncov-19|covd 19|no-cov|sars cov),COVID-19,lemma\n(?i)(?:coivid|(?:novel )?corona(?:virus)?(?: (?:20)?19)?|sars(?:\\s)?(?:-)?(?:\\s)?cov(?:id)?(?:-)?(?:2|19)),COVID-19,lemma\n(?i)(?:\\+(?: ve)?|\\(\\+\\)|positive|\\bpos\\b|active|confirmed),positive,lemma\n(?i)(?:pneum(?:onia)?|pna|hypoxia|septic shoc|ards\\(?(?:(?:[12])/2)\\)?|(?:hypoxemic|acute|severe)? resp(?:iratory)? failure(?:\\(?(?:[12]/2)\\)?)?)\",associated_diagnosis,lemma\n(?i)(?:(?:diagnos(?:is|ed)|dx(?:\\.)?)(?:of|with)?),diagnosis,lemma\n(?i)(?:^screen),screening,lemma\n(?i)(?:in contact with|any one|co-worker|at work|(?:the|a)(?:wo)?man|(?:another|a) (?:pt|patient|pt\\.)),other_experiencer,lemma\n(?i)(?:patient|pt(?:\\.)?|vt|veteran),patient,lemma\n(?i)(?:like_num (?:days|day|weeks|week|months|month) (?:ago|prior)),timesx,lemma\n(?i)(?:(?:antibody|antibodies|ab) test),antibody test,lemma\n(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\s)?(?:-)?(?:\\s)?(?: infection)?(?: strain)?(?:\\s)?(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63)),OTHER_CORONAVIRUS,lemma\n(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63) (?:coronavirus|hcovs?|ncovs?|covs?)),OTHER_CORONAVIRUS,lemma\n(?i)(?:non(?:\\s)?(?:-)?(?:\\s)?(?:novel|covid|ncovid|covid-19)(?: coronavirus)?|other coronavirus),OTHER_CORONAVIRUS,lemma\n(?i)(?:wife|husband|spouse|family|member|girlfriend|boyfriend|mother|father|nephew|niece|grandparent|grandparents|granddaughter|relative|relatives|caregiver),family,pos\n(?i)(?:grandchild|grandson|cousin|grandmother|grandfather|parent|son|daughter|mom|dad|brother|sister|aunt|uncle|child|children|sibling|siblings),family,pos\n(?i)(?:someone|somebody|person|anyone|anybody|people|individual|individuals|teacher|anybody|employees|employer|customer|client|residents),other_experiencer,pos\n(?i)(?:resident|pts|patients|coworker|coworkers|workers|colleague|captain|captains|pilot|pilots|sailor|sailors|meeting),other_experiencer,pos\n(?i)(?:boyfriend|persons|person|church|convention|guest|party|attendee|conference|roommate|friend|friends|coach|player|neighbor|manager|boss),other_experiencer,pos\n(?i)(?:cashier|landlord|worked|works|^mate|nobody|mates|housemate|housemates|hotel|soldier|airport|tsa|lady|ladies|lobby|staffer|staffers),other_experiencer,pos\n\n\n\nmagic_session.import_rel(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")\n\n\n\nLemma Rules:\nLemma rules are rules that used the attribute _lemma of each token in the NLP, we already lemmatized the texts, so now we can create a regex patterns for that.\nExample for a lemma rule from the original NLP:\n    TargetRule(\n        \"results positive\",\n        \"positive\",\n        pattern=[\n            {\"LOWER\": \"results\"},\n            {\"LEMMA\": \"be\", \"OP\": \"?\"},\n            {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n        ],\n    ),\nWe used the py_rgx_span to capture the patterns, and will use the spans later on in replace_spans that will replace each span with the correct label\n\n%%spannerlog\nLemmaMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -&gt; (Span)\n\nBefore:\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n# replace the matches with the correct label\nreplace_spans(\"LemmaMatches\", \"FilesPaths\", magic_session)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample1.txt\")':\n  Label   |    Span\n----------+------------\n COVID-19 |  [34, 42)\n COVID-19 |  [85, 93)\n COVID-19 | [96, 104)\n positive | [123, 131)\n positive |  [72, 80)\n patient  |   [0, 7)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample2.txt\")':\n  Label   |   Span\n----------+----------\n COVID-19 | [26, 34)\n positive | [48, 56)\n patient  | [4, 11)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample3.txt\")':\n   Label   |   Span\n-----------+----------\n COVID-19  | [58, 66)\n diagnosis | [37, 46)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample4.txt\")':\n  Label   |  Span\n----------+---------\n COVID-19 | [4, 12)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample5.txt\")':\n  Label   |  Span\n----------+---------\n COVID-19 | [9, 17)\n positive | [0, 8)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample6.txt\")':\n  Label   |   Span\n----------+----------\n COVID-19 | [26, 34)\n patient  | [4, 11)\n\nprinting results for query 'LemmaMatches(Label, Span, \"sample7.txt\")':\n[]\n\n\n\nAfter: As we can see for example in the sample1.txt, every other covid-19 name was changed to COVID-19.\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n\nPOS Rules:\nAs we mentioned above these rules used the POS attribute of each token, there were a small number of rules so we only used this to the tokens we needed. Example of the a rule from the original NLP:\n    TargetRule(\n        \"other experiencer\",\n        category=\"other_experiencer\",\n        pattern=[\n            {\n                \"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]},\n                \"LOWER\": {\n                    \"IN\": [\n                        \"someone\",\n                        \"somebody\",\n                        \"person\",\n                        \"anyone\",\n                        \"anybody\",\n                    ]\n                },\n            }\n        ],\n    ),\nThe patterns we’ve defined will match words listed under “IN”, We specifically capture words if their Part-of-Speech (POS) falls into one of the categories: [“NOUN”, “PROPN”, “PRON”, “ADJ”]. To accomplish this, two functions are employed: the first function determines the POS of each token, and the second one, py_rgx_span, captures the predefined patterns. After matching words, We confirm the accurate POS tags of the matched words using spans.\n\ndef annotate_text_with_pos(text_path):\n    \"\"\"\n    This function reads a text file, processes its content using spaCy's English language model,\n    and returns a tuple of (POS, Span) for each token if it's one of NOUN|PROPN|PRON|ADJ\n    otherwise an empty tuple will be returned\n    \n    Parameters:\n        text_path (str): The path to the text file to be annotated.\n\n    Returns:\n        tuple(str, Span): The POS of the token and it's span\n    \"\"\"\n    with open(text_path, 'r') as file:\n        contents = file.read()\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    for token in doc:\n        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n        else:\n            yield tuple()\nmagic_session.register(ie_function=annotate_text_with_pos, ie_function_name = \"annotate_text_with_pos\", in_rel=[DataTypes.string], out_rel=[DataTypes.string, DataTypes.span])\n\n\n# usage example\nwith open('example.txt', 'w') as file:\n    file.write('sick boy')\n\nfor POS, span in annotate_text_with_pos('example.txt'):\n    print(f\"{POS}, ({span.span_start},{span.span_end})\")\n\nADJ, (0,4)\nNOUN, (5,8)\n\n\n\n%%spannerlog\nPOSTable(POS, Span, Path) &lt;- FilesContent(Path, Content), annotate_text_with_pos(Path) -&gt; (POS, Span)\n?POSTable(POS, Span, Path)\n\nPOSMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -&gt; (Span)\n?POSMatches(Label, Span, Path)\n\nPOSRuleMatches(Label, Span, Path) &lt;- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n?POSRuleMatches(Label, Span, Path)\n\nprinting results for query 'POSTable(POS, Span, Path)':\n  POS  |    Span    |    Path\n-------+------------+-------------\n  ADJ  |   [0, 7)   | sample1.txt\n  ADJ  | [123, 131) | sample1.txt\n  ADJ  |  [72, 80)  | sample1.txt\n NOUN  | [105, 112) | sample1.txt\n NOUN  |  [49, 55)  | sample1.txt\n NOUN  |  [8, 16)   | sample1.txt\n PRON  |  [45, 48)  | sample1.txt\n PROPN |  [34, 42)  | sample1.txt\n PROPN |  [85, 93)  | sample1.txt\n PROPN | [96, 104)  | sample1.txt\n  ADJ  |  [48, 56)  | sample2.txt\n NOUN  |  [37, 44)  | sample2.txt\n NOUN  |  [4, 11)   | sample2.txt\n PROPN |  [26, 34)  | sample2.txt\n NOUN  |  [0, 12)   | sample3.txt\n PROPN |  [15, 23)  | sample3.txt\n PROPN |  [47, 55)  | sample3.txt\n PROPN |  [58, 66)  | sample3.txt\n PROPN |  [67, 75)  | sample3.txt\n NOUN  |  [13, 22)  | sample4.txt\n PROPN |   [0, 3)   | sample4.txt\n PROPN |  [4, 12)   | sample4.txt\n  ADJ  |   [0, 8)   | sample5.txt\n NOUN  |  [18, 28)  | sample5.txt\n PROPN |  [9, 17)   | sample5.txt\n NOUN  |  [4, 11)   | sample6.txt\n PROPN |  [26, 34)  | sample6.txt\n  ADJ  |   [0, 8)   | sample7.txt\n  ADJ  |  [36, 43)  | sample7.txt\n NOUN  |  [21, 27)  | sample7.txt\n NOUN  |  [44, 54)  | sample7.txt\n NOUN  |  [59, 68)  | sample7.txt\n NOUN  |  [69, 80)  | sample7.txt\n NOUN  |  [9, 20)   | sample7.txt\n\nprinting results for query 'POSMatches(Label, Span, Path)':\n  Label  |   Span   |    Path\n---------+----------+-------------\n family  | [49, 55) | sample1.txt\n\nprinting results for query 'POSRuleMatches(Label, Span, Path)':\n  Label  |   Span   |    Path\n---------+----------+-------------\n family  | [49, 55) | sample1.txt\n\n\n\nBefore:\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n# replace the matches with the correct label\nreplace_spans(\"POSRuleMatches\", \"FilesPaths\", magic_session)\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample1.txt\")':\n  Label  |   Span\n---------+----------\n family  | [49, 55)\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample2.txt\")':\n[]\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample3.txt\")':\n[]\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample4.txt\")':\n[]\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample5.txt\")':\n[]\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample6.txt\")':\n[]\n\nprinting results for query 'POSRuleMatches(Label, Span, \"sample7.txt\")':\n[]\n\n\n\nAfter: As we can see for example in sample1.txt, wife has changed to family.\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n\nTarget Rules:\nThese rules used the label that was assigned through the concept tagger, to capture some more complex patterns and assign a label for them inorder to decremnt the cases of false positive. Each rule look like this:\n    TargetRule(\n        literal=\"coronavirus screening\",\n        category=\"IGNORE\",\n        pattern=[\n            {\"_\": {\"concept_tag\": \"COVID-19\"}},\n            {\"LOWER\": {\"IN\": [\"screen\", \"screening\", \"screenings\"]}},\n        ],\n    ),\nSince we replaced the spans we found with the corresponding label we didn’t need the concept_tag attribute of the token/span.\nLiteral : This specifies the literal text or word that this rule is targeting.\nCategory : This specifies the category or label associated with the identified entity.\nPattern : This defines the pattern or conditions under which the entity should be recognized. It’s a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\nSimilar to the concept tag apporach we defined regex patterns, we have added these pattern in target_rules.csv file Each rule in the csv file is like this : regexPattern, label\n\nprint_csv_file('target_rules.csv')\n\n(?i)(?:COVID-19 positive (?:unit|floor)|positive COVID-19 (?:unit|floor|exposure)),COVID-19\n(?i)(?:known(?: positive)? COVID-19(?: positive)? (?:exposure|contact)),COVID-19\n(?i)(?:COVID-19 positive screening|positive COVID-19 screening|screening COVID-19 positive|screening positive COVID-19),positive coronavirus screening\n(?i)(?:diagnosis : COVID-19 (?:test|screening)),COVID-19\n(?i)(?:COVID-19 screening),coronavirus screening\n(?i)(?:active COVID-19 precaution|droplet isolation precaution|positive for (?:flu|influenza)|(?:the|a) positive case|results are confirm),1 2 3\n(?i)(?:exposed to positive|[ ] COVID-19|age like_num(?: )?\\+|(?:return|back) to work|COVID-19 infection rate),1 2 3\n(?i)(?:COVID-19 (?:restriction|emergency|epidemic|outbreak|crisis|breakout|pandemic|spread|screening)|droplet precaution),1 2\n(?i)(?:contact precautions|positive (?:flu|influenza)|positive (?:patient|person)|confirm (?:with|w/(?:/)?|w)|(?:the|positive) case),1 2\n(?i)(?:results confirm|(?:neg|pos)\\S+ pressure|positive (?:attitude|feedback|serology)|COVID-19 (guidelines|rate)),1 2\n(?i)(?:has the patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5 6\n(?i)(?:has patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5\n(?i)((?:person|patient) with confirm COVID-19),1 2 3 4\n(?i)(?:COVID-19 positive (?:tested )?other_experiencer),COVID-19\n(?i)(?:in order to decrease the spread of the COVID-19 infection),1 2 3 4 5 6 7 8 9 10\n(?i)(?:COVID-19 positive (?:patient|person|people|veteran)),OTHER_PERSON\n(?i)(?:positive COVID-19 (?:tested )?other_experiencer),COVID-19\n(?i)(?:(?:(?:contact|exposure) (?:with|to)? )?positive COVID-19 (?:patient|person|veteran)),OTHER_PERSON\n(?i)(?:(?:patient|person) (?:who|that) test (?:positive|confirm) for COVID-19),OTHER_PERSON\n(?i)(ref : not detected|history of present illness|does not know|but|therefore|flu|metapneumovirus|;),&lt;IGNORE&gt;\n\n\n\nmagic_session.import_rel(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")\n\n\n%%spannerlog\nTargetTagMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content, Pattern) -&gt; (Span)\n\nBefore:\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\nreplace_spans(\"TargetTagMatches\", \"FilesPaths\", magic_session)\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample1.txt\")':\n[]\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample2.txt\")':\n[]\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample3.txt\")':\n[]\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample4.txt\")':\n[]\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample5.txt\")':\n[]\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample6.txt\")':\n[]\n\nprinting results for query 'TargetTagMatches(Label, Span, \"sample7.txt\")':\n[]\n\n\n\nAfter: As we can see in sample6.txt, the covid positive exposure has changed to covid, in order to not give false positive.\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n\nSection Rules:\nHere, we’ll add a section detection component that defines rules for detecting sections titles, which usually appear before a semicolon. Section rules are utilized to identify specific section names, enabling the separation of text into different parts. Entities occurring in certain sections are considered positive.\nIn the original project, the SectionRule class was used to define rules for identifying specific section text. Each SectionRule has the following structure\n  SectionRule(category=\"problem_list\", literal=\"Active Problem List:\"),\n  SectionRule(category=\"problem_list\", literal=\"Current Problems:\"),\nLiteral : This specifies the literal section text or word that this rule is targeting.\nCategory : This specifies the section category associated with the identified section.\nSimilar to the approach used in the concept tagger stage, regex patterns were derived from these literals, and these patterns are stored in the ‘section_target_rules.csv’ file and are used to match section texts and replace them with their appropriate category.\nEach rule in the CSV file follows this format: regexPattern, sectionLabel\n\nprint_csv_file('section_rules.csv')\n\n(?i)(?:Lab results :),labs :\n(?i)(?:Addendum :),addendum :\n(?i)(?:(?:ALLERGIC REACTIONS|ALLERGIES) :),allergies :\n(?i)(?:(?:CC|Chief Complaint) :),chief_complaint :\n(?i)(?:COMMENTS :),comments :\n(?i)(?:(?:(?:ADMISSION )?DIAGNOSES|Diagnosis|Primary Diagnosis|Primary|Secondary(?: (?:Diagnoses|Diagnosis))) :),diagnoses :\n(?i)(?:(?:Brief Hospital Course|CONCISE SUMMARY OF HOSPITAL COURSE BY ISSUE/SYSTEM|HOSPITAL COURSE|SUMMARY OF HOSPITAL COURSE) :),hospital_course :\n(?i)(?:(?:Imaging|MRI|INTERPRETATION|Radiology) :),imaging :\n(?i)(?:(?:ADMISSION LABS|Discharge Labs|ECHO|Findings|INDICATION|Labs|Micro|Microbiology|Studies|Pertinent Results) :),labs_and_studies :\n(?i)(?:(?:ACTIVE MEDICATIONS(?: LIST)|ADMISSION MEDICATIONS|CURRENT MEDICATIONS|DISCHARGE MEDICATIONS|HOME MEDICATIONS|MEDICATIONS) :),medications :\n(?i)(?:(?:MEDICATIONS AT HOME|MEDICATIONS LIST|MEDICATIONS ON ADMISSION|MEDICATIONS ON DISCHARGE|MEDICATIONS ON TRANSFER|MEDICATIONS PRIOR TO ADMISSION) :),medications :\n(?i)(?:Neuro :),neurological :\n(?i)(?:(?:A/P|MEDICATIONS LIST|ASSESSMENT/PLAN|ASSESSMENT|Clinical Impression|DISCHARGE DIAGNOSES|DISCHARGE DIAGNOSIS) :),observation_and_plan :\n(?i)(?:(?:Discharge Condition|Discharge Disposition|FINAL DIAGNOSES|FINAL DIAGNOSIS|IMPRESSION|Impression and Plan|Impression and Recommendation) :),observation_and_plan :\n(?i)(?:(?:Facility|Service) :),other :\n(?i)(?:(?:Current Medical Problems|History of Chronic Illness|MHx|PAST HISTORY|PAST MEDICAL Hx|PAST SURGICAL HISTORY|PMH|PMHx|PAST MEDICAL HISTORY|UNDERLYING MEDICAL CONDITION) :),past_medical_history :\n(?i)(?:(?:Education|Patient Education|DISCHARGE INSTRUCTIONS/FOLLOWUP|DISCHARGE INSTRUCTIONS|Followup Instructions) :),patient_education :\n(?i)(?:(?:PE|PHYSICAL EXAM|PHYSICAL EXAMINATION) :),physical_exam :\n(?i)(?:(?:Active Problem List|Current Problems|Medical Problems|PROBLEM LIST) :),problem_list :\n(?i)(?:REASON FOR THIS EXAMINATION :),reason_for_examination :\n(?i)(?:(?:Electronic Signature|Signed electronically by) :),signature :\n(?i)(?:(?:PMHSx|PSH|SH|Sexual History:|Social History) :),social_history :\n\n\n\nmagic_session.import_rel(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")\n\n\n%%spannerlog\nSectionRulesMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content, Pattern) -&gt; (Span)\n?SectionRulesMatches(Label, Span, Path)\n\nprinting results for query 'SectionRulesMatches(Label, Span, Path)':\n[]\n\n\n\nBefore:\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\nreplace_spans(\"SectionRulesMatches\", \"FilesPaths\", magic_session)\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample1.txt\")':\n[]\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample2.txt\")':\n[]\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample3.txt\")':\n[]\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample4.txt\")':\n[]\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample5.txt\")':\n[]\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample6.txt\")':\n[]\n\nprinting results for query 'SectionRulesMatches(Label, Span, \"sample7.txt\")':\n[]\n\n\n\nAfter: As we can see in sample 3, current problems has changed to problems_list.\n\n%%spannerlog\n?FilesContent(Path, Content)\n\nprinting results for query 'FilesContent(Path, Content)':\n    Path     |                                                                Content\n-------------+---------------------------------------------------------------------------------------------------------------------------------------\n sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n sample4.txt |                                                       neg COVID-19 education .\n sample5.txt |                                                    positive COVID-19 precaution .\n sample6.txt |                                                 The patient have reported COVID-19 .\n sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\n\n\n\n\nAttribute Assertion:\nNext, we will explore how to assert attributes indicating whether a mention of COVID-19 is positive or not. In our project, we have created a table named ‘CovidAttributes’ that contains all attributes for each COVID-19 mention. This table will be used for classifying documents.\n\n%%spannerlog\n#Here, we employ a pattern to identify entities present in specific sections and mark them as positive,\n#and adding them to the 'CovidAttributes' table.\n\npattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\nnew SectionRulesAttribute(str, str)\nSectionRulesAttribute(pattern, \"positive\")\n\nSectionMatches(Path, Span, CovidAttribute) &lt;- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -&gt; (Span)\n?SectionMatches(Path, Span, CovidAttribute)\n\nCovidMatches(Path, Span) &lt;- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -&gt; (Span)\n?CovidMatches(Path, Span)\n\nprinting results for query 'SectionMatches(Path, Span, CovidAttribute)':\n    Path     |  Span   |  CovidAttribute\n-------------+---------+------------------\n sample3.txt | [0, 76) |     positive\n\nprinting results for query 'CovidMatches(Path, Span)':\n    Path     |   Span\n-------------+-----------\n sample1.txt | [34, 42)\n sample1.txt | [85, 93)\n sample1.txt | [96, 104)\n sample2.txt | [26, 34)\n sample3.txt | [58, 66)\n sample4.txt |  [4, 12)\n sample5.txt |  [9, 17)\n sample6.txt | [26, 34)\n\n\n\n\n%%spannerlog\nSectionCovidAttributes(Path, CovidSpan, CovidAttribute) &lt;- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -&gt; (CovidSpan)\n?SectionCovidAttributes(Path, CovidSpan, CovidAttribute)\n\nprinting results for query 'SectionCovidAttributes(Path, CovidSpan, CovidAttribute)':\n    Path     |  CovidSpan  |  CovidAttribute\n-------------+-------------+------------------\n sample3.txt |  [58, 66)   |     positive\n\n\n\n\n\nTokenizing the Text into Sentences:\nIn the subsequent stages, where attributes are assigned to COVID-19 mentions, a departure from the previous stages occurs. Here, patterns are no longer applied to the entire text, instead, they are applied at the sentence level, since the attributes of COVID-19 mentions are typically determined by the context of the sentence in which they appear. This means the text is processed and tokenized into sentences using spaCy’s English language model. This process is accomplished through the use of ie functions and relations.\n\n%%spannerlog\n#Sentences of the text\nSents(Path, Sent) &lt;- FilesPaths(Path), sent_tokenization(Path) -&gt; (Sent)\n?Sents(Path, Sent)\n\n#SentSpan is the span of the sentence in the text\nSentSpans(Path, Sent, SentSpan) &lt;- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -&gt; (SentSpan)\n?SentSpans(Path, Sent, SentSpan)\n\nprinting results for query 'Sents(Path, Sent)':\n    Path     |                                        Sent\n-------------+------------------------------------------------------------------------------------\n sample1.txt |                       COVID-19 results came back positive .\n sample1.txt |                 His family recently tested positive for COVID-19 .\n sample1.txt |                    patient presents to be tested for COVID-19 .\n sample2.txt |                               Results be positive .\n sample2.txt |                        The patient be tested for COVID-19 .\n sample3.txt |                                 COVID-19 like_num\n sample3.txt |                                associated_diagnosis\n sample3.txt |                                     like_num .\n sample3.txt |                             problem_list : like_num .\n sample4.txt |                              neg COVID-19 education .\n sample5.txt |                           positive COVID-19 precaution .\n sample6.txt |                        The patient have reported COVID-19 .\n sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments .\n\nprinting results for query 'SentSpans(Path, Sent, SentSpan)':\n    Path     |                                        Sent                                        |  SentSpan\n-------------+------------------------------------------------------------------------------------+------------\n sample1.txt |                       COVID-19 results came back positive .                        | [96, 133)\n sample1.txt |                 His family recently tested positive for COVID-19 .                 |  [45, 95)\n sample1.txt |                    patient presents to be tested for COVID-19 .                    |  [0, 44)\n sample2.txt |                               Results be positive .                                |  [37, 58)\n sample2.txt |                        The patient be tested for COVID-19 .                        |  [0, 36)\n sample3.txt |                                 COVID-19 like_num                                  |  [58, 75)\n sample3.txt |                                associated_diagnosis                                |  [26, 46)\n sample3.txt |                                     like_num .                                     |  [15, 25)\n sample3.txt |                                     like_num .                                     |  [47, 57)\n sample3.txt |                             problem_list : like_num .                              |  [0, 25)\n sample4.txt |                              neg COVID-19 education .                              |  [0, 24)\n sample5.txt |                           positive COVID-19 precaution .                           |  [0, 30)\n sample6.txt |                        The patient have reported COVID-19 .                        |  [0, 36)\n sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments . |  [0, 82)\n\n\n\n\n%%spannerlog\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent) &lt;- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\nSentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -&gt; (CovidSpan)\n?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\n\nprinting results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n    Path     |  CovidSpan  |  CovidAttribute  |       Sent\n-------------+-------------+------------------+-------------------\n sample3.txt |   [0, 8)    |     positive     | COVID-19 like_num\n\n\n\n\n\nContext Rules:\nThese rules assign an attribute for each COVID-19 label based on the context, these attributes will be used later to classify each text.\nExample for this rule is:\nConTextRule(\n    literal=\"Not Detected\",\n    category=\"NEGATED_EXISTENCE\",\n    direction=\"BACKWARD\",\n    pattern=[\n        {\"LOWER\": {\"IN\": [\"not\", \"non\"]}},\n        {\"IS_SPACE\": True, \"OP\": \"*\"},\n        {\"TEXT\": \"-\", \"OP\": \"?\"},\n        {\"LOWER\": {\"REGEX\": \"detecte?d\"}},\n    ],\n    allowed_types={\"COVID-19\"},\n),\ndirection specify if the allowed_types should be before or after the pattern, allowed_types specify on what labels should this rule be applied on\n\nprint_csv_file('context_rules.csv')\n\n(?i)(?:positive COVID-19|COVID-19 (?:\\([^)]*\\)) (?:positive|detected)|COVID-19(?: positive)? associated_diagnosis)#positive\n(?i)(?:COVID-19 status : positive)#positive\n(?i)(?:associated_diagnosis COVID-19|associated_diagnosis (?:with|w|w//|from) (?:associated_diagnosis )?COVID-19)#positive\n(?i)(?:COVID-19 positive(?: patient| precaution)?|associated_diagnosis (?:due|secondary) to COVID-19)#positive\n(?i)(?:(?:current|recent) COVID-19 diagnosis)#positive\n(?i)(?:COVID-19 (?:- )?related (?:admission|associated_diagnosis)|admitted (?:due to|(?:with|w|w/)) COVID-19)#positive\n(?i)(?:COVID-19 infection|b34(?:\\.)?2|b97.29|u07.1)#positive\n(?i)(?:COVID-19 eval(?:uation)?|(?:positive )? COVID-19 symptoms|rule out COVID-19)#uncertain\n(?i)(?:patient (?:do )?have COVID-19)#positive\n(?i)(?:diagnosis : COVID-19(?: (?:test|screen)(?:ing|ed|s)? positive)?(?: positive)?)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:not|non) (?:- )?detecte?d)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,1} negative screening|negative screening(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,2} : negative)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:not be|none) detected)#negated\n(?i)(?:free from(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? not (?:be )?tested)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,4} not indicated)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? NEGATIVE NEG)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,3} negative test)#negated\n(?i)(?:negative test(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#negated\n(?i)(?:without any(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:denie(?:s|d)(?: any| travel)?(?: (?!&lt;IGNORE&gt;)\\S+){0,9} COVID-19)#negated\n(?i)(?:no (?:evidence(?: of)?|(?:hx|-hx|history) of|diagnosis (?:of)?)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:no(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:no (?:positive|one|residents|confirm case|contact(?: w/?(?:ith)?$))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? no confirm case)#negated\n(?i)(?:(?:no|n't) (?:be )? confirm(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:(?:no known|not have)(?: (?!&lt;IGNORE&gt;)\\S+){0,4} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:answer(?:ed|s|ing)? (?:no|negative|neg)|negative))#negated\n(?i)(?:(?:answer(?:ed|s|ing)? (?:no|negative|neg)|(?:neg|negative)(?: for)?)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:not positive(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? not positive)#negated\n(?i)(?:excluded(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,3} excluded)#negated\n(?i)(?:no risk factor for(?: (?!&lt;IGNORE&gt;)\\S+){0,4} COVID-19)#uncertain\n(?i)(?:negative screening(?: for)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? screening (?:negative|neg))#negated\n(?i)(?:(?:screening (?:negative|neg) for|do (?:not|n't) have (?:any )?(?:signs|symptoms|ss|s/s))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? do not screening positive)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:be negative|not test positive))#negated\n(?i)(?:(?:be negative|not test positive|not? screening(?: for)|no signs of|no (?:sign|symptom|indication(?:of|for)?)|not? test(?:\\S+)? for)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:no exposure|(?:without|w/o) (?:signs|symptoms)(?:or (?:signs|symptoms))|do)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,4} not have)#negated\n(?i)(?:(?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria|no concern (?:for|of)|not? (?:at )risk)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria))#negated\n(?i)(?:(?:no suspicion(?: for)|not suspect|ruled out for|no(?: recent) travel|not be in|clear(?:ed|s|ing) (?:of|for|from))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:not(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:be ruled out|be not likely|not have contact with))#negated\n(?i)(?:(?:no (?:hx|history) (?:of )travel|not have contact with|no symptoms of|no risk factors|no (?:confirm case|report))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:no (?:exposure|contact) (?:to|with)|not test(?:\\S+)? positive))#negated\n(?i)(?:(?:no (?:exposure|contact) (?:to|with)|do (?:not|n't) meet(?: screening)(?: criteria)(?: for)|not test(?:\\S+)? positive(?: for)|not tested(?: or diagnosis))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:no|any)(?: known) contact(?: with)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,3} : no)#negated\n(?i)(?:(?:(?:not|never) diagnosis with|not been tested (?:for )?or diagnosis with)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,1} confirm)#positive\n(?i)(?:(?:confirm|known)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#positive\n(?i)(?:(?:(?:test(?:\\S+)?)?positive(?: for)?|notif(?:y|ied) of positive (?:results?|test(?:\\S+)?|status))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,2} (?:positive status|results be positive))#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,4} results positive)#positive\n(?i)(?:results positive(?: (?!&lt;IGNORE&gt;)\\S+){0,4} COVID-19)#positive\n(?i)(?:notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status))#positive\n(?i)(?:likely secondary to(?: (?!&lt;IGNORE&gt;)\\S+){0,0} COVID-19)#positive\n(?i)(?:(?:problem(?: list)? (?:of|:)|(?:active|current|acute) problems :|admi(?:t|ssion) diagnosis(?: :)?)(?: (?!&lt;IGNORE&gt;)\\S+){0,9} COVID-19)#positive\n(?i)(?:(?:reason for admission :|treatment of|(?:admitting )diagnosis(?: :)?)(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,3} diagnosis like_num)#positive\n(?i)(?:(?:Reason for admission :|inpatient with|discharged from|in m?icu (?:for|with))(?: (?!&lt;IGNORE&gt;)\\S+){0,5} COVID-19)#admission\n(?i)(?:(?:admit(?:ted|s|ting) (?:like_num|with|for)|admitted (?:to|on)|Reason for ICU :|admission for)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#admission\n(?i)(?:Reason for ED visit or Hospital Admission :(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#admission\n(?i)(?:(?:(?:in|to) (?:the )(?:hospital|icu|micu) (?:for|due to)|hospitali(?:zed)?(?: timesx)? (?:for|due to))(?: (?!&lt;IGNORE&gt;)\\S+){0,4} COVID-19)#admission\n(?i)(?:(?:diagnosis with|found to be positive for)(?: (?!&lt;IGNORE&gt;)\\S+){0,5} COVID-19)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,5} found to be positive)#positive\n(?i)(?:(?:positive test|presum(?:e|ed|es|ing) positive|not(?: yet)? recover(?:s|ing|ed)?)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:positive test|presum(?:e|ed|es|ing) positive))#positive\n(?i)(?:(?:management of|ards(?: (?:from|with|secondary to))?|acute respiratory distress|post - extubation)(?: (?!&lt;IGNORE&gt;)\\S+){0,2} COVID-19)#positive\n(?i)(?:(?:in(?: the)? setting of|in the s / o|found to have|present(?:s|ed|ing)? with)(?: (?!&lt;IGNORE&gt;)\\S+){0,5} COVID-19)#positive\n(?i)(?:resp(?:iratory) failure(?:(?: (?:with|due to))?|like_num|\\( like_num \\))(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#positive\n(?i)(?:(?:active(?: for)|recovering from)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,2} recovering from)#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,4} (?:detected|value : detected|POSITIVEH))#positive\n(?i)(?:(?:\\d+(?: )?-|like_num )year(?:(?: )?-(?: )?old| old) (?:(?:aa|white|black|hispanic|caucasian) )?(?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:with|w|w/|admitted)(?: (?!&lt;IGNORE&gt;)\\S+){0,9} COVID-19)#patient_experiencer\n(?i)(?:(?:like_num (?:y[or]|y / o)|[\\d]+yo) (?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:patient |veteran )?(?:with|w|w/)(?: (?!&lt;IGNORE&gt;)\\S+){0,9} COVID-19)#patient_experiencer\n(?i)(?:the (?:veteran|vet|patient) have(?: (?!&lt;IGNORE&gt;)\\S+){0,2} COVID-19)#patient_experiencer\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,1} precaution)#future\n(?i)(?:(?:(?:precaution|protection|protect) (?:for|against)|concern about|reports of|vaccine|protect yourself|prevent(?:ed|ion|s|ing)|avoid)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#future\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:prevent(?:ed|ion|s|ing)|vaccine|educat\\S*|instruction))#future\n(?i)(?:(?:questions (?:about|regarding|re|concerning|on|for)|(?:anxiety|ask(?:ing|ed|es|ed)?) about|educat(?:ion|ed|ing|ed)?|instruction)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#future\n(?i)(?:(?:information(?: )?(?:on|about|regarding|re)?|protocols?)(?: (?!&lt;IGNORE&gt;)\\S+){0,2} COVID-19)#future\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,2} protocols?)#future\n(?i)(?:(?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?|(?:anxious|worr(?:ied|ies|y|ying)) (?:about|re|regarding))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#future\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?))#future\n(?i)(?:if(?: (?!&lt;IGNORE&gt;)\\S+){0,9} COVID-19)#future\n(?i)(?:(?:advisor(?:y|ies)|travel screen(?: :)?|Travel History Questionnaire|prescreen|front gate)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#screening\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,2} (?:questionnaire :|questionn?aire|question\\S*|prescreen|front gate))#screening\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,9} screen\\S*)#screening\n(?i)(?:screen\\S*(?: (?!&lt;IGNORE&gt;)\\S+){0,9} COVID-19)#screening\n(?i)(?:have you(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#not relevant\n(?i)(?:(?:mers)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:This patient was screened for the following suspected travel related illness(?:es)?)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#future\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? This patient was screened for the following suspected travel related illness(?:es)?)#future\n(?i)(?:(?:will(?: be) travel|travel plans|if you need|plan to travel)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#future\n(?i)(?:(?:(?:limit|reduce|lower|minimize)(?: the)? (?:risk|chance|possibility) of|if you)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#future\n(?i)(?:(?:(?:(?:-)?hx|history|) of)(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#negated\n(?i)(?:(?:^(?:check|test|retest|eval)(?: for)?)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#test\n(?i)(?:(?:work(?:-|\\s)up)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#test\n(?i)(?:(?:evaluation)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#test\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,1} (?:evaluation))#test\n(?i)(?:(?:swab|PCR|specimen sent)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#test\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:swab|PCR|specimen sent))#test\n(?i)(?:(?:awaiting results|at risk for|risk for|currently being ruled out or has tested positive for|to exclude)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:awaiting results|currently being ruled out or has tested positive for|(?:patient|person) of interest))#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,0} (?:risk))#uncertain\n(?i)(?:(?:investigation of)(?: (?!&lt;IGNORE&gt;)\\S+){0,0} COVID-19)#uncertain\n(?i)(?:(?:question of|differential diagnosis :|ddx :)(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#uncertain\n(?i)(?:(?:awaiting|questionnaire|r(?:/)?o(?:\\.)?)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,1} (?:awaiting|questionnaire|r(?:/)?o(?:\\.)?))#uncertain\n(?i)(?:(?:under investigation|(?:may|might) be positive(?: for)?|flew|tarvel(?:ed)?|travelled)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:under investigation|(?:may|might) be positive))#uncertain\n(?i)(?:(?:facility (?:with|has)(?: a)?|known to have|(?:same )?room|patients with)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:(?:area|county|community|city) (?:with|of)|in the building|(?:several|multiple|one)(?:of )?(?:the )? other_experiencer)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:in the building))#negated\n(?i)(?:(?:(?:he|she) thinks (?:he|she) (?:have|had|has)|\\S+ would like)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|unlikely to be)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|possible positive))#uncertain\n(?i)(?:(?:(?:possible|potential)? exposure|possibly|possible positive)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:(?:risk of|likely|probable|probably)(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#uncertain\n(?i)(?:(?:suspicion(?: for)?|^suspect|differential diagnosis|ddx(?: :)?|doubt)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:suspicion|^suspect|differential diagnosis|ddx(?: :)?|may have been exposed))#uncertain\n(?i)(?:(?:(?:positive )?(?:sign|symptom) of)(?: (?!&lt;IGNORE&gt;)\\S+){0,3} COVID-19)#uncertain\n(?i)(?:(?:sx|s/s|rule (?:- )out|be ruled out(?: for)?|^(?:vs\\.?|versus)$)(?: (?!&lt;IGNORE&gt;)\\S+){0,4} COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,4} (?:sx|s/s|rule (?:- )out|^(?:vs\\.?|versus)$))#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:(?:possible|potential)? exposure|may have been exposed))#uncertain\n(?i)(?:(?:concern(:?s)?(?: for| of)?|if (?:negative|positive)|c/f|assess(?:ed)? for|concerning for)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:(?:unlikely(?: to be positive)?|low (?:suspicion|probability|risk (?:for|in|of)))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:unlikely(?: to be positive)?|low (?:suspicion|probability)|is unlikely))#uncertain\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,2} (?:extremely low))#uncertain\n(?i)(?:(?:low risk of)(?: (?!&lt;IGNORE&gt;)\\S+){0,2} COVID-19)#uncertain\n(?i)(?:(?:(?:other_experiencer|family) ^test positive(?: for)?|any one|contact with(?: known))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:other_experiencer|family)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:other_experiencer|any one|contact with(?: known)))#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,0} (?:(?:a|an|another) \\S+ tested positive))#negated\n(?i)(?:(?:had contact|same (?:building|floor)|care for|clean)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:had contact|same (?:building|floor)|care for|clean))#negated\n(?i)(?:(?:concern(?:ed)? about)(?: (?!&lt;IGNORE&gt;)\\S+){0,2} COVID-19)#negated\n(?i)(?:(?:patient concern (?:for|of)|desire|(?:concerned|prepare) (?:for|about))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:seen in|a (?:positive|confirmed) case of|cases|epidemic|pandemic)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,1} (?:cases|epidemic|pandemic|national emergency|crisis|situation|mandate|\\?))#negated\n(?i)(?:(?:national emergency|crisis|situation|mandate)(?: (?!&lt;IGNORE&gt;)\\S+){0,1} COVID-19)#negated\n(?i)(?:(?:seen in(?: the)? setting of)(?: (?!&lt;IGNORE&gt;)\\S+){0,5} COVID-19)#negated\n(?i)(?:(?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off|goals :)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off))#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:in the (?:area|community)|outbreak))#negated\n(?i)(?:(?:in the (?:area|community)|outbreak)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation))#negated\n(?i)(?:(?:^read about|deploy|(?:come|been) in close contact(?: with)?)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:^read about|deploy|(?:come|been) in close contact(?: with)?|error))#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:have you had close contact|web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases))#negated\n(?i)(?:(?:have you had close contact|the group|session|(?:nurse(?:s)?|rn) notes)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases|error)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:(?:(?:person|patients) with(?: confirmed)?(?: or)?(?: suspected)?|cases of)(?: (?!&lt;IGNORE&gt;)\\S+){0,2} COVID-19)#negated\n(?i)(?:elective(?: (?!&lt;IGNORE&gt;)\\S+){0,4} COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,4} elective)#negated\n(?i)(?:(?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?))(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?)))#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:(?:someone|person) who (?:has|have) tested positive|contact with))#negated\n(?i)(?:(?:(?:someone|person) who (?:has|have) tested positive|contact with)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#negated\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+){0,0} (?:\\(resolved\\)))#positive\n(?i)(?:COVID-19(?: (?!&lt;IGNORE&gt;)\\S+)*? (?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :))#IGNORE\n(?i)(?:(?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :)(?: (?!&lt;IGNORE&gt;)\\S+)*? COVID-19)#IGNORE\n\n\n\nmagic_session.import_rel(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")\n\n\n%%spannerlog\n#covid_attributes: negated, other_experiencer, is_future, not_relevant, uncertain, positive\nContextMatches(CovidAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\n?ContextMatches(CovidAttribute, Span, Path, Sent)\n\nCovidSpans(Path, Span, Sent) &lt;- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -&gt; (Span)\n?CovidSpans(Path, Span, Sent)\n\nprinting results for query 'ContextMatches(CovidAttribute, Span, Path, Sent)':\n   CovidAttribute    |   Span   |    Path     |                        Sent\n---------------------+----------+-------------+----------------------------------------------------\n      positive       | [0, 17)  | sample5.txt |           positive COVID-19 precaution .\n       negated       | [0, 12)  | sample4.txt |              neg COVID-19 education .\n      positive       | [27, 48) | sample1.txt | His family recently tested positive for COVID-19 .\n      positive       | [0, 35)  | sample1.txt |       COVID-19 results came back positive .\n patient_experiencer | [0, 34)  | sample6.txt |        The patient have reported COVID-19 .\n       future        | [9, 28)  | sample5.txt |           positive COVID-19 precaution .\n       future        | [4, 22)  | sample4.txt |              neg COVID-19 education .\n       negated       | [4, 48)  | sample1.txt | His family recently tested positive for COVID-19 .\n\nprinting results for query 'CovidSpans(Path, Span, Sent)':\n    Path     |   Span   |                        Sent\n-------------+----------+----------------------------------------------------\n sample1.txt |  [0, 8)  |       COVID-19 results came back positive .\n sample1.txt | [40, 48) | His family recently tested positive for COVID-19 .\n sample1.txt | [34, 42) |    patient presents to be tested for COVID-19 .\n sample2.txt | [26, 34) |        The patient be tested for COVID-19 .\n sample3.txt |  [0, 8)  |                 COVID-19 like_num\n sample4.txt | [4, 12)  |              neg COVID-19 education .\n sample5.txt | [9, 17)  |           positive COVID-19 precaution .\n sample6.txt | [26, 34) |        The patient have reported COVID-19 .\n\n\n\n\n%%spannerlog\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent) &lt;- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -&gt; (CovidSpan)\n?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\n\nprinting results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n-------------+-------------+---------------------+----------------------------------------------------\n sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n\n\n\n\n\nPostprocessor:\nThe postprocessor is designed to apply extra adjustments to the processed text using custom logic or specific requirements not addressed by the spaCy pipeline. These rules modify, remove, or change attributes for each mention of COVID-19 based on either their existing attributes, the context of the sentences in which they appear, or a combination of both. This flexibility allows us to address data issues and implement targeted improvements. For instance, it proves useful in identifying and rectifying incorrectly labeled positive cases, thereby enhancing the accuracy of our classification.\nHow we implemented it:\nAs mentioned earlier, postprocess rules are responsible for modifying, removing, or changing attributes for each mention of COVID-19. In the original project, these attributes are represented as boolean variables stored in an object class for each COVID-19 mention. The rules simply switch the corresponding boolean variable to assign or remove the attribute. However, in spannerlog, we don’t have the luxury of creating classes. In our project, when we want to remove a specific attribute, we introduce an additional attribute that acts as its negation. For instance, for the attribute ‘positive,’ we add ‘no_positive,’ causing the document classifier to behave as if there is no positive attribute.\nAdditionally, in some cases, the entire COVID-19 mention is removed by eliminating its object. In our project, we introduce an ‘IGNORE’ attribute, which results in the exclusion of the mention from consideration in the document classifier stage. \nIn the subsequent cells, we will explore three types of postprocess rules: 1) Rules based on patterns 2) Rules utilizing existing attributes and patterns 3) Rules applied to the next sentence.\n\n1 - Postprocess rules based on patterns:\nExample rule in the original project:\nPostprocessingRule(\n        patterns=[\n            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n            PostprocessingPattern(\n                postprocessing_functions.sentence_contains,\n                condition_args=({\"deny\", \"denies\", \"denied\"},),\n            ),\n            PostprocessingPattern(\n                postprocessing_functions.sentence_contains,\n                condition_args=({\"contact\", \"contacts\", \"confirmed\"},),\n            ),\n        \\],\n        action=postprocessing_functions.remove_ent,\n        description=\"Remove a coronavirus entity if 'denies' and 'contact' are in. This will help get rid of false positives from screening.\",\n    ),    \nThis rule iterates through each entity and checks a series of conditions which are the “PostprocessingPattern”. If all conditions evaluate as True, then some action is taken on the entity, which is ‘remove’ action in this example.\nIn our case, we assign “IGNORE” attribute to the COVID-19 mention causing it to be excluded from consideration during the document classification process.\nEach rule in the CSV file follows this format: regexPattern, Attribute\n\nprint_csv_file('postprocess_pattern_rules.csv')\n\n.*education.*#IGNORE\n.* \\?#IGNORE\n(?=.*\\b(?:deny|denies|denied)\\b)(?=.*\\b(?:contact|confirm)\\b).*#IGNORE\n(?=.*\\b(?:setting of|s/o)\\b)(?!.*\\b(?:COVID-19 infection|COVID-19 ards)\\b).*#no_positive\n(?i)(.*benign.*)#uncertain\nadmitted to COVID-19 unit#positive\n\n\n\nmagic_session.import_rel(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")\n\n\n%%spannerlog\nPostprocessMatches(CovidAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\n?PostprocessMatches(CovidAttribute, Span, Path, Sent)\n\nprinting results for query 'PostprocessMatches(CovidAttribute, Span, Path, Sent)':\n  CovidAttribute  |  Span   |    Path     |           Sent\n------------------+---------+-------------+--------------------------\n      IGNORE      | [0, 24) | sample4.txt | neg COVID-19 education .\n\n\n\n\n%%spannerlog\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent) &lt;- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -&gt; (CovidSpan)\n?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\n\nprinting results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n-------------+-------------+---------------------+----------------------------------------------------\n sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n\n\n\n\n\n2 - Postprocess rules utilizing existing attributes and patterns:\nPostprocessingRule(\n        patterns=[\n        \n            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n            PostprocessingPattern(\n                postprocessing_functions.is_modified_by_category,\n                condition_args=(\"DEFINITE_POSITIVE_EXISTENCE\",),\n            ),\n            # PostprocessingPattern(postprocessing_functions.is_modified_by_category, condition_args=(\"TEST\",)),\n            PostprocessingPattern(\n                postprocessing_functions.sentence_contains,\n                condition_args=(\n                    {\n                        \"should\",\n                        \"unless\",\n                        \"either\",\n                        \"if comes back\",\n                        \"if returns\",\n                        \"if s?he tests positive\",\n                    },\n                    True,\n                ),\n            ),\n        ],\n        action=set_is_uncertain,\n        action_args=(True,),\n        description=\"Subjunctive of test returning positive. 'Will contact patient should his covid-19 test return positive.'\",\n    ),\nThis rule examines whether a COVID-19 mention possesses a positive attribute and if the sentence containing it includes any of the words specified in ‘condition_args’ If these conditions are met, the uncertain attribute is set to true.\nIn our case, we check for each COVID-19 mention in the ‘CovidAttributes’ table if it’s labeled as ‘positive’, also, we check if any of the specified words in ‘condition_args’ are present in the same sentence using a regex search. If the conditions are met, then we simply assign it an ‘uncertain’ attribute.\nEach rule in the CSV file follows this format: regexPattern, ExistingAttribute, NewAttribute\n\nprint_csv_file('postprocess_attributes_rules.csv')\n\n.*pending.*#negated#no_negated\n.*(?:should|unless|either|if comes back|if returns|if s?he tests positive).*#positive#uncertain\n.*precaution.*#positive#no_future\n.*(?:re[ -]?test|second test|repeat).*#negated#no_negated\n.*(?:sign|symptom|s/s).*#positive#uncertain\n\n\n\nmagic_session.import_rel(\"postprocess_attributes_rules.csv\", relation_name=\"PostprocessRulesWithAttributes\", delimiter=\"#\")\n\n\n%%spannerlog\nPostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), PostprocessRulesWithAttributes(Pattern, CovidAttribute, NewAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\n?PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)\n\nprinting results for query 'PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)':\n  CovidAttribute  |  NewAttribute  |  Span   |    Path     |              Sent\n------------------+----------------+---------+-------------+--------------------------------\n     positive     |   no_future    | [0, 30) | sample5.txt | positive COVID-19 precaution .\n\n\n\n\n%%spannerlog\nCovidAttributes(Path, CovidSpan, NewAttribute, Sent) &lt;- CovidAttributes(Path, CovidSpan, CovidAttribute, Sent), PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)\n?CovidAttributes(Path, CovidSpan, NewAttribute, Sent)\n\nprinting results for query 'CovidAttributes(Path, CovidSpan, NewAttribute, Sent)':\n    Path     |  CovidSpan  |    NewAttribute     |                        Sent\n-------------+-------------+---------------------+----------------------------------------------------\n sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n\n\n\n\n\n3 - Postprocess rules applied to the next sentence:\nThere’s a rule that checks if the following sentence contains positive mentions. If it does, the COVID-19 mentions in the current sentence are also marked as positive. To Implement this rule in our project, we defined a new relation that pairs each sentence with its subsequent sentence.\n\ndef next_sent(text_path):\n    with open(text_path, 'r') as file:\n        contents = file.read()\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    # Tokenize sentences\n    sentences = list(doc.sents)\n    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n        yield(sentences[i].text, sentences[i + 1].text)\n\nmagic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])\n\n\n# usage example\nfor first_sent, second_sent in next_sent(\"sample1.txt\"):\n    print(f\"sentence: {first_sent}\", f\"next sentence: {second_sent}\")\n\nsentence: patient presents to be tested for COVID-19 . next sentence: His family recently tested positive for COVID-19 .\nsentence: His family recently tested positive for COVID-19 . next sentence: COVID-19 results came back positive .\n\n\n\n%%spannerlog\nNextSent(Path, Sent1, Sent2) &lt;- FilesPaths(Path), next_sent(Path) -&gt; (Sent1, Sent2)\n?NextSent(Path, Sent1, Sent2)\n\nprinting results for query 'NextSent(Path, Sent1, Sent2)':\n    Path     |                       Sent1                        |                       Sent2\n-------------+----------------------------------------------------+----------------------------------------------------\n sample1.txt | His family recently tested positive for COVID-19 . |       COVID-19 results came back positive .\n sample1.txt |    patient presents to be tested for COVID-19 .    | His family recently tested positive for COVID-19 .\n sample2.txt |        The patient be tested for COVID-19 .        |               Results be positive .\n sample3.txt |                associated_diagnosis                |                     like_num .\n sample3.txt |                     like_num .                     |                 COVID-19 like_num\n sample3.txt |             problem_list : like_num .              |                associated_diagnosis\n\n\n\n\n%%spannerlog\nnew PostProcessWithNextSentenceRules(str, str)\nPostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\nPostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\n\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) &lt;- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\n\nprinting results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n-------------+-------------+---------------------+----------------------------------------------------\n sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n sample2.txt |  [26, 34)   |      positive       |        The patient be tested for COVID-19 .\n sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n\n\n\n\n\n\nDocument Classifier:\nNow we have the basic pieces in place to make our document classification. Each document is classified as either ‘POS’, ‘UNK’, or ‘NEG’ determined by the attributes of its COVID-19 mentions. The Results are stored in a DataFrame.\nDocument Classifier stage has 2 parts: 1) Attribute filtering: Our pipeline assigns various attributes to each COVID-19 mention. However, during this stage, each COVID-19 case is refined to possess only one attribute. This filtering process operates based on specific conditions outlined in the ‘attribute_filter’ function. 2) Document classification: Documents are classified based on distinct conditions, as detailed in the ‘classify_doc_helper’ function. This step ensures the accurate categorization of each document according to the specified criteria.\n\ndef attribute_filter(group):\n    \"\"\"\n    Filters attributes within each \"CovidSpan\" of a DataFrame table based on specific conditions.\n\n    Parameters:\n        group (pandas.Series): A pandas Series representing attributes for each \"CovidSpan\" within a DataFrame.\n\n    Returns:\n        str: Filtered \"CovidSpan\" attribute determined by the following rules:\n            - If 'IGNORE' is present, returns 'IGNORE'.\n            - If 'negated' is present (and 'no_negated' is not present), returns 'negated'.\n            - If 'future' is present (and 'no_future' is not present), returns 'negated'.\n            - If 'other experiencer' or 'not relevant' is present, returns 'negated'.\n            - If 'positive' is present (and 'uncertain' and 'no_positive' are not present), returns 'positive'.\n            - Otherwise, returns 'uncertain'.\n    \"\"\"\n    if 'IGNORE' in group.values:\n        return 'IGNORE'\n    elif 'negated' in group.values and not 'no_negated' in group.values:\n        return 'negated'\n    elif 'future' in group.values and not 'no_future' in group.values:\n        return 'negated'\n    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n        return 'negated'\n    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n        return 'positive'\n    else:\n        return 'uncertain'\n\n\n# usage example\ndata = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n        'Attribute': ['IGNORE', 'negated', 'positive', 'positive']}\ndf_example = pd.DataFrame(data)\nprint(\"Before:\")\nprint(df_example)\n\ndf_example['Attribute'] = df_example.groupby(['Path'])['Attribute'].transform(attribute_filter)\ndf_example = df_example.drop_duplicates().reset_index(drop=True)\nprint(\"\\nAfter:\")\nprint(df_example)\n\nBefore:\n          Path Attribute\n0  sample1.txt    IGNORE\n1  sample1.txt   negated\n2  sample1.txt  positive\n3  sample2.txt  positive\n\nAfter:\n          Path Attribute\n0  sample1.txt    IGNORE\n1  sample2.txt  positive\n\n\n\ndf = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\nif len(df) == 0:\n    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\ndf['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\ndf = df.drop_duplicates().reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\n\nPath\nCovidSpan\nCovidAttribute\nSent\n\n\n\n\n0\nsample1.txt\n[0, 8)\npositive\nCOVID-19 results came back positive .\n\n\n1\nsample1.txt\n[40, 48)\nnegated\nHis family recently tested positive for COVID-...\n\n\n2\nsample2.txt\n[26, 34)\npositive\nThe patient be tested for COVID-19 .\n\n\n3\nsample3.txt\n[0, 8)\npositive\nCOVID-19 like_num\n\n\n4\nsample4.txt\n[4, 12)\nIGNORE\nneg COVID-19 education .\n\n\n5\nsample5.txt\n[9, 17)\npositive\npositive COVID-19 precaution .\n\n\n6\nsample6.txt\n[26, 34)\nuncertain\nThe patient have reported COVID-19 .\n\n\n\n\n\n\n\n\n\ndef classify_doc_helper(group):\n    \"\"\"\nClassifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n\nParameters:\n    group (pandas.Series): A pandas Series representing COVID-19 attributes for each document within a DataFrame.\n    \nReturns:\n    str: Document classification determined as follows:\n         - 'POS': If at least one COVID-19 attribute with \"positive\" is present in the group.\n         - 'UNK': If at least one COVID-19 attribute with \"uncertain\" is present in the group and no \"positive\" attributes,\n                  or there's at least one COVID-19 attribute with 'IGNORE' and no other COVID-19 attributes exist.\n         - 'NEG': Otherwise.\n\"\"\"\n    if 'positive' in group.values:\n        return 'POS'\n    elif 'uncertain' in group.values:\n        return 'UNK'\n    elif 'negated' in group.values:\n        return 'NEG'\n    else:\n        return 'UNK'\n\n\n# usage example\ndata = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n        'Attribute': ['uncertain', 'negated', 'positive', 'positive']}\ndf_example = pd.DataFrame(data)\nprint(\"Before:\")\nprint(df_example)\n\ndf_example['DocResult'] = df_example.groupby(['Path'])['Attribute'].transform(classify_doc_helper)\ndf_example = df_example[['Path', 'DocResult']]\ndf_example = df_example.drop_duplicates().reset_index(drop=True)\nprint(\"\\nAfter:\")\nprint(df_example)\n\nBefore:\n          Path  Attribute\n0  sample1.txt  uncertain\n1  sample1.txt    negated\n2  sample1.txt   positive\n3  sample2.txt   positive\n\nAfter:\n          Path DocResult\n0  sample1.txt       POS\n1  sample2.txt       POS\n\n\n\ndf['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\ndf = df[['Path', 'DocResult']]\ndf = df.drop_duplicates().reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\n\nPath\nDocResult\n\n\n\n\n0\nsample1.txt\nPOS\n\n\n1\nsample2.txt\nPOS\n\n\n2\nsample3.txt\nPOS\n\n\n3\nsample4.txt\nUNK\n\n\n4\nsample5.txt\nPOS\n\n\n5\nsample6.txt\nUNK\n\n\n\n\n\n\n\n\n\nHandling unmentioned paths:\nAt this step, we assign a classification result ‘UNK’ to paths not identified in the previous DataFrame result. This occurs when our pipeline doesn’t detect any mention of COVID-19 or its synonyms in the text of those paths. As a result, these paths are excluded from all types of relations, consistent with our primary focus on COVID-19 entities.\n\ndf_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\ndf = (pd.merge(df, df_path, on='Path', how='outer'))\ndf['DocResult'] = df['DocResult'].fillna(\"UNK\")\ndf\n\n\n\n\n\n\n\n\n\nPath\nDocResult\n\n\n\n\n0\nsample1.txt\nPOS\n\n\n1\nsample2.txt\nPOS\n\n\n2\nsample3.txt\nPOS\n\n\n3\nsample4.txt\nUNK\n\n\n4\nsample5.txt\nPOS\n\n\n5\nsample6.txt\nUNK\n\n\n6\nsample7.txt\nUNK",
    "crumbs": [
      "Tutorials",
      "Covid-19 NLP Pipeline"
    ]
  },
  {
    "objectID": "covid-nlp/covid_pipeline.html#bringing-it-all-together",
    "href": "covid-nlp/covid_pipeline.html#bringing-it-all-together",
    "title": "Covid-19 NLP Pipeline",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together\nIn this section, we will directly compare the original Python Spacy pipeline project with its spannerlog counterpart. Our emphasis is on showcasing the overall brevity of the spannerlog implementation in contrast to the Python Spacy pipeline.\n\nCode Metrics\nLet’s commence by providing an estimated count of total lines in each implementation:\n\nTotal Number of Lines in the original Python implementation: 4435\nTotal Number of Lines in our spannerlog implementation: 596 (7 times smaller!)\n\nAnd here’s a detailed comparison:\n\n\n\ncode line comparison\n\n\nWith the caveat that number of lines do not fully capture code complexity, let us analyze the lines of code a little more in depth. Analyzing our implementation vs the original we note that:\n\nWe used the same libraries as the original implementations, so both\n\nthe core computations, that should turn into ie functions\nthe wrapping logic which remains in pure python did not significantly change in size.\n\neven if we assume that our 203 lines of python code are worth over 300 lines of the original implementations core and wrapping logic, we are still left with over 4000 lines of code that were converted into 393 (107+251+35) of either declarative code and data.\nThis means that over 90% of the original code base, which constitutes control flow and data ingestion logic, underwent a ten-fold decrease in size while providing less surface areas for errors since declarative languages and data can be statically analyzed to a greater extent than imperative code.\n\n\n\nImplementation - raw lines of code\nNow, we will present the combined spannerlog and python code (excluding “generic ie” functions and excluding queries) to visually illustrate the compactness of the implementation:\n\nConcept tagger:\ndef lemmatize_text(text_path, lemma_words_path):\n    # Define a list of words to be lemmatized\n    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n\n    with open(text_path, 'r') as file:\n        contents = file.read()\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    lemmatized_text = \"\"\n    for token in doc:\n        if token.lemma_ in lemma_words:\n            lemmatized_text += token.lemma_\n        elif token.like_num:\n            lemmatized_text += \"like_num\"\n        else:\n            lemmatized_text += token.text\n        lemmatized_text += \" \"\n\n    # Write the lemmatized text back to the same file\n    with open(text_path, 'w') as file:\n        file.writelines(lemmatized_text)\n\n    yield lemmatized_text\n\ndef annotate_text_with_pos(text_path):\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    for token in doc:\n        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n        else:\n            yield tuple()\nsession.import_rel(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")\n\n%%spannerlog\nLemmaMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -&gt; (Span)\nreplace_spans(\"LemmaMatches\", \"FilesPaths\")\nPOSTable(POS, Span, Path) &lt;- FilesContent(Path, Content), annotate_text_with_pos(Path) -&gt; (POS, Span)\nPOSMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -&gt; (Span)\nPOSRuleMatches(Label, Span, Path) &lt;- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\nreplace_spans(\"POSRuleMatches\", \"FilesPaths\")\n\n\nTarget matcher:\nmagic_session.import_rel(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")\n\n%%spannerlog\nTargetTagMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content,Pattern) -&gt; (Span)\nreplace_spans(\"TargetTagMatches\", \"FilesPaths\")\n\n\nSectionizer:\n\nmagic_session.import_rel(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")\n\n%%spannerlog\nSectionRulesMatches(Label, Span, Path) &lt;- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content,Pattern) -&gt; (Span)\nreplace_spans(\"SectionRulesMatches\", \"FilesPaths\")\n\npattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n\nnew SectionRulesAttribute(str, str)\nSectionRulesAttribute(pattern, \"positive\")\nSectionMatches(Path, Span, CovidAttribute) &lt;- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -&gt; (Span)\nCovidMatches(Path, Span) &lt;- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -&gt; (Span)\nSectionCovidAttributes(Path, CovidSpan, CovidAttribute) &lt;- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -&gt; (CovidSpan)\n\nSents(Path, Sent) &lt;- FilesPaths(Path), sent_tokenization(Path) -&gt; (Sent)\nSentSpans(Path, Sent, SentSpan) &lt;- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -&gt; (SentSpan)\n\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent) &lt;- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\nSentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -&gt; (CovidSpan)\n \n\n\nContext matcher:\nmagic_session.import_rel(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")\n\n%%spannerlog\nContextMatches(CovidAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\nCovidSpans(Path, Span, Sent) &lt;- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -&gt; (Span)\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent) &lt;- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -&gt; (CovidSpan)\n\n\nPostprocessor:\ndef next_sent(text_path):\n    with open(text_path, 'r') as file:\n        contents = file.read()\n\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(contents)\n\n    # Tokenize sentences\n    sentences = list(doc.sents)\n    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n        yield(sentences[i].text, sentences[i + 1].text)\n\nmagic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])\nmagic_session.import_relation_from_csv(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")\n\n%%spannerlog\nPostprocessMatches(CovidAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent) &lt;- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -&gt; (CovidSpan)\n\nNextSent(Path, Sent1, Sent2) &lt;- FilesPaths(Path), next_sent(Path) -&gt; (Sent1, Sent2)\nnew PostProcessWithNextSentenceRules(str, str)\nPostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\nPostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) &lt;- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\npy_rgx_span(Sent, Pattern) -&gt; (Span)\nCovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) &lt;- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n\n\nDocument Classifier:\ndef attribute_filter(group):\n    if 'IGNORE' in group.values:\n        return 'IGNORE'\n    elif 'negated' in group.values and not 'no_negated' in group.values:\n        return 'negated'\n    elif 'future' in group.values and not 'no_future' in group.values:\n        return 'negated'\n    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n        return 'negated'\n    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n        return 'positive'\n    else:\n        return 'uncertain'\n\ndf = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\nif len(df) == 0:\n    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\ndf['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\ndf = df.drop_duplicates().reset_index(drop=True)\n\ndef classify_doc_helper(group):\n    if 'positive' in group.values:\n        return 'POS'\n    elif 'uncertain' in group.values:\n        return 'UNK'\n    elif 'negated' in group.values:\n        return 'NEG'\n    else:\n        return 'UNK'\n        \ndf['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\ndf = df[['Path', 'DocResult']]\ndf = df.drop_duplicates().reset_index(drop=True)\n\ndf_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\ndf = (pd.merge(df, df_path, on='Path', how='outer'))\ndf['DocResult'] = df['DocResult'].fillna(\"UNK\")\ndf",
    "crumbs": [
      "Tutorials",
      "Covid-19 NLP Pipeline"
    ]
  },
  {
    "objectID": "engine.html",
    "href": "engine.html",
    "title": "Engine",
    "section": "",
    "text": "Here is an overview of the engine module, explaining where data is stored, the structure of database tables, and how rules and queries operate within the system.",
    "crumbs": [
      "src",
      "Engine",
      "Engine"
    ]
  },
  {
    "objectID": "engine.html#data-storage",
    "href": "engine.html#data-storage",
    "title": "Engine",
    "section": "Data Storage",
    "text": "Data Storage\n\nWhere is all the data about the relations stored?\nData about relations is stored in a SQLite3 database.\n\nSQLite3: A module in Python’s standard library that offers a lightweight and self-contained relational database management system. It enables creating, connecting to, and manipulating SQLite databases using Python.",
    "crumbs": [
      "src",
      "Engine",
      "Engine"
    ]
  },
  {
    "objectID": "engine.html#database-table-structure",
    "href": "engine.html#database-table-structure",
    "title": "Engine",
    "section": "Database Table Structure",
    "text": "Database Table Structure\n\nWhat specific data is stored in the db tables, and what is the structure of these tables?\nWhenever a relation is defined, a corresponding table is created in the database.  The table has the same name as the relation, and its columns correspond to the variables in the relation—i.e., len(term_list). the names of columns in the database don’t have an informative name and there is no need because from Python’s perspective it doesn’t understand what the relation or its values mean, it just sorts information in the way they were provided, for example brothers(“Jack”,“Michael”) from Python’s perspective it just sees that “Jack” is the first term so it puts it in the col0 and “Michael” is the second so it gets added to col1.\n\nExample\nFor example, if we define a new relation brothers(str, str):\n%%spannerlog\nnew brothers(str, str)\nThe engine will create for us a new table for relation brothers with two columns called col0 and col1.",
    "crumbs": [
      "src",
      "Engine",
      "Engine"
    ]
  },
  {
    "objectID": "engine.html#rules-and-queries",
    "href": "engine.html#rules-and-queries",
    "title": "Engine",
    "section": "Rules and Queries",
    "text": "Rules and Queries\n\nHow Do Rules and Queries Work?\nWhen a new rule is added to the system, an empty table is created in the database.  This table remains empty initially because multiple rules can share the same rule head.  During the querying process, the engine iterates through the list of rules, and the query result is a logical OR of these rules.\n\nExample\nFor instance, if we define the rules:\nA(X,Y) &lt;- B(X,Y)\nA(X,Y) &lt;- C(X,Y)\nresult would include all tuples that satisfy either B(X, Y) OR C(X, Y).",
    "crumbs": [
      "src",
      "Engine",
      "Engine"
    ]
  },
  {
    "objectID": "engine.html#queries",
    "href": "engine.html#queries",
    "title": "Engine",
    "section": "Queries",
    "text": "Queries\n\nQuery Decomposition and Rule Breakdown\nTo understand querying in-depth, it’s beneficial to break down a rule into its constituent parts.\n\nExample: Grandparent Rule\nFor example, let’s consider the rule:\ngrandparent(X, Z) &lt;- parent(X, Y), parent(Y, Z)\nthe rule head: grandparent(X,Z)  the rule body: relations: [parent(X,Y), parent(Y,Z)]\nThere are 4 operators in the engine that get called when breaking that rule, we will break down each one of them below.\n\n\n\n\n\n\nNote\n\n\n\nEach operator yields a relation that is added to the database temporarily. (deleted when we’re done querying).\n\n\n\nsource\n\n\n\nspannerlogEngineBase\n\n spannerlogEngineBase ()\n\nAn abstraction for a spannerlog execution engine, used by GenericExecution. it includes relational algebra operators like join and project, database modification operators like add_fact (insert) and remove_fact (delete), and spannerlog-specific operators like compute_ie_relation.\n\nsource\n\n\nspannerlogEngineBase.declare_relation_table\n\n spannerlogEngineBase.declare_relation_table\n                                              (relation_decl:spannerlib.as\n                                              t_node_types.RelationDeclara\n                                              tion)\n\nDeclares a relation in the spannerlog engine.\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_decl\nRelationDeclaration\na relation declaration\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the relation is already declared does nothing.\n\n\n\nsource\n\n\nspannerlogEngineBase.add_fact\n\n spannerlogEngineBase.add_fact (fact:spannerlib.ast_node_types.AddFact)\n\nAdds a fact to the spannerlog engine.\n\n\n\n\nType\nDetails\n\n\n\n\nfact\nAddFact\nthe fact to be added\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nspannerlogEngineBase.remove_fact\n\n spannerlogEngineBase.remove_fact\n                                   (fact:spannerlib.ast_node_types.RemoveF\n                                   act)\n\nRemoves a fact from the spannerlog engine.\n\n\n\n\nType\nDetails\n\n\n\n\nfact\nRemoveFact\nthe fact to be removed\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nspannerlogEngineBase.query\n\n spannerlogEngineBase.query (query:spannerlib.ast_node_types.Query)\n\n*Queries the spannerlog engine. Outputs a preformatted query result, e.g. [(“a”,5),(“b”,6)]. notice that query isn’t a string; it’s a Query object which inherits from Relation.  for example, parsing the string ?excellent(\"bill\",\"ted\") yields the following Query:\nrelation_name = excellent\nterm_list = [\"bill\", \"ted\"]\ntype_list = [DataTypes.string, DataTypes.string]\n```*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| query | Query | a query for the spannerlog engine |\n| **Returns** | **List[Tuple]** | **a list of tuples that are the query's results** |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L110){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase.remove_tables\n\n&gt;      spannerlogEngineBase.remove_tables (tables_names:Iterable[str])\n\n*Removes all the tables inside the input from sql.*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| tables_names | Iterable[str] | tables to remove |\n| **Returns** | **None** |  |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase.clear_relation\n\n&gt;      spannerlogEngineBase.clear_relation (table:str)\n\n*Resets the table (deletes all its tuples).*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| table | str | tables to reset |\n| **Returns** | **None** |  |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase.clear_tables\n\n&gt;      spannerlogEngineBase.clear_tables (tables_names:Iterable[str])\n\n*Resets all the tables inside the input (deletes all their tuples).*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| tables_names | Iterable[str] | tables to reset |\n| **Returns** | **None** |  |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase.get_table_len\n\n&gt;      spannerlogEngineBase.get_table_len (table:str)\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| table | str | name of a table |\n| **Returns** | **int** | **number of tuples inside the table** |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L143){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase.compute_ie_relation\n\n&gt;      spannerlogEngineBase.compute_ie_relation\n&gt;                                                (ie_relation:spannerlib.ast_nod\n&gt;                                                e_types.IERelation, ie_func:spa\n&gt;                                                nnerlib.ie_function.IEFunction,\n&gt;                                                bounding_relation:Union[spanner\n&gt;                                                lib.ast_node_types.Relation,Non\n&gt;                                                eType])\n\n*Computes an information extraction relation, returning the result as a normal relation.\n\nsince ie relations may have input free variables, we need to use another relation to determine the inputs\nfor ie_relation.\nEach free variable that appears as an ie_relation input term must appear at least once in the\nbounding_relation terms.\nShould the ie relation not have any input free variables, bounding_relation can (but not must) be None\n\nnote the an ie relation is effectively two relations: a relation that defines the inputs to an ie function\nand a relation that filters the output of the ie functions.\nseeing that, we can define a general algorithm for this function:\n\n1. using the input relation to the ie function, filter \"bounding_relation\" into a relation who's\ntuples are all of the inputs to the ie function. if the input relation has no free variable terms,\nit is basically a fact, i.e., a single tuple, meaning we could skip this step and just use that tuple\nas an input instead.\n\n2. run the ie function on each one of the tuples of the relation we created in step 1, and save the\nresults to a new relation (the output relation)\n\n3. use the output relation of the ie function, filter the results we got in step 2 to a new relation\n\nexample:\nlet's follow the steps for the ie relation \"RGX&lt;X,Y&gt;-&gt;(Z,[1,2))\" with the bounding relation \"bind(X,Y,W)\":\n\n1. input_relation(X,Y) &lt;- bind(X,Y,W)  # filter the bounding relation tuples into the input relation\n\n2. for each tuple in input_relation(X,Y), use it as an input for RGX(x,y) and save the result to a new\nrelation called output_relation\n\n3. filter the outputs of the ie function into a relation like this\nfinal_ie_result(X,Y,Z) &lt;- input_relation(X,Y), output_relation(Z,[1,2))\nfor details on this join operation see \"spannerlogEngineBase.join_relations\"\n\nnote that \"final_ie_result\" is defined using only free variables, as an ie relation in spannerlog is merely\na part of a rule body, which serves to define the rule head relation. the rule head only \"cares\" about\nfree variables, so we can throw away all of the columns defined by constant terms.*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| ie_relation | IERelation | an ie relation that determines the input and output terms of the ie function |\n| ie_func | IEFunction | the data for the ie function that will be used to compute the ie relation |\n| bounding_relation | Optional[Relation] | a relation that contains the inputs for ie_funcs. the actual input needs to be queried from it |\n| **Returns** | **Relation** | **a normal relation that contains all of the resulting tuples in the spannerlog engine** |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L190){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase._convert_relation_term_to_string_or_int\n\n&gt;      spannerlogEngineBase._convert_relation_term_to_string_or_int\n&gt;                                                                    (datatype:s\n&gt;                                                                    pannerlib.p\n&gt;                                                                    rimitive_ty\n&gt;                                                                    pes.DataTyp\n&gt;                                                                    es,\n&gt;                                                                    term:Any)\n\n*Return the string/int representation of a relation term, e.g. \"[1,4)\"*\n\n|    | **Type** | **Details** |\n| -- | -------- | ----------- |\n| datatype | DataTypes | the type of the term |\n| term | Any | the term object itself |\n| **Returns** | **Union[str, int]** | **string/int representation** |\n\n\n---\n\n[source](https://github.com/DeanLight/spannerlib/blob/master/spannerlib/engine.py#L200){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### spannerlogEngineBase.operator_select\n\n&gt;      spannerlogEngineBase.operator_select\n&gt;                                            (relation:spannerlib.ast_node_types\n&gt;                                            .Relation, select_info:Set[Tuple[in\n&gt;                                            t,Any,spannerlib.primitive_types.Da\n&gt;                                            taTypes]], *args:Any)\n\n*The [`operator_select`](https://DeanLight.github.io/spannerlib/engine.html#operator_select) function operates on a relation, which is represented as a database table, and a set of constant variables. Its primary objective is to filter tuples from the given relation based on certain conditions and return the filtered relation.\nFor instance, consider the rule: \n```prolog\nbrothers_of_jack(X) &lt;- brothers(X, \"Jack\")\nThe goal is to print all the brothers of “Jack” by retrieving tuples from the “brothers” relation table in the database where “Jack” appears on the right side of the tuple. The operator_select function serves precisely this purpose.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation\nRelation\nthe relation from which we select tuples\n\n\nselect_info\nSet[Tuple[int, Any, DataTypes]]\nset of tuples. each tuple contains the index of the column, the value to select and the type of the column\n\n\nargs\nAny\n\n\n\nReturns\nRelation\na filtered relation\n\n\n\n\nsource\n\n\nspannerlogEngineBase.operator_join\n\n spannerlogEngineBase.operator_join\n                                     (relations:List[spannerlib.ast_node_t\n                                     ypes.Relation], *args:Any)\n\n*The operator_join function operates on a list of relations and its primary objective is to generate a logical AND operator among the tables of those relations within the database.  This function plays a crucial role in situations where the following rule is defined: \ngrandparent(X,Z) &lt;- parent(X,Y), parent(Y,Z)\nThe purpose of creating a relation through the operator_join function is to establish a connection between the columns containing all possible combinations of triplets (X, Y, Z) where both parent(X,Y) and parent(Y,Z) evaluate to true.  The reason for having three columns instead of two is to ensure that we have an answer for every query pertaining to the grandparent rule.  With three free variables, it becomes essential to capture all possible combinations in order to provide comprehensive responses to queries.  Subsequently, the columns required by a specific query can be projected at a later stage. *\n\n\n\n\nType\nDetails\n\n\n\n\nrelations\nList[Relation]\na list of normal relations\n\n\nargs\nAny\n\n\n\nReturns\nRelation\na new relation as described above\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt should be noted that the relation generated at this point represents the logical AND operation applied to all the relations within the rule’s body relation, considering the free variables involved.\n\n\n\nsource\n\n\nspannerlogEngineBase.operator_project\n\n spannerlogEngineBase.operator_project\n                                        (relation:spannerlib.ast_node_type\n                                        s.Relation,\n                                        project_vars:List[str], *args:Any)\n\n*The operator_project function operates on a relation, which is represented as a database table, and a list of project variables.  Its primary objective is to yield a relation with the projected variables of the src_relation.\nContinuing from the previous example, once the select relation table is added to the database, it consists of two columns where “Jack” appears on the right-hand side.  However, the purpose of the query is to print only a single column containing the brothers of “Jack”, without including the right column that holds the value “Jack”. \nTo address this requirement, the operator_project comes into play.  Its primary objective is to operate on the relation resulting from the select operation and generate another relation where “Jack” no longer appears on the right-hand side.*\n\n\n\n\nType\nDetails\n\n\n\n\nrelation\nRelation\nthe relation on which we project\n\n\nproject_vars\nList[str]\na list of variables on which we project\n\n\nargs\nAny\n\n\n\nReturns\nRelation\nthe projected relation\n\n\n\n\nsource\n\n\nspannerlogEngineBase.operator_union\n\n spannerlogEngineBase.operator_union\n                                      (relations:List[spannerlib.ast_node_\n                                      types.Relation], *args:Any)\n\n*a rule can be defined multiple times as long as it shares the same rule head. To illustrate this, let’s consider the following example:\nSuppose we have the rules:\nA(X,Y) &lt;- B(X,Y)\nA(X,Y) &lt;- C(X,Y)\nIf we execute the query ?A(X,Y), the engine will produce results that include tuples from both B(X,Y) and C(X,Y) relations.\nHere’s how the engine handles this scenario:  1. Rule Matching: The engine identifies all the rules that have the same rule head (A(X,Y)) as the queried relation.  2. Rule Execution: For each matched rule, the engine executes all the steps described earlier, such as assigning temporary names, performing joins, and creating intermediate relations, selecting and filtering.  3. Intermediate Relations: At this stage, we have intermediate relations generated for each rule, containing the relevant tuples based on the rule’s body.  4. Operator Union: To consolidate the results from all the rules into a single unified relation, the operator union is applied. This operation combines the tuples from all the intermediate relations. \nThe operator_union ensures that the final unified relation contains all the tuples from the individual intermediate relations, effectively merging the results obtained from different rule instances with the same rule head. In summary, the engine processes each rule with the same rule head as the queried relation, performs the necessary steps for each rule individually, and then applies the operator union to merge the intermediate relations into a single unified relation, ensuring all relevant tuples are included in the final result. @note: we assume that all the relations have same free_vars in the same order.*\n\n\n\n\nType\nDetails\n\n\n\n\nrelations\nList[Relation]\na list of relations to unite\n\n\nargs\nAny\n\n\n\nReturns\nRelation\nthe united relation\n\n\n\n\nsource\n\n\nspannerlogEngineBase.operator_copy\n\n spannerlogEngineBase.operator_copy\n                                     (src_rel:spannerlib.ast_node_types.Re\n                                     lation, output_relation:Union[spanner\n                                     lib.ast_node_types.Relation,NoneType]\n                                     =None, *args:Any)\n\nCopies computed_relation to rule_relation.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_rel\nRelation\n\nthe relation to copy from\n\n\noutput_relation\nOptional[Relation]\nNone\nif this is None, create a unique name for the output relation. otherwise, this will be the name of the output relation\n\n\nargs\nAny\n\n\n\n\nReturns\nRelation\n\nthe copied relation\n\n\n\n\nsource\n\n\nSqliteEngine\n\n SqliteEngine (database_name:Optional[str]=None)\n\nin this implementation of the engine, we use python’s sqlite3, which allows creating an SQL database easily, without using servers. the engine is called from GenericExecution, and uses run_sql as an interface to the database, which queries/modifies a table. each operator method implements a relational algebra operator by constructing an SQL command and executing it.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndatabase_name\nOptional[str]\nNone\nopen an existing database instead of a new one\n\n\n\n\n\n\nSqliteEngine.is_table_exists\n\n SqliteEngine.is_table_exists (table_name:str)\n\nChecks whether a table exists in the database.\n\n\n\n\nType\nDetails\n\n\n\n\ntable_name\nstr\nthe table which is checked for existence.\n\n\nReturns\nbool\nTrue if it exists, else False.\n\n\n\n\n\n\nSqliteEngine.remove_table\n\n SqliteEngine.remove_table (table_name:str)\n\nRemoves a table from the sql database, if it exists.\n\n\n\n\nType\nDetails\n\n\n\n\ntable_name\nstr\nthe table to remove\n\n\n\n\n\n\nSqliteEngine.remove_tables\n\n SqliteEngine.remove_tables (table_names:Iterable[str])\n\nRemoves the given tables from sql.\n\n\n\n\nType\nDetails\n\n\n\n\ntable_names\nIterable[str]\ntables to remove\n\n\n\n\nsource\n\n\noperator_copy\n\n operator_copy (src_rel:spannerlib.ast_node_types.Relation,\n                output_relation:Union[spannerlib.ast_node_types.Relation,N\n                oneType]=None, *args:Any)\n\nSee spannerlogEngineBase.operator_copy for explanation\n\n\n\nSqliteEngine.compute_ie_relation\n\n SqliteEngine.compute_ie_relation\n                                   (ie_relation:spannerlib.ast_node_types.\n                                   IERelation, ie_func:spannerlib.ie_funct\n                                   ion.IEFunction, bounding_relation:Union\n                                   [spannerlib.ast_node_types.Relation,Non\n                                   eType])\n\nComputes an information extraction relation, returning the result as a normal relation. for more details see spannerlogEngineBase.compute_ie_relation. notice comments below regarding constants\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nie_relation\nIERelation\nan ie relation that determines the input and output terms of the ie function\n\n\nie_func\nIEFunction\nthe ie function that will be used to compute the ie relation\n\n\nbounding_relation\nOptional[Relation]\na relation that contains the inputs for ie_funcs. the actual input needs to be queried from it\n\n\nReturns\nRelation\na normal relation that contains all of the resulting tuples in the spannerlog engine\n\n\n\n\n\n\nSqliteEngine.add_fact\n\n SqliteEngine.add_fact (fact:spannerlib.ast_node_types.AddFact)\n\nAdd a row into an existing sql table, based on fact’s terms and types\n\n\n\n\nType\nDetails\n\n\n\n\nfact\nAddFact\nthe fact to be added\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nmy_engine = SqliteEngine()\n\nrelation = RelationDeclaration(\"yoyo\", [DataTypes.integer, DataTypes.string])\nmy_engine.declare_relation_table(relation)\n\nfacts = [\n    AddFact(\"yoyo\", [10, \"hello\"], [DataTypes.integer, DataTypes.string]),\n    AddFact(\"yoyo\", [15, \"world\"], [DataTypes.integer, DataTypes.string])\n]\n\nfor fact in facts:\n    my_engine.add_fact(fact)\n\nmy_engine.table_to_dataframe(\"yoyo\")\n\n\n\n\n\n\n\n\n\ncol0\ncol1\n\n\n\n\n0\n10\nhello\n\n\n1\n15\nworld\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSqliteEngine.remove_fact\n\n SqliteEngine.remove_fact (fact:spannerlib.ast_node_types.RemoveFact)\n\nRemove a row from an existing sql table, based on fact’s terms and types\n\n\n\n\nType\nDetails\n\n\n\n\nfact\nRemoveFact\nthe fact to be removed\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nmy_engine = SqliteEngine()\n\nrelation = RelationDeclaration(\"yoyo\", [DataTypes.integer, DataTypes.string])\nmy_engine.declare_relation_table(relation)\n\nfacts = [\n    AddFact(\"yoyo\", [10, \"hello\"], [DataTypes.integer, DataTypes.string]),\n    AddFact(\"yoyo\", [15, \"world\"], [DataTypes.integer, DataTypes.string])\n]\n\nfor fact in facts:\n    my_engine.add_fact(fact)\nmy_engine.table_to_dataframe(\"yoyo\")\n\n\n\n\n\n\n\n\n\ncol0\ncol1\n\n\n\n\n0\n10\nhello\n\n\n1\n15\nworld\n\n\n\n\n\n\n\n\n\nmy_engine.remove_fact(RemoveFact(\"yoyo\", [15, \"world\"], [DataTypes.integer, DataTypes.string]))\nmy_engine.table_to_dataframe(\"yoyo\")\n\n\n\n\n\n\n\n\n\ncol0\ncol1\n\n\n\n\n0\n10\nhello\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSqliteEngine.query\n\n SqliteEngine.query (query:spannerlib.ast_node_types.Query,\n                     allow_duplicates:bool=False)\n\n*Outputs a preformatted query result, e.g. [(“a”,5),(“b”,6)]. notice that query isn’t a string; it’s a Query object which inherits from Relation. for example, parsing the string ?excellent(\"bill\",\"ted\") yields the following Query:\nrelation_name = excellent\nterm_list = [\"bill\", \"ted\"]\ntype_list = [DataType.string, DataType.string]\n```*\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| query | Query |  | the query to be performed |\n| allow_duplicates | bool | False | if True, query result may contain duplicate values |\n| **Returns** | **List[Tuple]** |  | **a query results which is True, False, or a list of tuples** |\n\n\n::: {.callout-note collapse=\"true\"}\n\n##### Example\n\n::: {#cell-39 .cell}\n``` {.python .cell-code}\nrelation = RelationDeclaration(\"people\", [DataTypes.integer, DataTypes.string])\nmy_engine.declare_relation_table(relation)\n\nfacts = [\n    AddFact(\"people\", [1, \"Alice\"], [DataTypes.integer, DataTypes.string]),\n    AddFact(\"people\", [2, \"Bob\"], [DataTypes.integer, DataTypes.string])\n]\n\nfor fact in facts:\n    my_engine.add_fact(fact)\n\nquery_result = my_engine.query(Query(\"people\", [\"ID\", \"Name\"],\n                                        [DataTypes.free_var_name, DataTypes.free_var_name]))\n\nprint(query_result)\n\n[(1, 'Alice'), (2, 'Bob')]\n\n:::\n:::\n\n\n\nSqliteEngine.declare_relation_table\n\n SqliteEngine.declare_relation_table\n                                      (relation_decl:spannerlib.ast_node_t\n                                      ypes.RelationDeclaration)\n\nDeclares a relation as an SQL table, whose types are named t0, t1, … if the relation is already declared, do nothing.\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_decl\nRelationDeclaration\nthe declaration info\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nmy_engine = SqliteEngine()\n\nrelation = RelationDeclaration(\"people\", [DataTypes.integer, DataTypes.string])\nmy_engine.declare_relation_table(relation)\n\nmy_engine.is_table_exists(\"people\")\n\nTrue\n\n\n\n\n\n\nsource\n\n\noperator_select\n\n operator_select (src_relation:spannerlib.ast_node_types.Relation, constan\n                  t_variables_info:Set[Tuple[int,Any,spannerlib.primitive_\n                  types.DataTypes]], *args:Any)\n\nPerforms sql WHERE, whose constraints are based on select_info\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsrc_relation\nRelation\nthe relation from which we select tuples\n\n\nconstant_variables_info\nSet[Tuple[int, Any, DataTypes]]\na set of tuples. each tuple contains the index of the column, the value to select (a constant variable), and the type of the column\n\n\nargs\nAny\n\n\n\nReturns\nRelation\na filtered relation\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nmy_engine = SqliteEngine()\n\nrelation = RelationDeclaration(\"wordy\", [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string])\n\nmy_engine.declare_relation_table(relation)\nfacts = [\n    AddFact(\"wordy\", [\"apple\", \"banana\", \"cherry\", \"date\"], [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string]),\n    AddFact(\"wordy\", [\"pear\", \"kiwi\", \"lemon\", \"mango\"], [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string]),\n    AddFact(\"wordy\", [\"pear\", \"quince\", \"raspberry\", \"pear\"], [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string]),\n]\n\nfor fact in facts:\n    my_engine.add_fact(fact)\n\nrelation_res = my_engine.operator_select(Relation(\"wordy\", [\"pear\",\"Y\",\"Z\",\"W\"], [DataTypes.string, DataTypes.free_var_name, DataTypes.free_var_name, DataTypes.free_var_name]),[(0,'pear',DataTypes.string)])\nprint(my_engine.table_to_dataframe(relation_res.relation_name))\n\n   col0    col1       col2   col3\n0  pear    kiwi      lemon  mango\n1  pear  quince  raspberry   pear\n\n\n\n\n\n\n\n\nSqliteEngine.operator_join\n\n SqliteEngine.operator_join\n                             (relations:List[spannerlib.ast_node_types.Rel\n                             ation], *args:Any)\n\nnote: SQL’s inner_join without IN is actually cross-join (product), so this covers product as well.\n\n\n\n\nType\nDetails\n\n\n\n\nrelations\nList[Relation]\na list of normal relation\n\n\nargs\nAny\n\n\n\nReturns\nRelation\na new relation as described above\n\n\n\n\nsource\n\n\noperator_project\n\n operator_project (src_relation:spannerlib.ast_node_types.Relation,\n                   project_vars:List[str], *args:Any)\n\nPerforms SQL select.\n\n\n\n\nType\nDetails\n\n\n\n\nsrc_relation\nRelation\nthe relation on which we project\n\n\nproject_vars\nList[str]\na list of variables on which we project\n\n\nargs\nAny\n\n\n\nReturns\nRelation\nthe projected relation\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nmy_engine = SqliteEngine()\n\nrelation = RelationDeclaration(\"wordy\", [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string])\n\nmy_engine.declare_relation_table(relation)\nfacts = [\n    AddFact(\"wordy\", [\"apple\", \"banana\", \"cherry\", \"date\"], [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string]),\n    AddFact(\"wordy\", [\"pear\", \"kiwi\", \"lemon\", \"mango\"], [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string]),\n    AddFact(\"wordy\", [\"pear\", \"quince\", \"raspberry\", \"pear\"], [DataTypes.string, DataTypes.string, DataTypes.string, DataTypes.string]),\n]\n\nfor fact in facts:\n    my_engine.add_fact(fact)\n\nproject_relation = my_engine.operator_select(Relation(\"wordy\", [\"pear\",\"Y\",\"Z\",\"W\"], [DataTypes.string, DataTypes.free_var_name, DataTypes.free_var_name, DataTypes.free_var_name]),[(0,'pear',DataTypes.string)])\ntable_name = my_engine.operator_project(project_relation, ['Y','Z','W']).relation_name\nmy_engine.table_to_dataframe(table_name)\n\n\n\n\n\n\n\n\n\ncol0\ncol1\ncol2\n\n\n\n\n0\nkiwi\nlemon\nmango\n\n\n1\nquince\nraspberry\npear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSqliteEngine.operator_union\n\n SqliteEngine.operator_union\n                              (relations:List[spannerlib.ast_node_types.Re\n                              lation], *args:Any)\n\n*a rule can be defined multiple times as long as it shares the same rule head. To illustrate this, let’s consider the following example:\nSuppose we have the rules:\nA(X,Y) &lt;- B(X,Y)\nA(X,Y) &lt;- C(X,Y)\nIf we execute the query ?A(X,Y), the engine will produce results that include tuples from both B(X,Y) and C(X,Y) relations.\nHere’s how the engine handles this scenario:  1. Rule Matching: The engine identifies all the rules that have the same rule head (A(X,Y)) as the queried relation.  2. Rule Execution: For each matched rule, the engine executes all the steps described earlier, such as assigning temporary names, performing joins, and creating intermediate relations, selecting and filtering.  3. Intermediate Relations: At this stage, we have intermediate relations generated for each rule, containing the relevant tuples based on the rule’s body.  4. Operator Union: To consolidate the results from all the rules into a single unified relation, the operator union is applied. This operation combines the tuples from all the intermediate relations. \nThe operator_union ensures that the final unified relation contains all the tuples from the individual intermediate relations, effectively merging the results obtained from different rule instances with the same rule head. In summary, the engine processes each rule with the same rule head as the queried relation, performs the necessary steps for each rule individually, and then applies the operator union to merge the intermediate relations into a single unified relation, ensuring all relevant tuples are included in the final result. @note: we assume that all the relations have same free_vars in the same order.*\n\n\n\n\nType\nDetails\n\n\n\n\nrelations\nList[Relation]\na list of relations to unite\n\n\nargs\nAny\n\n\n\nReturns\nRelation\nthe united relation\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nmy_engine = SqliteEngine()\n\nrelation1 = RelationDeclaration(\"relation1\", [DataTypes.integer, DataTypes.integer])\nmy_engine.declare_relation_table(relation1)\n\nfacts1 = [\n    AddFact(\"relation1\", [\"two\", 2], [DataTypes.string, DataTypes.integer]),\n    AddFact(\"relation1\", [\"four\", 4], [DataTypes.string, DataTypes.integer]),\n    AddFact(\"relation1\", [\"six\", 6], [DataTypes.string, DataTypes.integer]),\n]\nfor fact in facts1:\n    my_engine.add_fact(fact)\n\nrelation2 = RelationDeclaration(\"relation2\", [DataTypes.string, DataTypes.string])\nmy_engine.declare_relation_table(relation2)\nfacts2 = [\n    AddFact(\"relation2\", [\"one\", 1], [DataTypes.string, DataTypes.integer]),\n]\nfor fact in facts2:\n    my_engine.add_fact(fact)\nrel1 = Relation(\"relation1\", [\"X\", \"Y\"], [DataTypes.free_var_name, DataTypes.free_var_name])\nrel2 = Relation(\"relation2\", [\"Z\", \"W\"], [DataTypes.free_var_name, DataTypes.free_var_name])\ntable_name = my_engine.operator_union([rel1,rel2]).relation_name\nmy_engine.table_to_dataframe(table_name)\n\n\n\n\n\n\n\n\n\ncol0\ncol1\n\n\n\n\n0\nfour\n4\n\n\n1\none\n1\n\n\n2\nsix\n6\n\n\n3\ntwo\n2",
    "crumbs": [
      "src",
      "Engine",
      "Engine"
    ]
  },
  {
    "objectID": "execution.html",
    "href": "execution.html",
    "title": "Execution",
    "section": "",
    "text": "source\n\nnaive_execution\n\n naive_execution (parse_graph:spannerlib.graphs.GraphBase,\n                  term_graph:spannerlib.graphs.TermGraphBase,\n                  symbol_table:spannerlib.symbol_table.SymbolTableBase, sp\n                  annerlog_engine:spannerlib.engine.spannerlogEngineBase)\n\n*Executes a parse graph this execution is generic, meaning it does not require any specific kind of term graph, symbol table or spannerlog engine in order to work.  This execution performs no special optimization and merely serves as an interface between the term graph and the spannerlog engine.\nThe main idea behind this class is that it uses the term_graph to understand how relations are related to one another, and thanks to that information, it is able to execute the commands in the parse_graph. for example, let’s say the parse graph looks like this:\n(root) -&gt; (query relation a)\nand the term graph looks like this:\n(a)  --&gt; union --&gt; (b)\n               --&gt; (c)\nthe execution class will perform a union over b and c, and put it in a new relation, let’s say union_b_c. then it will copy union_b_c into a, and finally it will query a and return the result.\nmore precisely, the execution traverses the parse tree, when it reaches a query node it compute the relevant relation using the term graph (i.e. if the query is ?A(X) it will compute the relation A). read the documentation of compute_rule function to understand how the computation is done.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nparse_graph\nGraphBase\na parse graph to execute\n\n\nterm_graph\nTermGraphBase\na term graph\n\n\nsymbol_table\nSymbolTableBase\na symbol table\n\n\nspannerlog_engine\nspannerlogEngineBase\na spannerlog engine that will be used to execute the term graph\n\n\nReturns\nOptional[Tuple[Query, List]]",
    "crumbs": [
      "src",
      "Engine",
      "Execution"
    ]
  },
  {
    "objectID": "passes_utils.html",
    "href": "passes_utils.html",
    "title": "Passes Utils",
    "section": "",
    "text": "source\n\nParseNodeType\n\n ParseNodeType (value, names=None, module=None, qualname=None, type=None,\n                start=1)\n\nwill be used as parse graph node types.\n\nsource\n\n\nassert_expected_node_structure_aux\n\n assert_expected_node_structure_aux (lark_node:Any)\n\nChecks whether a lark node has a structure that the lark passes expect.\n\n\n\n\nType\nDetails\n\n\n\n\nlark_node\nAny\nthe lark node to be checked\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nassert_expected_node_structure\n\n assert_expected_node_structure (func:Callable)\n\n*Use this decorator to check whether a method’s input lark node has a structure that is expected by the lark passes the lark node and its children are checked recursively\nsome lark nodes may have multiple structures (e.g. Assignment). in this case this check will succeed if the lark node has one of those structures.*\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\nCallable\nA function to run the decorator on\n\n\nReturns\nCallable\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nnote that this decorator should only be used on methods that expect lark nodes that weren’t converted to structured nodes.\n\n\n\nsource\n\n\nunravel_lark_node\n\n unravel_lark_node (func:Callable)\n\nEven after converting a lark tree to use structured nodes, the methods in lark passes will still receive a lark node as an input, and the child of said lark node will be the actual structured node that the method will work with.\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\nCallable\nA function to run the decorator on\n\n\nReturns\nCallable\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nuse this decorator to replace a method’s lark node input with its child structured node.\n\n\n\nsource\n\n\nget_new_rule_nodes\n\n get_new_rule_nodes (parse_graph:spannerlib.graphs.GraphBase)\n\nFinds all rules that weren’t added to the term graph yet.",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Passes Utils"
    ]
  },
  {
    "objectID": "extended_version.html",
    "href": "extended_version.html",
    "title": "Advanced IE functions",
    "section": "",
    "text": "spannerlog has been enhanced with additional advanced IE functions. To utilize these functions, specific installations are required prior to usage. \nRust: To download and utilize the Rust-based ie functions, execute the following code:\n\nfrom spannerlib.ie_func.rust_spanner_regex import download_and_install_rust_regex\ndownload_and_install_rust_regex()\n\n\nWrapping shell-based functions\nspannerlog’s rgx_string ie function is a good example of running an external shell as part of spannerlog code,  rgx_string is a rust-based ie function, we can use it only after we installed the rust package.  This time we won’t remove the built-in function - we’ll just show the implementation:\ndef rgx(text, regex_pattern, out_type: str):\n    \"\"\"\n    An IE function which runs regex using rust's `enum-spanner-rs` and yields tuples of strings/spans (not both).\n\n    @param text: the string on which regex is run.\n    @param regex_pattern: the pattern to run.\n    @param out_type: string/span - decides which one will be returned.\n    @return: a tuple of strings/spans.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        rgx_temp_file_name = os.path.join(temp_dir, TEMP_FILE_NAME)\n        with open(rgx_temp_file_name, \"w+\") as f:\n            f.write(text)\n\n        if out_type == \"string\":\n            rust_regex_args = rf\"{REGEX_EXE_PATH} {regex_pattern} {rgx_temp_file_name}\"\n            format_function = _format_spanner_string_output\n        elif out_type == \"span\":\n            rust_regex_args = rf\"{REGEX_EXE_PATH} {regex_pattern} {rgx_temp_file_name} --bytes-offset\"\n            format_function = _format_spanner_span_output\n        else:\n            assert False, \"illegal out_type\"\n\n        regex_output = format_function(run_cli_command(rust_regex_args, stderr=True))\n\n        for out in regex_output:\n            yield out\n\ndef rgx_string(text, regex_pattern):\n    \"\"\"\n    @param text: The input text for the regex operation.\n    @param regex_pattern: the pattern of the regex operation.\n    @return: tuples of strings that represents the results.\n    \"\"\"\n    return rgx(text, regex_pattern, \"string\")\n\nRGX_STRING = dict(ie_function=rgx_string,\n                  ie_function_name='rgx_string',\n                  in_rel=RUST_RGX_IN_TYPES,\n                  out_rel=rgx_string_out_type)\n\n# another version of these functions exists (rgx_from_file), it can be seen in the source code\nrun_cli_command is an STDLIB function used in spannerlog, which basically runs a command using python’s Popen.\nin order to denote regex groups, use (?P&lt;name&gt;pattern). the output is in alphabetical order. Let’s run the ie function:\n\nimport spannerlib\n\n\n%%spannerlog\ntext = \"zcacc\"\npattern = \"(?P&lt;group_not_c&gt;[^c]+)(?P&lt;group_c&gt;[c]+)\"\nstring_rel(X,Y) &lt;- rgx_string(text, pattern) -&gt; (X,Y)\n?string_rel(X,Y)\n\nprinting results for query 'string_rel(X, Y)':\n  X  |  Y\n-----+-----\n  a  | cc\n  a  |  c\n  z  |  c\n\n\n\nSimilarly, to use nlp-based ie functions you need to first install nlp:\n\nfrom spannerlib.ie_func.nlp import download_and_install_nlp\ndownload_and_install_nlp()\n\n\n%%spannerlog\nsentence = \"Hello world. Hello world again.\"\ntokens(X, Y) &lt;- Tokenize(sentence) -&gt; (X, Y)\n?tokens(Token, Span)\n\nprinting results for query 'tokens(Token, Span)':\n  Token  |   Span\n---------+----------\n  Hello  |  [0, 5)\n  world  | [6, 11)\n    .    | [11, 12)\n  Hello  | [13, 18)\n  world  | [19, 24)\n  again  | [25, 30)\n    .    | [30, 31)",
    "crumbs": [
      "Tutorials",
      "Advanced IE functions"
    ]
  },
  {
    "objectID": "ie_func/python_regex.html",
    "href": "ie_func/python_regex.html",
    "title": "Python Regex",
    "section": "",
    "text": "source\n\npy_rgx_string\n\n py_rgx_string (text:str, regex_pattern:str)\n\n*An IE function which runs regex using python’s re and yields tuples of strings.\n@param text: The input text for the regex operation. @param regex_pattern: the pattern of the regex operation. @return: tuples of strings that represents the results.*\n\nsource\n\n\npy_rgx_string_out_types\n\n py_rgx_string_out_types (output_arity:int)\n\n\nsource\n\n\npy_rgx\n\n py_rgx (text:str, regex_pattern:str)\n\n*An IE function which runs regex using python’s re and yields tuples of spans.\n@param text: The input text for the regex operation. @param regex_pattern: the pattern of the regex operation. @return: tuples of spans that represents the results.*\n\nsource\n\n\npy_rgx_out_type\n\n py_rgx_out_type (output_arity:int)"
  },
  {
    "objectID": "ie_func/json_path.html",
    "href": "ie_func/json_path.html",
    "title": "Json Path",
    "section": "",
    "text": "source\n\nparse_match\n\n parse_match (match:Any)\n\n@param match: a match result of json path query. @return: a string that represents the match in string format.\n\nsource\n\n\njson_path\n\n json_path (json_document:str, path_expression:str)\n\n@param json_document: The document on which we will run the path expression. @param path_expression: The query to execute. @return: json documents.\n\nsource\n\n\njson_path_full\n\n json_path_full (json_document:str, path_expression:str)\n\n@param json_document: The document on which we will run the path expression. @param path_expression: The query to execute. @return: json documents with the full results paths."
  },
  {
    "objectID": "sample_question.html",
    "href": "sample_question.html",
    "title": "Spannerlib",
    "section": "",
    "text": "After familiarizing yourself with the syntax of spannerlog,  let’s explore another example that highlights the power of this declarative language.  given a story.txt file which contains in between its lines parent-child information, the information is provided as follows:  1- X is the parent/father of Y  2- Y is the child/daughter of X\nThe story also has some last names. The task is to identify the last name of all individuals. It’s important to note that you don’t have access to individual last names, but you can deduce them by constructing family trees based on the information provided in the story.\nHere is the first line of the story as an example:\nWhat you should deduce from this line is that Owen is the parent of Aria as it matches the first template of provided information (Y is the daughter of X),  and since Aria’s name was given along with her lastname,  you should also conclude that Owen’s last name is also Walker.\nTake a couple of minutes to think a solution for this problem.\nNow that you understand the complexity of deducing last names based on family trees and extracting the parent-child relationships using Python,  let us show you how easily we can do that by incorporating spannerlog into the equation.\nimport spannerlib\n%%spannerlog\nstory = read(\"sample_data/story.txt\")\nparent(X,Y) &lt;- py_rgx_string(story, \"(\\w+)\\s+is\\s+the\\s+daughter\\s+of\\s+(\\w+)\") -&gt; (Y,X)\nparent(X,Y) &lt;- py_rgx_string(story, \"(\\w+)\\s+is\\s+the\\s+child\\s+of\\s+(\\w+)\") -&gt; (Y,X)\nparent(X,Y) &lt;- py_rgx_string(story, \"(\\w+)\\s+is\\s+the\\s+parent\\s+of\\s+(\\w+)\") -&gt; (X,Y)\nparent(X,Y) &lt;- py_rgx_string(story, \"(\\w+)\\s+is\\s+the\\s+father\\s+of\\s+(\\w+)\") -&gt; (X,Y)\nlastname(X,Y) &lt;- py_rgx_string(story, \"([A-Z][a-z]+)\\s([A-Z][a-z]+)\") -&gt; (X,Y)\n?parent(X,Y)\n?lastname(X,Y)\n\nprinting results for query 'parent(X, Y)':\n    X     |    Y\n----------+----------\n   Aria   |   Mila\n Benjamin |  Daniel\n Benjamin |   Lila\n   Owen   |   Aria\n   Owen   |  Caleb\n   Owen   |  Eliana\n   Yaer   | Benjamin\n\nprinting results for query 'lastname(X, Y)':\n    X     |   Y\n----------+--------\n   Aria   | Walker\n Benjamin | Smith"
  },
  {
    "objectID": "sample_question.html#deducing-last-names-based-on-family-trees-with-spannerlog",
    "href": "sample_question.html#deducing-last-names-based-on-family-trees-with-spannerlog",
    "title": "Spannerlib",
    "section": "Deducing Last Names Based on Family Trees with spannerlog",
    "text": "Deducing Last Names Based on Family Trees with spannerlog\nThe following spannerlog code provides a simple yet powerful way to deduce the last name of each person based on the family tree information extracted to the parent relation.\n\n%%spannerlog\nFamily(X,Y) &lt;- lastname(Y,X)\nFamily(X,Y) &lt;- Family(X,Z), parent(Y,Z)\nFamily(X,Y) &lt;- Family(X,Z), parent(Z,Y)\n?Family(X,Y)\n\nprinting results for query 'Family(X, Y)':\n   X    |    Y\n--------+----------\n Smith  | Benjamin\n Smith  |  Daniel\n Smith  |   Lila\n Smith  |   Yaer\n Walker |   Aria\n Walker |  Caleb\n Walker |  Eliana\n Walker |   Mila\n Walker |   Owen"
  },
  {
    "objectID": "ie_function.html",
    "href": "ie_function.html",
    "title": "IE Function",
    "section": "",
    "text": "source\n\nIEFunction\n\n IEFunction (ie_function_def:Callable, in_types:Sequence[DataTypes],\n             out_types:Union[List[DataTypes],Callable[[int],Sequence[DataT\n             ypes]]])\n\nA class that contains all the functions that provide data needed for using a single information extraction function\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nie_function_def\nCallable\nthe user defined ie function implementation\n\n\nin_types\nSequence[DataTypes]\niterable of the input types to the function\n\n\nout_types\nUnion[List[DataTypes], Callable[[int], Sequence[DataTypes]]]\neither a function (int-&gt;iterable) or an iterable\n\n\n\n\nsource\n\n\nIEFunction.ie_function\n\n IEFunction.ie_function (*args:Any)\n\n*The actual information extraction function that will be used the function must return a list of lists/tuples that represents the results, another option is to yield the tuples.\ncurrently the values inside the returned tuples can belong to three datatypes: string, integer and span string should be returned as a str instance an integer should be returned as an int instance a span could be returned either as a tuple of length 2, or as a datatypes.Span instance*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nargs\nAny\n\n\n\nReturns\nIterable[Iterable[Union[str, int, Tuple[int, int]]]]\nTuple[int, int] represents a Span\n\n\n\n\nsource\n\n\nIEFunction.get_input_types\n\n IEFunction.get_input_types ()\n\n@return: an iterable of the input types to the function. This function must be defined as it is used for type checking in semantic passes and execution.\n\nsource\n\n\nIEFunction.get_output_types\n\n IEFunction.get_output_types (output_arity:int)\n\nif the ie function cannot return an output of length output_arity, should return None. This function must be defined as it is used for type checking in semantic passes and execution.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\noutput_arity\nint\nexpected output_arity\n\n\nReturns\nSequence[DataTypes]\ngiven an expected output arity returns an iterable of the output types to the function\n\n\n\n\nsource\n\n\nIEFunction.get_meta_data\n\n IEFunction.get_meta_data ()\n\n@return: metadata about the ie function.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ndef sample_ie_func1(x: int, y: str):\n    yield (x, y, x + 1)\n\nie_func = IEFunction(sample_ie_func1, [DataTypes.integer, DataTypes.string], [DataTypes.integer, DataTypes.string, DataTypes.integer])\nprint(ie_func.get_meta_data())\n\nInput types: [&lt;DataTypes.integer: 2&gt;, &lt;DataTypes.string: 0&gt;].\nOutput types: [&lt;DataTypes.integer: 2&gt;, &lt;DataTypes.string: 0&gt;, &lt;DataTypes.integer: 2&gt;]\n\n\n\nresult = list(ie_func.ie_function(1, \"example\"))\nprint(\"result =\",result)\n\nresult = [(1, 'example', 2)]",
    "crumbs": [
      "src",
      "Graphs",
      "IE Function"
    ]
  },
  {
    "objectID": "symbol_table.html",
    "href": "symbol_table.html",
    "title": "Symbol Table",
    "section": "",
    "text": "This module contains the implementations of symbol tables\n\nsource\n\nSymbolTableBase\n\n SymbolTableBase ()\n\nAn abstraction for a symbol table.  the symbol table keeps track of:  1. the variables that were defined in the program, their types and their values  2. the relations that were defined in the program and their schemas  3. the information extraction functions that were registered in the program and their data\n\nsource\n\n\nSymbolTableBase.set_var_value_and_type\n\n SymbolTableBase.set_var_value_and_type (var_name:str,\n                                         var_value:Union[str,spannerlib.pr\n                                         imitive_types.Span,int], var_type\n                                         :spannerlib.primitive_types.DataT\n                                         ypes)\n\nSets the type and value of a variable in the symbol table.\n\n\n\n\nType\nDetails\n\n\n\n\nvar_name\nstr\nthe name of the variable\n\n\nvar_value\nDataTypeMapping.term\nthe value of the variable\n\n\nvar_type\nDataTypes\nthe type of the variable\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nSymbolTableBase.get_variable_type\n\n SymbolTableBase.get_variable_type\n                                    (var_name:Union[str,spannerlib.primiti\n                                    ve_types.Span,int])\n\n\n\n\n\nType\nDetails\n\n\n\n\nvar_name\nDataTypeMapping.term\na variable name\n\n\nReturns\nDataTypes\nthe variable’s type\n\n\n\n\nsource\n\n\nSymbolTableBase.get_variable_value\n\n SymbolTableBase.get_variable_value\n                                     (var_name:Union[str,spannerlib.primit\n                                     ive_types.Span,int])\n\n\n\n\n\nType\nDetails\n\n\n\n\nvar_name\nDataTypeMapping.term\na variable name\n\n\nReturns\nDataTypeMapping.term\nthe variable’s value\n\n\n\n\nsource\n\n\nSymbolTableBase.get_all_variables\n\n SymbolTableBase.get_all_variables ()\n\n@return: an iterable that contains tuples of the format (variable name, variable type, variable value) for each variable in the symbol table.\n\nsource\n\n\nSymbolTableBase.contains_variable\n\n SymbolTableBase.contains_variable (var_name:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nvar_name\nstr\na variable name\n\n\nReturns\nbool\ntrue if the variable is in the symbol table, else false\n\n\n\n\nsource\n\n\nSymbolTableBase.add_relation_schema\n\n SymbolTableBase.add_relation_schema (relation_name:str,\n                                      schema:Sequence[spannerlib.primitive\n                                      _types.DataTypes], is_rule:bool)\n\nAdds a new relation schema to the symbol table.  Note: Trying to add two schemas for the same relation will result in an exception as relation redefinitions are not allowed.\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe relation’s name.\n\n\nschema\nSequence[DataTypes]\nthe relation’s schema.\n\n\nis_rule\nbool\ntrue if rule false if relation.\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTrying to add two schemas for the same relation will result in an exception as relation redefinitions are not allowed.\n\n\n\nsource\n\n\nSymbolTableBase.get_relation_schema\n\n SymbolTableBase.get_relation_schema (relation_name:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\na relation name\n\n\nReturns\nSequence[DataTypes]\nthe relation’s schema\n\n\n\n\nsource\n\n\nSymbolTableBase.get_all_relations\n\n SymbolTableBase.get_all_relations ()\n\n@return: an iterable that contains tuples of the format (relation name, relation schema) for each relation in the symbol table.\n\nsource\n\n\nSymbolTableBase.contains_relation\n\n SymbolTableBase.contains_relation (relation_name:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\na relation name\n\n\nReturns\nbool\ntrue if the relation exists in the symbol table, else false\n\n\n\n\nsource\n\n\nSymbolTableBase.register_ie_function\n\n SymbolTableBase.register_ie_function (ie_function:Callable,\n                                       ie_function_name:str, in_rel:Sequen\n                                       ce[spannerlib.primitive_types.DataT\n                                       ypes], out_rel:Union[List[spannerli\n                                       b.primitive_types.DataTypes],Callab\n                                       le[[int],Sequence[spannerlib.primit\n                                       ive_types.DataTypes]]])\n\nAdds a new ie function to the symbol table. @see params in IEFunction’s init.\n\nsource\n\n\nSymbolTableBase.contains_ie_function\n\n SymbolTableBase.contains_ie_function (ie_func_name:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nie_func_name\nstr\na name of an information extraction function\n\n\nReturns\nbool\ntrue if the ie function exists in the symbol table, else false\n\n\n\n\nsource\n\n\nSymbolTableBase.get_ie_func_data\n\n SymbolTableBase.get_ie_func_data (ie_func_name:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nie_func_name\nstr\na name of an information extraction function\n\n\nReturns\nIEFunction\nthe ie function’s data (see ie_function_base.IEFunctionData for more information on ie function data instances)\n\n\n\n\nsource\n\n\nSymbolTableBase.get_all_registered_ie_funcs\n\n SymbolTableBase.get_all_registered_ie_funcs ()\n\n@return: an iterable containing the names of all of the ie functions that are registered in the symbol table.\n\nsource\n\n\nSymbolTableBase.register_predefined_ie_functions\n\n SymbolTableBase.register_predefined_ie_functions\n                                                   (ie_funcs:Iterable[Dict\n                                                   ])\n\nAdds to symbol table all the predefined ie functions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nie_funcs\nIterable[Dict]\niterable of the predefined ie functions in dict format\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nSymbolTableBase.remove_ie_function\n\n SymbolTableBase.remove_ie_function (name:str)\n\nRemoves a function from the symbol table.\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nthe name of the ie function to remove\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nSymbolTableBase.remove_all_ie_functions\n\n SymbolTableBase.remove_all_ie_functions ()\n\nRemoves all the ie functions from the symbol table.\n\nsource\n\n\nSymbolTableBase.print_registered_ie_functions\n\n SymbolTableBase.print_registered_ie_functions ()\n\nPrints all the registered ie functions.\n\nsource\n\n\nSymbolTableBase.remove_rule_relation\n\n SymbolTableBase.remove_rule_relation (relation_name:str)\n\nRemoves a rule relation from the symbol table.\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nthe name of the relation to remove\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nSymbolTableBase.remove_all_rule_relations\n\n SymbolTableBase.remove_all_rule_relations ()\n\n*Removes all the rule relations.\n@return: iterable of all the relations name it removed.*\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ntable = SymbolTable()\ntable.set_var_value_and_type(\"var1\", 10, int)\nprint(\"var1 type is\",table.get_variable_type(\"var1\"),\"and its value is\",table.get_variable_value(\"var1\"))\n\nvar1 type is &lt;class 'int'&gt; and its value is 10\n\n\n\ntable.add_relation_schema(\"relation1\", [int, str], is_rule=False)\nprint(table.get_relation_schema(\"relation1\"))\n\n[&lt;class 'int'&gt;, &lt;class 'str'&gt;]\n\n\n\ndef ie_function(x):\n        return x + 1\ntable.register_ie_function(ie_function, \"func1\", [DataTypes.integer], [DataTypes.integer])\nprint((\"func1\" in table.get_all_registered_ie_funcs()))\n\nTrue",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Symbol Table"
    ]
  },
  {
    "objectID": "lark_passes.html",
    "href": "lark_passes.html",
    "title": "Lark Passes",
    "section": "",
    "text": "Pass Types\nLearning Resources",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Lark Passes"
    ]
  },
  {
    "objectID": "lark_passes.html#overview",
    "href": "lark_passes.html#overview",
    "title": "Lark Passes",
    "section": "",
    "text": "Pass Types\nLearning Resources",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Lark Passes"
    ]
  },
  {
    "objectID": "lark_passes.html#pass-types",
    "href": "lark_passes.html#pass-types",
    "title": "Lark Passes",
    "section": "Pass Types",
    "text": "Pass Types\nThe engine module incorporates three primary types of passes:\n\n1. Transformer\nTakes a Lark tree as an input and returns a completely new Lark tree. The order in which the nodes are visited can be user-defined.\n\n\n2. Visitor / Visitor_Recursive\nTakes a Lark tree as input and modifies it in-place, if at all. The visitation order is not defined, making this class suitable for cases where nodes are visited without requiring specific context. Visitor_Recursive is often the choice for efficiency, as per the documentation.\n\n\n3. Interpreter\nFunctions similarly to the Visitor class, but with a defined order for visiting nodes. By default, nodes are visited in breadth-first search (BFS) order from the root of the Lark tree.",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Lark Passes"
    ]
  },
  {
    "objectID": "lark_passes.html#learning-resources",
    "href": "lark_passes.html#learning-resources",
    "title": "Lark Passes",
    "section": "Learning Resources",
    "text": "Learning Resources\nFor those interested in diving deeper into Lark, the following resources are recommended:\n\nOfficial Documentation: Lark Documentation\nCheat Sheet: Lark Cheat Sheet\nTutorial: Lark Tutorial on JSON Parsing\n\n\n\nsource\n\nGenericPass\n\n GenericPass ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nVisitorPass\n\n VisitorPass ()\n\n*Tree visitor, non-recursive (can handle huge trees).\nVisiting a node calls its methods (provided by the user via inheritance) according to tree.data*\n\nsource\n\n\nVisitorRecursivePass\n\n VisitorRecursivePass ()\n\n*Bottom-up visitor, recursive.\nVisiting a node calls its methods (provided by the user via inheritance) according to tree.data\nSlightly faster than the non-recursive version.*\n\nsource\n\n\nInterpreterPass\n\n InterpreterPass ()\n\n*Interpreter walks the tree starting at the root.\nVisits the tree, starting with the root and finally the leaves (top-down)\nFor each tree node, it calls its methods (provided by user via inheritance) according to tree.data.\nUnlike Transformer and Visitor, the Interpreter doesn’t automatically visit its sub-branches. The user has to explicitly call visit, visit_children, or use the @visit_children_decor. This allows the user to implement branching and loops.*\n\nsource\n\n\nTransformerPass\n\n TransformerPass (visit_tokens=True)\n\n*Transformers visit each node of the tree, and run the appropriate method on it according to the node’s data.\nMethods are provided by the user via inheritance, and called according to tree.data. The returned value from each method replaces the node in the tree structure.\nTransformers work bottom-up (or depth-first), starting with the leaves and ending at the root of the tree. Transformers can be used to implement map & reduce patterns. Because nodes are reduced from leaf to root, at any point the callbacks may assume the children have already been transformed (if applicable).\nTransformer can do anything Visitor can do, but because it reconstructs the tree, it is slightly less efficient.\nAll these classes implement the transformer interface:\n\nTransformer - Recursively transforms the tree. This is the one you probably want.\nTransformer_InPlace - Non-recursive. Changes the tree in-place instead of returning new instances\nTransformer_InPlaceRecursive - Recursive. Changes the tree in-place instead of returning new instances\n\nParameters: visit_tokens (bool, optional): Should the transformer visit tokens in addition to rules. Setting this to False is slightly faster. Defaults to True. (For processing ignored tokens, use the lexer_callbacks options)\nNOTE: A transformer without methods essentially performs a non-memoized partial deepcopy.*\n\nsource\n\n\nRemoveTokens\n\n RemoveTokens (**kw:Any)\n\nA lark pass that should be used before the semantic checks.  Transforms the lark tree by removing the redundant tokens. \n\nsource\n\n\nCheckReservedRelationNames\n\n CheckReservedRelationNames (**kw:Any)\n\nA lark tree semantic check. Checks if there are relations in the program with a name that starts with RESERVED_RELATION_PREFIX if such relations exist, throw an exception as this is a reserved name for spannerlog.\n\nsource\n\n\nFixStrings\n\n FixStrings (**kw:Any)\n\nFixes the strings in the lark tree.  Removes the line overflow escapes from strings.\n\nsource\n\n\nConvertSpanNodesToSpanInstances\n\n ConvertSpanNodesToSpanInstances (**kw:Any)\n\nConverts each span node in the ast to a span instance.  This means that a span in the tree will be represented by a single value (a DataTypes.Span instance) instead of two integer nodes, making it easier to work with (as other data types are also represented by a single value).\n\nsource\n\n\nConvertStatementsToStructuredNodes\n\n ConvertStatementsToStructuredNodes (**kw:Any)\n\nconverts each statement node in the tree to a structured node, making it easier to parse in future passes.  a structured node is a class representation of a node in the abstract syntax tree. \n\n\n\n\n\n\nNote\n\n\n\nnote that after using this pass, non statement nodes will no longer appear in the tree, so passes that should work on said nodes need to be used before this pass in the passes pipeline (e.g. FixString).\n\n\n\nsource\n\n\nCheckDefinedReferencedVariables\n\n CheckDefinedReferencedVariables (symbol_table:SymbolTableBase, **kw:Any)\n\nA lark tree semantic check.  checks whether each variable reference refers to a defined variable.\n\nsource\n\n\nCheckReferencedRelationsExistenceAndArity\n\n CheckReferencedRelationsExistenceAndArity (symbol_table:SymbolTableBase,\n                                            **kw:Any)\n\nA lark tree semantic check.  Checks whether each normal relation (that is not an ie relation) reference refers to a defined relation. Also checks if the relation reference uses the correct arity.\n\nsource\n\n\nCheckReferencedIERelationsExistenceAndArity\n\n CheckReferencedIERelationsExistenceAndArity\n                                              (symbol_table:SymbolTableBas\n                                              e, **kw:Any)\n\n*A lark tree semantic check.  Checks whether each ie relation reference refers to a defined ie function.  Also checks if the correct input arity and output arity for the ie function were used.\nCurrently, an ie relation can only be found in a rule’s body, so this is the only place where this check will be performed.*\n\nsource\n\n\nCheckRuleSafety\n\n CheckRuleSafety (**kw:Any)\n\n*Performs semantic checks on rules using a Lark tree to ensure their safety.  A rule is considered “safe” when it meets certain conditions.\n\nRule Safety Conditions\n\nFree Variable in Rule Head\nBound Free Variable\n\nExamples\nSafe Relations\n\n\n\n\nRule Safety Conditions\nFor a rule to be considered safe, the following two conditions must be met:\n\n\n1. Free Variable in Rule Head\nEvery free variable that appears in the rule head must occur at least once in the body as an output term of a relation.\n\nExamples\n\nparent(X,Y) &lt;- son(X) is not a safe rule because the free variable Y only appears in the rule head.\n\nparent(X,Z) &lt;- parent(X,Y), parent(Y,Z) is a safe rule since both X and Z appear in the rule body.\n\n\n\n\n2. Bound Free Variable\nA free variable is considered “bound” if it is constrained in a manner that limits the range of values it can take.\nTo ensure that every free variable is bound, we must ensure that every relation in the rule body is a safe relation.\n\n\nSafe Relations\nA safe relation adheres to the following:\n\nIts input relation is safe, meaning all its input’s free variables are bound. Normal relations are always considered safe as they don’t have input relations.\n\nA bound variable is one that exists in the output of a safe relation.\n\n\nExamples\n\nrel2(X,Y) &lt;- rel1(X,Z), ie1&lt;X&gt;(Y) is a safe rule as the only input free variable, X, exists in the output of the safe relation rel1(X, Z).\n\nrel2(Y) &lt;- ie1&lt;Z&gt;(Y) is not safe as the input free variable Z does not exist in the output of any safe relation.\n\n—*\n\nsource\n\n\n\nTypeCheckAssignments\n\n TypeCheckAssignments (symbol_table:SymbolTableBase, **kw:Any)\n\nA lark semantic check  performs type checking for Assignments  in the current version of lark, this type checking is only required for read assignments.\n\nsource\n\n\nTypeCheckRelations\n\n TypeCheckRelations (symbol_table:SymbolTableBase, **kw:Any)\n\n*A Lark Tree Semantic Check\n\n\nAssumptions\nThis pass operates under the following assumptions. Failure to meet these may lead to incorrect results:\n\nReferences to relations and IE (Information Extraction) relations, as well as their arity, have been properly checked.\nVariable references have been verified.\nThe pass only processes a single statement as input.\n\n\n\nSemantic Checks\nThe pass performs the following specific checks:\n\n1. Typed Relation References\nIt verifies if the relation references in the rule are correctly typed.\n\n\n2. Typed IE Relations\nIt verifies if the IE relations in the rule are correctly typed.\n\n\n3. Conflicting Types in Free Variables\nChecks if free variables within rules have conflicting types. This is crucial to ensure that the rules are logically coherent.\n\n\n\nExample\nHere is an example that illustrates how a semantic check may fail on the third type of check:\nprolog new A(str) new B(int) C(X) &lt;- A(X), B(X)  # Error: X is expected to be both an int and a string.*\n\nsource\n\n\nSaveDeclaredRelationsSchemas\n\n SaveDeclaredRelationsSchemas (symbol_table:SymbolTableBase, **kw:Any)\n\nThis pass writes the relation schemas that it finds in relation declarations and rule heads to the symbol table.\nThis pass assumes that type checking was already performed on its input.*\n\n\n\n\n\n\nNote\n\n\n\nNote that a rule is a relation declaration of the rule head relation and a definition of its contents\n\n\n\nsource\n\n\nResolveVariablesReferences\n\n ResolveVariablesReferences (symbol_table:SymbolTableBase, **kw:Any)\n\nA lark execution pass,  this pass replaces variable references with their literal values.  also replaces DataTypes.var_name types with the real type of the variable.\n\nsource\n\n\nExecuteAssignments\n\n ExecuteAssignments (symbol_table:SymbolTableBase, **kw:Any)\n\nA lark execution pass,  executes assignments by saving variables’ values and types in the symbol table  should be used only after variable references are resolved, meaning the assigned values and read() arguments are guaranteed to be literals.\n\nsource\n\n\nAddStatementsToNetxParseGraph\n\n AddStatementsToNetxParseGraph (parse_graph:NetxStateGraph, **kw:Any)\n\n*A lark execution pass.  This pass adds each statement in the input parse tree to the parse graph.  This pass is made to work with execution.naive_execution as the execution function and term_graph.NetxStateGraph as the parse graph.\nEach statement in the parse graph will be a child of the parse graph’s root.\nEach statement in the parse graph will have a type attribute that contains the statement’s name in the spannerlog grammar.\nSome nodes in the parse graph will contain a value attribute that would contain a relation that describes that statement. e.g. a add_fact node would have a value which is a structured_nodes.AddFact instance (which inherits from structured_nodes.Relation) that describes the fact that will be added.\nSome statements are more complex and will be described by more than a single node, e.g. a rule node. The reason for this is that we want a single netx node to not contain more than one Relation (or IERelation) instance. This will make the parse graph a “graph of relation nodes”, allowing for flexibility for optimization in the future.*",
    "crumbs": [
      "src",
      "Grammar & Parsing",
      "Lark Passes"
    ]
  },
  {
    "objectID": "spannerlog_magic.html",
    "href": "spannerlog_magic.html",
    "title": "spannerlog Magic",
    "section": "",
    "text": "source\n\nspannerlogMagic\n\n spannerlogMagic (*args:t.Any, **kwargs:t.Any)\n\n*Base class for implementing magic functions.\nShell functions which can be reached as %function_name. All magic functions should accept a string, which they can parse for their own needs. This can make some functions easier to type, eg %cd ../ vs. %cd(\"../\")\nClasses providing magic functions need to subclass this class, and they MUST:\n\nUse the method decorators @line_magic and @cell_magic to decorate individual methods as magic functions, AND\nUse the class decorator @magics_class to ensure that the magic methods are properly registered at the instance level upon instance initialization.\n\nSee :mod:magic_functions for examples of actual implementation classes.*"
  },
  {
    "objectID": "graphs.html",
    "href": "graphs.html",
    "title": "Graphs",
    "section": "",
    "text": "This module contains the implementations of graphs (directed graphs).\nThe graphs we use when executing commands are called term_graph, parse_graph, and dependency_graph. the parse graph is an abstract syntax tree. it contains nodes which represent commands, like add_fact, rule, query, etc. In order to compute rule nodes, or compute query nodes, we use the term graph.\nThe term graph is holding the connection between all the relations and rules in the program, for more description read TermGraph’s documentation.\nThe dependency graph stores dependencies between relations in the program, it is used by the term graph to recognize mutually recursive relations. for more information read DependencyGraph’s documentation.\n\nsource\n\nEvalState\n\n EvalState (value, names=None, module=None, qualname=None, type=None,\n            start=1)\n\nwill be used to determine if a term is computed or not.\n\nsource\n\n\nTermNodeType\n\n TermNodeType (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nwill be used to represent type of term graph nodes.\n\nsource\n\n\nGraphBase\n\n GraphBase ()\n\nThis is an interface for a simple graph.\n\nsource\n\n\nGraphBase.add_node\n\n GraphBase.add_node (node_id:Union[int,str,NoneType]=None, **attr:Dict)\n\nAdds a node to the graph.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnode_id\nOptional[NodeIdType]\nNone\nthe id of the node (optional)\n\n\nattr\nDict\n\n\n\n\nReturns\nNodeIdType\n\na node id that refers to the node that was added\n\n\n\n\nsource\n\n\nGraphBase.get_root_id\n\n GraphBase.get_root_id ()\n\n@return: the node id of the root of the graph.\n\nsource\n\n\nGraphBase.remove_node\n\n GraphBase.remove_node (node_id:Union[int,str])\n\nRemoves a node from the graph.\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node that will be removed\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nGraphBase.add_edge\n\n GraphBase.add_edge (father_id:Union[int,str], son_id:Union[int,str],\n                     **attr:Dict)\n\nAdds the edge (father_id, son_id) to the graph.  the edge signifies that the father node is dependent on the son node.\n\n\n\n\nType\nDetails\n\n\n\n\nfather_id\nNodeIdType\nthe id of the father node\n\n\nson_id\nNodeIdType\nthe id of the son node\n\n\nattr\nDict\n\n\n\nReturns\nNone\nthe attributes for the edge\n\n\n\n\nsource\n\n\nGraphBase.pre_order_dfs_from\n\n GraphBase.pre_order_dfs_from (node_id:Union[int,str])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node we will start the pre_order dfs from\n\n\nReturns\nIterable[NodeIdType]\nan iterable of the node ids generated from a depth-first-search pre-ordering starting at the root of the graph\n\n\n\n\nsource\n\n\nGraphBase.pre_order_dfs\n\n GraphBase.pre_order_dfs ()\n\n@return: an iterable of the node ids generated from a depth-first-search pre-ordering starting at the root of the graph.\n\nsource\n\n\nGraphBase.post_order_dfs_from\n\n GraphBase.post_order_dfs_from (node_id:Union[int,str])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node we will run post_order dfs to\n\n\nReturns\nIterable[NodeIdType]\nan iterable of the node ids generated from a depth-first-search post-ordering starting at the root of the graph.\n\n\n\n\nsource\n\n\nGraphBase.post_order_dfs\n\n GraphBase.post_order_dfs ()\n\n@return: an iterable of the node ids generated from a depth-first-search post-ordering starting at the root of the graph.\n\nsource\n\n\nGraphBase.get_children\n\n GraphBase.get_children (node_id:Union[int,str])\n\nIn a term graph the children of a node are its dependencies this function returns the children of a node.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node that we wish to get its children\n\n\nReturns\nIterable[NodeIdType]\nan iterable of the children of the node\n\n\n\n\nsource\n\n\nGraphBase.get_child\n\n GraphBase.get_child (node_id:Union[int,str])\n\n@raise: RuntimeError if node_id has more than one child\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node that we wish to get its child\n\n\nReturns\nNodeIdType\nthe child of the node\n\n\n\n\nsource\n\n\nGraphBase.get_parents\n\n GraphBase.get_parents (node_id:Union[int,str])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node that we wish to get its predecessors\n\n\nReturns\nIterable[NodeIdType]\nthe parents (predecessors) of the node\n\n\n\n\nsource\n\n\nGraphBase.get_parent\n\n GraphBase.get_parent (node_id:Union[int,str])\n\n@raise: RuntimeError if node_id has more than one parent\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node that we wish to get its predecessor\n\n\nReturns\nNodeIdType\nthe parent of the node\n\n\n\n\nsource\n\n\nGraphBase.is_contains_node\n\n GraphBase.is_contains_node (node_id:Union[int,str])\n\nChecks if node is in the graph.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe node to look for\n\n\nReturns\nbool\nTrue if node is in the graph, False otherwise\n\n\n\n\nsource\n\n\nGraphBase.set_node_attribute\n\n GraphBase.set_node_attribute (node_id:Union[int,str], attr_name:str,\n                               attr_value:Any)\n\nSets an attribute of a node.\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node\n\n\nattr_name\nstr\nthe name of the attribute\n\n\nattr_value\nAny\nthe value that will be set for the attribute\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nGraphBase.get_node_attributes\n\n GraphBase.get_node_attributes (node_id:Union[int,str])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnode_id\nNodeIdType\nthe id of the node we wish to get its attributes\n\n\nReturns\nDict[str, Any]\na dict containing the attributes of the node\n\n\n\n\nsource\n\n\nGraphBase.get_all_nodes_with_attributes\n\n GraphBase.get_all_nodes_with_attributes\n                                          (sub_graph_root:Union[int,str,No\n                                          neType]=None, **attributes:Any)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsub_graph_root\nOptional[NodeIdType]\nNone\nthe sub-graph root. if set to None then we search in the entire graph\n\n\nattributes\nAny\n\n\n\n\nReturns\nIterable[NodeIdType]\n\nall nodes that contains all the attributes inside the sub-graph\n\n\n\n\nsource\n\n\nNetxGraph\n\n NetxGraph ()\n\nImplementation of a graph using a NetworkX graph.  The official documentation for NetworkX can be found here: NetworkX Documentation.  For a basic tutorial on NetworkX, you can visit: NetworkX Introduction.\n\nsource\n\n\nNetxStateGraph\n\n NetxStateGraph ()\n\nThis is a wrapper to NetxGraph that stores a state and type for each node in the graph. (This class will be the base class of the term graph and the parse graph while NetxGraph will be the base of dependency graph).\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ngraph = NetxStateGraph()\n\nnode0 = graph.add_node(type=\"int\")\nnode1 = graph.add_node(type=\"string\")\nnode2 = graph.add_node(type=\"float\")\n\ngraph.add_edge(node0,node1)\ngraph.add_edge(node0,node2)\n\nprint(graph._get_node_string(node0))\n\n(0) (not_computed) int\n\n\n\nprint(graph.get_children(node0))\n\n[1, 2]\n\n\n\n\n\n\nsource\n\n\nDependencyGraph\n\n DependencyGraph ()\n\n*The DependencyGraph class is designed to map and manage dependencies between rule relations in an spannerlog program. Each rule relation in the program corresponds to a node in this graph, with the node’s ID being the name of the rule relation.\n\n\nExample\nConsider the following spannerlog program:\nnew A(int)\nB(X) &lt;- A(X)\nC(X) &lt;- A(X)\nB(X) &lt;- C(X)  # B is dependent on C\nC(X) &lt;- B(X)  # C is dependent on B\nD(X) &lt;- C(X)  # D is dependent on C\nIn this case the dependency graph will be:  Nodes = {A, B, C, D} (all the rule relations)  Edges = {(B, C), (C, B), (D, C)}*\n\n\n\n\n\n\nMain utility\n\n\n\nIn order to find mutually recursive rule relations we just need to compute the strongly connected components of the graph (all the relations in a certain component are mutually recursive).\n\n\n\nsource\n\n\nTermGraphBase\n\n TermGraphBase ()\n\nA wrapper to NetxStateGraph that adds utility functions which are independent of the structure of the term graph.\n\nsource\n\n\nTermGraphBase.add_rule_to_term_graph\n\n TermGraphBase.add_rule_to_term_graph\n                                       (rule:spannerlib.ast_node_types.Rul\n                                       e)\n\nAdds the rule to the term graph. This function is responsible for the structure of the term graph.\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nRule\nthe rule to add.\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nTermGraphBase.remove_rule\n\n TermGraphBase.remove_rule (rule:str)\n\nRemoves rule from the term graph. This function depends on the structure of the term graph.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nstr\nthe rule to remove\n\n\nReturns\nbool\nWhether the deletion of the rule succeeded\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe rule is in string format and must be exactly equal the the original rule (i.e., if you want to delete the rule A(X) &lt;-B(X), you must pass A(X) &lt;- B(X) and not A(Y) &lt;- B(Y))\n\n\n\nsource\n\n\nTermGraphBase.add_rule_node\n\n TermGraphBase.add_rule_node (rule:spannerlib.ast_node_types.Rule,\n                              nodes:Set[str])\n\nAdds rule to term graph dict.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nRule\nThe rule to add\n\n\nnodes\nSet[str]\n(and thus should be removed if the rule is removed)\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nTermGraphBase._get_all_rules_with_head\n\n TermGraphBase._get_all_rules_with_head (relation_name:str)\n\nFind all the rule with rule head. @raise ValueError: if relation name doesn’t exist in the graph.\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nname of the relation\n\n\nReturns\nList[str]\na list of rules with rule head\n\n\n\n\nsource\n\n\nTermGraphBase.remove_rules_with_head\n\n TermGraphBase.remove_rules_with_head (rule_head_name:str)\n\nRemoves all rules with given rule head from the term graph.  @raise ValueError: if the rule_head_name is being used by another rule\n\n\n\n\nType\nDetails\n\n\n\n\nrule_head_name\nstr\na relation name\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nTermGraphBase.print_all_rules\n\n TermGraphBase.print_all_rules (head:Union[str,NoneType])\n\nPrints all the registered rules.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nhead\nOptional[str]\nIf you want to print all the rules with a specific head\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nTermGraphBase.get_mutually_recursive_relations\n\n TermGraphBase.get_mutually_recursive_relations (relation_name:str)\n\nSee documentation of get_mutually_recursive_relations in DependencyGraph\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation_name\nstr\nrelation name that you want to get its mutually recursive relations\n\n\nReturns\nSet[str]\nA set of mutually recursive relations with the given relation name\n\n\n\n\nsource\n\n\nTermGraph\n\n TermGraph ()\n\n*This class is designed to transform each rule node in an spannerlog program into an execution graph. These execution graphs are then added to a term graph. \nThe term graph serves to store relationships between the following entities:  1. The Rule Head: The leading variable or fact in a rule.  2. The Body Rule Relations: The components that make up the conditions of the rule.  3. The Body Base Relations and IE Relations: Relations that are intrinsic or part of external functions.  4. Computation Paths of The Rule Head: All possible paths that lead to the evaluation of the rule head.\nHere is a sample spannerlog program for illustration:\nnew A(int, int)\nnew B(int, int)\nC(X, Y) &lt;- A(X, Y)\nD(X, Y) &lt;- C(X, Y)\nD(X, Y) &lt;- A(X, 1), B(X, Y), ID(X) -&gt; (Y)  # ID is some ie function\nWe will explain the meaning of the four core entities in the term graph with respect to the rules of D\n\nRule Head: The rule head for these rules is D(X, Y).\nBody Rule Relations: The body relations for the first rule is C(X, Y). The second rule has no body rule relations.\nBase Relations: The base relations for the second rule are A(X, 1) and B(X, Y). The first rule has no base relations.\nComputation Paths: The computation paths consist of paths from both the first and second rule.\n\nTerm Graph Structure\nNodes:\n\nrule_rel Node: Each rule relation has a corresponding node in the term graph. Every rule_rel node is connected to a global root.\nunion_node: Each rule_rel node connects to a union_node.\nproject_node: Starts each computation path, projecting the columns of the relation it receives. It is connected to the union_node.\njoin_node: This node is under the project_node and joins all the body relations of the rule. Special cases where this node isn’t used include:\n\nThere’s only one relation in the rule’s body.\nNone of the body relations have free variables.\n\ncalc_node: Connects each IE relation in the rule’s body to the join_node. This node itself connects to another join_node that links all the bounding relations of the IE relation.\nget_rel node: Connects each rule relation in the rule’s body to the join_node. This node is linked to the corresponding rule root.\nselect_node: Used for relations with the same free variable (e.g., A(X, X)) or relations with some constant value (e.g., A(1, X)). This node deals with filtering tuples and is connected to the join_node.\nBase Relations: Each base relation is directly connected to the join_node.\n\nHere’s a representation of the term graph for the spannerlog program discussed:\nglobal root\n\n   rule_rel node (of C)\n       union node\n           project node\n               get_rel node (get A)  @note: there isn't join node since there is only one body relation.\n\n   rule_rel node (of D)\n       union node\n           project node\n               get_rel node (get C)  @note: there isn't join node since there is only one body relation.\n                   rule_rel node (of C)\n\n           project node\n               join node (join A, B and ID)\n                   get_rel node (get B)\n                   select_node (select from A)\n                       get_rel node (get A)\n                   calc node (of ID)\n                       join node (join A and B)\n                           get_rel node (get B)         @note: this get_rel node is the same one from above.\n                           select_node (select from A)  @note: this select node is the same one from above.\n                               get_rel node (get A)*\n\nsource\n\n\nTermGraph._compute_bounding_graph\n\n TermGraph._compute_bounding_graph\n                                    (relations:Set[spannerlib.ast_node_typ\n                                    es.Relation], ie_relations:Set[spanner\n                                    lib.ast_node_types.IERelation])\n\nThis function gets body relations of a rule and computes for each ie relation the relations that bound it.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelations\nSet[Relation]\nset of the regular relations in the rule body\n\n\nie_relations\nSet[IERelation]\nset of the ie relations in the rule body\n\n\nReturns\nOrderedDictType[IERelation, Set[Union[Relation, IERelation]]]\na dictionary that maps each ie function to a set of it’s bounding relations\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn some cases ie relation is bounded by other ie relation. e.g. A(X) &lt;- B(Y), C(Z) -&gt; (X), D(Y) -&gt; (Z); in this example C is bounded only by D.",
    "crumbs": [
      "src",
      "Graphs",
      "Graphs"
    ]
  },
  {
    "objectID": "general_utils.html",
    "href": "general_utils.html",
    "title": "General Utils",
    "section": "",
    "text": "source\n\nstrip_lines\n\n strip_lines (text:str)\n\nremoves leading and trailing whitespace from each line in the input text and excludes empty lines.\n\nsource\n\n\nfixed_point\n\n fixed_point (start:Any, step:Callable, distance:Callable, thresh:int=0)\n\nImplementation of a generic fixed point algorithm - an algorithm that takes a step function and runs it until some distance is zero or below a threshold.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\ntyping.Any\n\na starting value\n\n\nstep\ntyping.Callable\n\na step function\n\n\ndistance\ntyping.Callable\n\na function that measures distance between the input and the output of the step function\n\n\nthresh\nint\n0\na distance threshold\n\n\nReturns\ntyping.Any\n\n\n\n\n\n\nsource\n\n\nget_free_var_names\n\n get_free_var_names (term_list:Sequence, type_list:Sequence)\n\n@raise Exception: if length of term list doesn’t match the length of type list.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nterm_list\ntyping.Sequence\na list of terms\n\n\ntype_list\ntyping.Sequence\na list of the term types\n\n\nReturns\ntyping.Set[str]\na set of all the free variable names in term_list\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nterm_list = [\"X\", 10, \"Z\"]\ntype_list = [DataTypes.free_var_name, DataTypes.integer, DataTypes.free_var_name]    \nprint(get_free_var_names(term_list, type_list))\n\n{'Z', 'X'}\n\n\n\n\n\n\nsource\n\n\nposition_freevar_pairs\n\n position_freevar_pairs (relation:Union[spannerlib.ast_node_types.Relation\n                         ,spannerlib.ast_node_types.IERelation])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation\ntyping.Union[spannerlib.ast_node_types.Relation, spannerlib.ast_node_types.IERelation]\na relation (either a normal relation or an ie relation)\n\n\nReturns\ntyping.List[typing.Tuple[int, str]]\na list of all (index, free_var) pairs based on term_list\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nterm_list = [\"X\", \"abc\", \"Y\", \"def\", \"Z\"]\ntype_list = [DataTypes.free_var_name, DataTypes.string, DataTypes.free_var_name, DataTypes.integer, DataTypes.free_var_name]\n\nrelation = Relation(\"relation1\",term_list, type_list)\n\nprint(position_freevar_pairs(relation))\n\n[(0, 'X'), (2, 'Y'), (4, 'Z')]\n\n\n\n\n\n\nsource\n\n\nget_input_free_var_names\n\n get_input_free_var_names (relation:Union[spannerlib.ast_node_types.Relati\n                           on,spannerlib.ast_node_types.IERelation])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation\ntyping.Union[spannerlib.ast_node_types.Relation, spannerlib.ast_node_types.IERelation]\na relation (either a normal relation or an ie relation)\n\n\nReturns\ntyping.Set[typing.Any]\na set of the free variables used as input terms in the relation.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the input is relation it returns empty set as the regular relation don’t have input free variables, if the input is ie-relation it returns it’s input free variables\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\ninput_term_list = [\"X\", \"Y\"]\ninput_type_list = [DataTypes.free_var_name, DataTypes.integer]\n\nie_relation = IERelation(\"relation1\",input_term_list, input_type_list, [], [])\n\nprint(get_input_free_var_names(ie_relation))\n\n{'X'}\n\n\n\n\n\n\nsource\n\n\nget_output_free_var_names\n\n get_output_free_var_names (relation:Union[spannerlib.ast_node_types.Relat\n                            ion,spannerlib.ast_node_types.IERelation])\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation\ntyping.Union[spannerlib.ast_node_types.Relation, spannerlib.ast_node_types.IERelation]\na relation (either a normal relation or an ie relation)\n\n\nReturns\ntyping.Set[str]\na set of the free variables used as output terms in the relation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the input is relation it returns empty set as the regular relation don’t have input free variables, if the input is ie-relation it returns it’s input free variables\n\n\n\nsource\n\n\nget_free_var_to_relations_dict\n\n get_free_var_to_relations_dict (relations:Set[Union[spannerlib.ast_node_t\n                                 ypes.Relation,spannerlib.ast_node_types.I\n                                 ERelation]])\n\nFinds for each free var in any of the relations, all the relations that contain it. also return the free vars’ index in each relation (as pairs).  for example:  relations = [a(X,Y), b(Y)] -&gt; dict = {X:[(a(X,Y),0)], Y:[(a(X,Y),1),(b(Y),0)]}\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelations\ntyping.Set[typing.Union[spannerlib.ast_node_types.Relation, spannerlib.ast_node_types.IERelation]]\na set of relations\n\n\nReturns\ntyping.Dict[str, typing.List[typing.Tuple[typing.Union[spannerlib.ast_node_types.Relation, spannerlib.ast_node_types.IERelation], int]]]\na mapping between each free var to the relations and corresponding columns in which it appears\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nterm_list_1 = [\"X\", \"abc\", \"Y\", \"def\", \"Z\"]\ntype_list_1 = [DataTypes.free_var_name, DataTypes.string, DataTypes.free_var_name, DataTypes.integer, DataTypes.free_var_name]\n\nrelation_1 = Relation(\"relation_1\",term_list_1, type_list_1)\n\nterm_list_2 = [\"X\", \"Y\"]\ntype_list_2 = [DataTypes.free_var_name, DataTypes.free_var_name]\n\nrelation_2 = Relation(\"relation_2\",term_list_2, type_list_2)\nrelations_set = {relation_1,relation_2}\nget_free_var_to_relations_dict(relations_set)\n\n{'Y': [(relation_1(X, \"abc\", Y, def, Z), 2), (relation_2(X, Y), 1)],\n 'Z': [(relation_1(X, \"abc\", Y, def, Z), 4)],\n 'X': [(relation_1(X, \"abc\", Y, def, Z), 0), (relation_2(X, Y), 0)]}\n\n\n\n\n\n\nsource\n\n\ncheck_properly_typed_term_list\n\n check_properly_typed_term_list (term_list:Sequence, type_list:Sequence,\n                                 correct_type_list:Sequence, symbol_table:\n                                 spannerlib.symbol_table.SymbolTableBase)\n\nChecks if the term list is properly typed. the term list could include free variables, this method will assume their actual type is correct.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nterm_list\ntyping.Sequence\nthe term list to be type checked\n\n\ntype_list\ntyping.Sequence\nthe types of the terms in term_list\n\n\ncorrect_type_list\ntyping.Sequence\na list of the types that the terms must have to pass the type check\n\n\nsymbol_table\nSymbolTableBase\na symbol table (used to get the types of variables)\n\n\nReturns\nbool\nTrue if the type check passed, else False\n\n\n\n\nsource\n\n\ncheck_properly_typed_relation\n\n check_properly_typed_relation (relation:Union[spannerlib.ast_node_types.R\n                                elation,spannerlib.ast_node_types.IERelati\n                                on], symbol_table:spannerlib.symbol_table.\n                                SymbolTableBase)\n\nChecks if a relation is properly typed, this check ignores free variables.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrelation\ntyping.Union[spannerlib.ast_node_types.Relation, spannerlib.ast_node_types.IERelation]\nthe relation to be checked\n\n\nsymbol_table\nSymbolTableBase\na symbol table (to check the types of regular variables)\n\n\nReturns\nbool\ntrue if the relation is properly typed, else false\n\n\n\n\nsource\n\n\ntype_check_rule_free_vars_aux\n\n type_check_rule_free_vars_aux (term_list:Sequence, type_list:Sequence,\n                                correct_type_list:Sequence,\n                                free_var_to_type:Dict,\n                                conflicted_free_vars:Set)\n\nA helper function for the method type_check_rule_free_vars performs the free variables type checking on term_list.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nterm_list\ntyping.Sequence\nthe term list of a rule body relation\n\n\ntype_list\ntyping.Sequence\nthe types of the terms in term_list\n\n\ncorrect_type_list\ntyping.Sequence\na list of the types that the terms in the term list should have\n\n\nfree_var_to_type\ntyping.Dict\na mapping of free variables to their type (those that are currently known)\n\n\nconflicted_free_vars\ntyping.Set\nthis function adds conflicting free variables that it finds to this set\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\ntype_check_rule_free_vars\n\n type_check_rule_free_vars (rule:spannerlib.ast_node_types.Rule,\n                            symbol_table:spannerlib.symbol_table.SymbolTab\n                            leBase)\n\nFree variables in rules get their type from the relations in the rule body.  it is possible for a free variable to be expected to be more than one type (meaning it has conflicting types).  for each free variable in the rule body relations, this method will check for its type and will check if it has conflicting types\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nRule\nThe rule to be checked\n\n\nsymbol_table\nSymbolTableBase\na symbol table (used to get the schema of the relation)\n\n\nReturns\ntyping.Tuple[typing.Dict[str, spannerlib.primitive_types.DataTypes], typing.Set[str]]\na tuple (free_var_to_type, conflicted_free_vars) wherefree_var_to_type: a mapping from a free variable to its typeconflicted_free_vars: a set of all the conflicted free variables\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nthis function updates free_var_to_type’s mapping if it finds new free variables in term_list\n\n\n\nsource\n\n\nrule_to_relation_name\n\n rule_to_relation_name (rule:str)\n\nExtracts the relation name from the rule string.\n\n\n\n\nType\nDetails\n\n\n\n\nrule\nstr\na string that represents a rule\n\n\nReturns\nstr\nthe name of the rule relation\n\n\n\n\nsource\n\n\nstring_to_span\n\n string_to_span (string_of_span:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\nstring_of_span\nstr\nstr represenation of a Span object\n\n\nReturns\ntyping.Union[spannerlib.primitive_types.Span, NoneType]\nSpan object initialized based on the string_of_span it received as input\n\n\n\n\nsource\n\n\nextract_one_relation\n\n extract_one_relation (func:Callable)\n\nThis decorator is used by engine operators that expect to get exactly one input relation but actually get a list of relations.",
    "crumbs": [
      "src",
      "Generic",
      "General Utils"
    ]
  },
  {
    "objectID": "tests/05e_test_recursive.html",
    "href": "tests/05e_test_recursive.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\n\n\ndef test_recursive() -&gt; None:\n    commands = '''\n            new parent(str, str)\n            parent(\"Liam\", \"Noah\")\n            parent(\"Noah\", \"Oliver\")\n            parent(\"James\", \"Lucas\")\n            parent(\"Noah\", \"Benjamin\")\n            parent(\"Benjamin\", \"Mason\")\n            ancestor(X,Y) &lt;- parent(X,Y)\n            ancestor(X,Y) &lt;- parent(X,Z), ancestor(Z,Y)\n\n            ?ancestor(\"Liam\", X)\n            ?ancestor(X, \"Mason\")\n            ?ancestor(\"Mason\", X)\n            '''\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'ancestor(\"Liam\", X)':\n            X\n        ----------\n          Mason\n          Oliver\n         Benjamin\n           Noah\n\n        {QUERY_RESULT_PREFIX}'ancestor(X, \"Mason\")':\n            X\n        ----------\n           Noah\n           Liam\n         Benjamin\n\n        {QUERY_RESULT_PREFIX}'ancestor(\"Mason\", X)':\n        []\n        \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_recursive()\n\nprinting results for query 'ancestor(\"Liam\", X)':\n    X\n----------\n Benjamin\n  Mason\n   Noah\n  Oliver\n\nprinting results for query 'ancestor(X, \"Mason\")':\n    X\n----------\n Benjamin\n   Liam\n   Noah\n\nprinting results for query 'ancestor(\"Mason\", X)':\n[]\n\n\n\n\ndef test_mutually_recursive_basic() -&gt; None:\n    commands = \"\"\"\n            new C(int)\n            C(1)\n            C(2)\n            C(3)\n\n            B(X) &lt;- C(X)\n            A(X) &lt;- B(X)\n            B(X) &lt;- A(X)\n\n            ?A(X)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n        X\n        -----\n        1\n        2\n        3\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_mutually_recursive_basic()\n\nprinting results for query 'A(X)':\n   X\n-----\n   1\n   2\n   3"
  },
  {
    "objectID": "tests/05o_test_complex.html",
    "href": "tests/05o_test_complex.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from typing import Iterable, Any, Tuple\n\nfrom spannerlib.primitive_types import DataTypes\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test, run_test_df_compare\nimport pandas as pd\n\n\n\ndef test_issue_80_1() -&gt; None:\n    def which_century(year) -&gt; Iterable[int]:\n        yield int(year / 100) + 1\n\n    in_out_types = [DataTypes.integer]\n\n    which_century_dict = dict(ie_function=which_century,\n                              ie_function_name='which_century',\n                              in_rel=in_out_types,\n                              out_rel=in_out_types)\n\n    def which_era(cet) -&gt; Iterable[str]:\n        if 1 &lt;= cet &lt; 4:\n            yield \"Targerian Regime\"\n        elif 4 &lt;= cet &lt; 8:\n            yield \"Lanister Regime\"\n        elif 8 &lt;= cet &lt; 12:\n            yield \"Stark Regime\"\n        elif 12 &lt;= cet &lt; 16:\n            yield \"Barathion Regime\"\n        elif cet &gt;= 16:\n            yield \"Long Winter\"\n\n    which_era_dict = dict(ie_function=which_era,\n                          ie_function_name='which_era',\n                          in_rel=[DataTypes.integer],\n                          out_rel=[DataTypes.string])\n\n    commands = \"\"\"new event(str, int)\n                        event(\"First Dragon\", 250)\n                        event(\"Mad king\", 390)\n                        event(\"Winter came\", 1750)\n                        event(\"Hodor\", 999)\n                        event(\"Joffery died\", 799)\n                        \n                        new important_year(int)\n                        important_year(999)\n                        important_year(1750)\n                        important_year(250)\n                        \n                        \n                        important_events(EVE, Y) &lt;- event(EVE, Y), important_year(Y)\n                        \n                        important_events_per_cet(EVE, CET) &lt;- important_events(EVE, Y), which_century(Y) -&gt; (CET)\n                        ?important_events_per_cet(EVE, CET)\n            \"\"\"\n    commands2 = \"\"\"\n                        important_events_per_era(EVE, ERA) &lt;- important_events_per_cet(EVE, CET), which_era(CET) -&gt; (ERA)\n                        ?important_events_per_era(EVE, ERA)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'important_events_per_cet(EVE, CET)':\n                         EVE      |   CET\n                    --------------+-------\n                     First Dragon |     3\n                     Winter came  |    18\n                        Hodor     |    10\n         \"\"\"\n\n    expected_result2 = f\"\"\"{QUERY_RESULT_PREFIX}'important_events_per_era(EVE, ERA)':\n                         EVE      |       ERA\n                    --------------+------------------\n                     First Dragon | Targerian Regime\n                     Winter came  |   Long Winter \n                        Hodor     |   Stark Regime\n        \"\"\"\n\n    session = run_test(commands, expected_result, [which_century_dict])\n\n    run_test(commands2, expected_result2, [which_era_dict], session=session)\n\ntest_issue_80_1()\n\nprinting results for query 'important_events_per_cet(EVE, CET)':\n     EVE      |   CET\n--------------+-------\n First Dragon |     3\n Winter came  |    18\n    Hodor     |    10\n\nprinting results for query 'important_events_per_era(EVE, ERA)':\n     EVE      |       ERA\n--------------+------------------\n First Dragon | Targerian Regime\n Winter came  |   Long Winter\n    Hodor     |   Stark Regime\n\n\n\n\ndef test_complex_history_analysis() -&gt; None:\n    def get_population_growth_rate(year) -&gt; Iterable[int]:\n        if year &lt;= 1700:\n            yield 1\n        elif 1700 &lt; year &lt;= 1900:\n            yield 3\n        elif 1900 &lt; year &lt;= 2000:\n            yield 5\n        else:  \n            yield 1\n\n    population_growth_dict = dict(ie_function=get_population_growth_rate,\n                                  ie_function_name='get_population_growth_rate',\n                                  in_rel=[DataTypes.integer],\n                                  out_rel=[DataTypes.integer])\n\n    def historical_event(year) -&gt; Iterable[str]:\n        if year == 1600:\n            yield \"Discovery of New Land\"\n        elif year == 1750:\n            yield \"Industrial Revolution\"\n        elif year == 1920:\n            yield \"Roaring Twenties\"\n        elif year == 2000:\n            yield \"Y2K Bug Panic\"\n        else:\n            yield \"No significant event\"\n\n    historical_event_dict = dict(ie_function=historical_event,\n                                 ie_function_name='historical_event',\n                                 in_rel=[DataTypes.integer],\n                                 out_rel=[DataTypes.string])\n\n    commands = \"\"\"new year_event(int, str)\n                        year_event(1600, \"Discovery of New Land\")\n                        year_event(1750, \"Industrial Revolution\")\n                        year_event(1920, \"Roaring Twenties\")\n                        year_event(2000, \"Y2K Bug Panic\")\n                        \n                        new population_year(int)\n                        population_year(1600)\n                        population_year(1700)\n                        population_year(1800)\n                        population_year(1900)\n                        population_year(2000)\n                        population_year(1750)\n                        population_year(1920)\n                        \n                        population_growth_rate(Y,GROWTH) &lt;- population_year(Y) ,get_population_growth_rate(Y) -&gt; (GROWTH)\n                        \n                        historical_event(Y, EVENT) &lt;- year_event(Y, EVENT)\n                        \n                        important_years(Y, EVENT, GROWTH) &lt;- population_growth_rate(Y, GROWTH), historical_event(Y, EVENT)\n                        \n                        ?important_years(Y, EVENT, GROWTH)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'important_years(Y, EVENT, GROWTH)':\n                          Y |         EVENT         |   GROWTH\n                      ------+-----------------------+----------\n                       1600 | Discovery of New Land |        1\n                       1750 | Industrial Revolution |        3\n                       1920 |   Roaring Twenties    |        5\n                       2000 |     Y2K Bug Panic     |        5\n         \"\"\"\n\n    session = run_test(commands, expected_result, [population_growth_dict, historical_event_dict])\n\ntest_complex_history_analysis()\n\nprinting results for query 'important_years(Y, EVENT, GROWTH)':\n    Y |         EVENT         |   GROWTH\n------+-----------------------+----------\n 1600 | Discovery of New Land |        1\n 1750 | Industrial Revolution |        3\n 1920 |   Roaring Twenties    |        5\n 2000 |     Y2K Bug Panic     |        5\n\n\n\n\ndef test_advanced_history_analysis() -&gt; None:\n    def get_population_growth_rate(year) -&gt; Iterable[int]:\n        if year &lt;= 1700:\n            yield 1\n        elif 1700 &lt; year &lt;= 1900:\n            yield 3\n        elif 1900 &lt; year &lt;= 2000:\n            yield 5\n        else:\n            yield 1\n\n    population_growth_dict = dict(ie_function=get_population_growth_rate,\n                                  ie_function_name='get_population_growth_rate',\n                                  in_rel=[DataTypes.integer],\n                                  out_rel=[DataTypes.integer])\n\n    def technological_advancement(year) -&gt; Iterable[str]:\n        if year &lt;= 1700:\n            yield \"Low\"\n        elif 1700 &lt; year &lt;= 1900:\n            yield \"Moderate\"\n        elif 1900 &lt; year &lt;= 2000:\n            yield \"High\"\n        else:\n            yield \"Low\"\n\n    technological_advancement_dict = dict(ie_function=technological_advancement,\n                                         ie_function_name='technological_advancement',\n                                         in_rel=[DataTypes.integer],\n                                         out_rel=[DataTypes.string])\n\n    commands = \"\"\"new year_event(int, str)\n                        year_event(1600, \"Discovery of New Land\")\n                        year_event(1750, \"Industrial Revolution\")\n                        year_event(1920, \"Roaring Twenties\")\n                        year_event(2000, \"Y2K Bug Panic\")\n                        \n                        new population_year(int)\n                        population_year(1600)\n                        population_year(1700)\n                        population_year(1800)\n                        population_year(1900)\n                        population_year(2000)\n                        population_year(1750)\n                        population_year(1920)\n                        \n                        population_growth_rate(Y, GROWTH) &lt;- population_year(Y), get_population_growth_rate(Y) -&gt; (GROWTH)\n                        \n                        technological_advancement(Y, TECH) &lt;- year_event(Y, Z), technological_advancement(Y) -&gt; (TECH)\n                        \n                        important_years(Y, EVENT, GROWTH, TECH) &lt;- population_growth_rate(Y, GROWTH), year_event(Y, EVENT), technological_advancement(Y, TECH)\n                        \n                        ?important_years(Y, EVENT, GROWTH, TECH)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'important_years(Y, EVENT, GROWTH, TECH)':\n                          Y |         EVENT         |   GROWTH |   TECH\n                      ------+-----------------------+----------+----------\n                       1600 | Discovery of New Land |        1 |   Low\n                       1750 | Industrial Revolution |        3 | Moderate\n                       1920 |   Roaring Twenties    |        5 |   High\n                       2000 |     Y2K Bug Panic     |        5 |   High\n         \"\"\"\n\n    session = run_test(commands, expected_result, [population_growth_dict, technological_advancement_dict])\n\ntest_advanced_history_analysis()\n\nprinting results for query 'important_years(Y, EVENT, GROWTH, TECH)':\n    Y |         EVENT         |   GROWTH |   TECH\n------+-----------------------+----------+----------\n 1600 | Discovery of New Land |        1 |   Low\n 1750 | Industrial Revolution |        3 | Moderate\n 1920 |   Roaring Twenties    |        5 |   High\n 2000 |     Y2K Bug Panic     |        5 |   High\n\n\n\n\ndef test_football_analysis() -&gt; None:\n    def get_match_result(home_team, away_team, home_goals, away_goals) -&gt; Iterable[str]:\n        if home_goals &gt; away_goals:\n            yield \"Home Win\"\n        elif home_goals &lt; away_goals:\n            yield \"Away Win\"\n        else:\n            yield \"Draw\"\n\n    match_result_dict = dict(ie_function=get_match_result,\n                             ie_function_name='get_match_result',\n                             in_rel=[DataTypes.string, DataTypes.string, DataTypes.integer, DataTypes.integer],\n                             out_rel=[DataTypes.string])\n\n    def get_player_position(player_name) -&gt; Iterable[str]:\n        if player_name in [\"Lionel Messi\", \"Cristiano Ronaldo\"]:\n            yield \"Forward\"\n        elif player_name in [\"Virgil van Dijk\", \"Sergio Ramos\"]:\n            yield \"Defender\"\n        else:\n            yield \"Unknown\"\n\n    player_position_dict = dict(ie_function=get_player_position,\n                                ie_function_name='get_player_position',\n                                in_rel=[DataTypes.string],\n                                out_rel=[DataTypes.string])\n\n    commands = \"\"\"new team(str)\n                        team(\"Barcelona\")\n                        team(\"Real Madrid\")\n                        team(\"Liverpool\")\n                        team(\"Juventus\")\n                        \n                        new player(str, str)\n                        player(\"Lionel Messi\", \"Barcelona\")\n                        player(\"Cristiano Ronaldo\", \"Juventus\")\n                        player(\"Virgil van Dijk\", \"Liverpool\")\n                        player(\"Sergio Ramos\", \"Real Madrid\")\n                        \n                        new match(str, str, int, int)\n                        match(\"Barcelona\", \"Real Madrid\", 3, 2)\n                        match(\"Real Madrid\", \"Barcelona\", 1, 1)\n                        match(\"Liverpool\", \"Juventus\", 2, 0)\n                        match(\"Juventus\", \"Liverpool\", 0, 1)\n                        \n                        match_result(HT, AT, HG, AG, RESULT) &lt;- match(HT, AT, HG, AG), get_match_result(HT, AT, HG, AG) -&gt; (RESULT)\n                        \n                        player_position(P, POS) &lt;- player(P, X), get_player_position(P) -&gt; (POS)\n                        \n                        ?match_result(HT, AT, HG, AG, RESULT)\n                        ?player_position(P, POS)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'match_result(HT, AT, HG, AG, RESULT)':\n                         HT      |     AT      |   HG |   AG |  RESULT\n                    -------------+-------------+------+------+----------\n                      Barcelona  | Real Madrid |    3 |    2 | Home Win\n                     Real Madrid |  Barcelona  |    1 |    1 |   Draw\n                      Liverpool  |  Juventus   |    2 |    0 | Home Win\n                      Juventus   |  Liverpool  |    0 |    1 | Away Win\n\n                    {QUERY_RESULT_PREFIX}'player_position(P, POS)':\n                             P         |   POS\n                    -------------------+----------\n                       Lionel Messi    | Forward\n                     Cristiano Ronaldo | Forward\n                      Virgil van Dijk  | Defender\n                       Sergio Ramos    | Defender\n         \"\"\"\n\n    session = run_test(commands, expected_result, [match_result_dict, player_position_dict])\n\ntest_football_analysis()\n\nprinting results for query 'match_result(HT, AT, HG, AG, RESULT)':\n     HT      |     AT      |   HG |   AG |  RESULT\n-------------+-------------+------+------+----------\n  Barcelona  | Real Madrid |    3 |    2 | Home Win\n Real Madrid |  Barcelona  |    1 |    1 |   Draw\n  Liverpool  |  Juventus   |    2 |    0 | Home Win\n  Juventus   |  Liverpool  |    0 |    1 | Away Win\n\nprinting results for query 'player_position(P, POS)':\n         P         |   POS\n-------------------+----------\n   Lionel Messi    | Forward\n Cristiano Ronaldo | Forward\n  Virgil van Dijk  | Defender\n   Sergio Ramos    | Defender\n\n\n\n\ndef test_movie_analysis() -&gt; None:\n    def get_movie_genre(movie_title) -&gt; Iterable[str]:\n        if movie_title == \"The Shawshank Redemption\":\n            yield \"Drama\"\n        elif movie_title == \"Inception\":\n            yield \"Sci-Fi\"\n        elif movie_title == \"The Dark Knight\":\n            yield \"Action\"\n        else:\n            yield \"Unknown\"\n\n    movie_genre_dict = dict(ie_function=get_movie_genre,\n                            ie_function_name='get_movie_genre',\n                            in_rel=[DataTypes.string],\n                            out_rel=[DataTypes.string])\n\n    def get_actor_role(actor_name) -&gt; Iterable[str]:\n        if actor_name == \"Tom Hanks\":\n            yield \"Protagonist\"\n        elif actor_name == \"Leonardo DiCaprio\":\n            yield \"Lead\"\n        else:\n            yield \"Supporting\"\n\n    actor_role_dict = dict(ie_function=get_actor_role,\n                          ie_function_name='get_actor_role',\n                          in_rel=[DataTypes.string],\n                          out_rel=[DataTypes.string])\n\n    commands = \"\"\"new movie(str)\n                        movie(\"The Shawshank Redemption\")\n                        movie(\"Inception\")\n                        movie(\"The Dark Knight\")\n                        movie(\"Interstellar\")\n                        \n                        new actor(str)\n                        actor(\"Tom Hanks\")\n                        actor(\"Leonardo DiCaprio\")\n                        actor(\"Christian Bale\")\n                        \n                        new cast(str, str)\n                        cast(\"The Shawshank Redemption\", \"Tom Hanks\")\n                        cast(\"Inception\", \"Leonardo DiCaprio\")\n                        cast(\"The Dark Knight\", \"Christian Bale\")\n                        cast(\"Interstellar\", \"Tom Hanks\")\n                        \n                        movie_genre(M, GENRE) &lt;- movie(M), get_movie_genre(M) -&gt; (GENRE)\n                        \n                        actor_role(A, ROLE) &lt;- actor(A), get_actor_role(A) -&gt; (ROLE)\n                        \n                        ?movie_genre(M, GENRE)\n                        ?actor_role(A, ROLE)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'movie_genre(M, GENRE)':\n                                M             |  GENRE\n                    --------------------------+---------\n                     The Shawshank Redemption |  Drama\n                            Inception         | Sci-Fi\n                         The Dark Knight      | Action\n                           Interstellar       | Unknown\n\n                    {QUERY_RESULT_PREFIX}'actor_role(A, ROLE)':\n                             A         |    ROLE\n                    -------------------+-------------\n                         Tom Hanks     | Protagonist\n                     Leonardo DiCaprio |    Lead\n                      Christian Bale   | Supporting\n         \"\"\"\n\n    session = run_test(commands, expected_result, [movie_genre_dict, actor_role_dict])\n\ntest_movie_analysis()\n\nprinting results for query 'movie_genre(M, GENRE)':\n            M             |  GENRE\n--------------------------+---------\n The Shawshank Redemption |  Drama\n        Inception         | Sci-Fi\n     The Dark Knight      | Action\n       Interstellar       | Unknown\n\nprinting results for query 'actor_role(A, ROLE)':\n         A         |    ROLE\n-------------------+-------------\n     Tom Hanks     | Protagonist\n Leonardo DiCaprio |    Lead\n  Christian Bale   | Supporting\n\n\n\n\ndef test_game_of_thrones_analysis() -&gt; None:\n    def get_house_allegiance(character) -&gt; Iterable[str]:\n        allegiances = {\n            \"Jon Snow\": \"Stark\",\n            \"Daenerys Targaryen\": \"Targaryen\",\n            \"Cersei Lannister\": \"Lannister\",\n            \"Tyrion Lannister\": \"Lannister\",\n            \"Arya Stark\": \"Stark\",\n            \"Sansa Stark\": \"Stark\",\n            \"Jaime Lannister\": \"Lannister\"\n        }\n        if character in allegiances:\n            yield allegiances[character]\n        else:\n            yield \"Unknown\"\n\n    house_allegiance_dict = dict(ie_function=get_house_allegiance,\n                                  ie_function_name='get_house_allegiance',\n                                  in_rel=[DataTypes.string],\n                                  out_rel=[DataTypes.string])\n\n    def get_kingdom(house) -&gt; Iterable[str]:\n        kingdoms = {\n            \"Stark\": \"North\",\n            \"Targaryen\": \"Westeros\",\n            \"Lannister\": \"Westeros\"\n        }\n        if house in kingdoms:\n            yield kingdoms[house]\n        else:\n            yield \"Unknown\"\n\n    house_kingdom_dict = dict(ie_function=get_kingdom,\n                              ie_function_name='get_kingdom',\n                              in_rel=[DataTypes.string],\n                              out_rel=[DataTypes.string])\n\n    commands = \"\"\"new character(str)\n                        character(\"Jon Snow\")\n                        character(\"Daenerys Targaryen\")\n                        character(\"Cersei Lannister\")\n                        character(\"Tyrion Lannister\")\n                        character(\"Arya Stark\")\n                        character(\"Sansa Stark\")\n                        character(\"Jaime Lannister\")\n                        character(\"Eddard Stark\")\n                        \n                        house_allegiance(NAME, HOUSE) &lt;- character(NAME), get_house_allegiance(NAME) -&gt; (HOUSE)\n                        house_kingdom(HOUSE, KINGDOM) &lt;- house_allegiance(NAME,HOUSE) ,get_kingdom(HOUSE) -&gt; (KINGDOM)\n                        \n                        ?house_allegiance(NAME, HOUSE)\n                        ?house_kingdom(HOUSE, KINGDOM)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'house_allegiance(NAME, HOUSE)':\n                        NAME        |   HOUSE\n                --------------------+-----------\n                      Jon Snow      |   Stark\n                 Daenerys Targaryen | Targaryen\n                  Cersei Lannister  | Lannister\n                  Tyrion Lannister  | Lannister\n                     Arya Stark     |   Stark\n                    Sansa Stark     |   Stark\n                  Jaime Lannister   | Lannister\n                    Eddard Stark    |  Unknown\n\n                    {QUERY_RESULT_PREFIX}'house_kingdom(HOUSE, KINGDOM)':\n                       HOUSE   |  KINGDOM\n                    -----------+-----------\n                       Stark   |   North\n                     Targaryen | Westeros\n                     Lannister | Westeros\n                      Unknown  |  Unknown\n         \"\"\"\n\n    session = run_test(commands, expected_result, [house_allegiance_dict, house_kingdom_dict])\n\ntest_game_of_thrones_analysis()\n\nprinting results for query 'house_allegiance(NAME, HOUSE)':\n        NAME        |   HOUSE\n--------------------+-----------\n      Jon Snow      |   Stark\n Daenerys Targaryen | Targaryen\n  Cersei Lannister  | Lannister\n  Tyrion Lannister  | Lannister\n     Arya Stark     |   Stark\n    Sansa Stark     |   Stark\n  Jaime Lannister   | Lannister\n    Eddard Stark    |  Unknown\n\nprinting results for query 'house_kingdom(HOUSE, KINGDOM)':\n   HOUSE   |  KINGDOM\n-----------+-----------\n   Stark   |   North\n Targaryen | Westeros\n Lannister | Westeros\n  Unknown  |  Unknown\n\n\n\n\ndef test_dinosaur_analysis() -&gt; None:\n    def carnivore_diet(dinosaur) -&gt; Iterable[int]:\n        carnivores = [\"Tyrannosaurus\", \"Velociraptor\", \"Spinosaurus\"]\n        if dinosaur in carnivores:\n            yield 1\n        else:\n            yield 0\n\n    carnivore_diet_dict = dict(ie_function=carnivore_diet,\n                               ie_function_name='carnivore_diet',\n                               in_rel=[DataTypes.string],\n                               out_rel=[DataTypes.integer])\n\n    def herbivore_diet(dinosaur) -&gt; Iterable[int]:\n        herbivores = [\"Triceratops\", \"Brachiosaurus\", \"Stegosaurus\"]\n        if dinosaur in herbivores:\n            yield 1\n        else:\n            yield 0\n\n    herbivore_diet_dict = dict(ie_function=herbivore_diet,\n                              ie_function_name='herbivore_diet',\n                              in_rel=[DataTypes.string],\n                              out_rel=[DataTypes.integer])\n\n    def get_dinosaur_period(dinosaur) -&gt; Iterable[str]:\n        periods = {\n            \"Tyrannosaurus\": \"Late Cretaceous\",\n            \"Triceratops\": \"Late Cretaceous\",\n            \"Velociraptor\": \"Late Cretaceous\",\n            \"Brachiosaurus\": \"Jurassic\",\n            \"Stegosaurus\": \"Jurassic\",\n            \"Spinosaurus\": \"Cretaceous\"\n        }\n        if dinosaur in periods:\n            yield periods[dinosaur]\n        else:\n            yield \"Unknown\"\n\n    dinosaur_period_dict = dict(ie_function=get_dinosaur_period,\n                               ie_function_name='get_dinosaur_period',\n                               in_rel=[DataTypes.string],\n                               out_rel=[DataTypes.string])\n\n    def get_dinosaur_classification(dinosaur) -&gt; Iterable[str]:\n        classifications = {\n            \"Tyrannosaurus\": \"Theropod\",\n            \"Triceratops\": \"Ceratopsian\",\n            \"Velociraptor\": \"Theropod\",\n            \"Brachiosaurus\": \"Sauropod\",\n            \"Stegosaurus\": \"Stegosaurid\",\n            \"Spinosaurus\": \"Spinosaurid\"\n        }\n        if dinosaur in classifications:\n            yield classifications[dinosaur]\n        else:\n            yield \"Unknown\"\n\n    dinosaur_classification_dict = dict(ie_function=get_dinosaur_classification,\n                                       ie_function_name='get_dinosaur_classification',\n                                       in_rel=[DataTypes.string],\n                                       out_rel=[DataTypes.string])\n\n    commands = \"\"\"new dinosaur(str)\n                        dinosaur(\"Tyrannosaurus\")\n                        dinosaur(\"Triceratops\")\n                        dinosaur(\"Velociraptor\")\n                        dinosaur(\"Brachiosaurus\")\n                        dinosaur(\"Stegosaurus\")\n                        dinosaur(\"Spinosaurus\")\n                        \n                        carnivore_diet(DINO, CARNIVORE) &lt;- dinosaur(DINO), carnivore_diet(DINO) -&gt; (CARNIVORE)\n                        herbivore_diet(DINO, HERBIVORE) &lt;- dinosaur(DINO), herbivore_diet(DINO) -&gt; (HERBIVORE)\n                        dinosaur_period(DINO, PERIOD) &lt;- dinosaur(DINO), get_dinosaur_period(DINO) -&gt; (PERIOD)\n                        dinosaur_classification(DINO, CLASS) &lt;- dinosaur(DINO), get_dinosaur_classification(DINO) -&gt; (CLASS)\n                        \n                        ?carnivore_diet(DINO, CARNIVORE)\n                        ?herbivore_diet(DINO, HERBIVORE)\n                        ?dinosaur_period(DINO, PERIOD)\n                        ?dinosaur_classification(DINO, CLASS)\n            \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'carnivore_diet(DINO, CARNIVORE)':\n                         DINO      |   CARNIVORE\n                    ---------------+-------------\n                     Tyrannosaurus |           1\n                      Triceratops  |           0\n                     Velociraptor  |           1\n                     Brachiosaurus |           0\n                      Stegosaurus  |           0\n                      Spinosaurus  |           1\n\n                    {QUERY_RESULT_PREFIX}'herbivore_diet(DINO, HERBIVORE)':\n                         DINO      |   HERBIVORE\n                    ---------------+-------------\n                     Tyrannosaurus |           0\n                      Triceratops  |           1\n                     Velociraptor  |           0\n                     Brachiosaurus |           1\n                      Stegosaurus  |           1\n                      Spinosaurus  |           0\n\n                    {QUERY_RESULT_PREFIX}'dinosaur_period(DINO, PERIOD)':\n                         DINO      |     PERIOD\n                    ---------------+-----------------\n                     Tyrannosaurus | Late Cretaceous\n                      Triceratops  | Late Cretaceous\n                     Velociraptor  | Late Cretaceous\n                     Brachiosaurus |    Jurassic\n                      Stegosaurus  |    Jurassic\n                      Spinosaurus  |   Cretaceous\n\n                    {QUERY_RESULT_PREFIX}'dinosaur_classification(DINO, CLASS)':\n                         DINO      |    CLASS\n                    ---------------+-------------\n                     Tyrannosaurus |  Theropod\n                      Triceratops  | Ceratopsian\n                     Velociraptor  |  Theropod\n                     Brachiosaurus |  Sauropod\n                      Stegosaurus  | Stegosaurid\n                      Spinosaurus  | Spinosaurid\n         \"\"\"\n\n    session = run_test(commands, expected_result, [carnivore_diet_dict, herbivore_diet_dict,\n                                                   dinosaur_period_dict, dinosaur_classification_dict])\n    \ntest_dinosaur_analysis()\n\nprinting results for query 'carnivore_diet(DINO, CARNIVORE)':\n     DINO      |   CARNIVORE\n---------------+-------------\n Tyrannosaurus |           1\n  Triceratops  |           0\n Velociraptor  |           1\n Brachiosaurus |           0\n  Stegosaurus  |           0\n  Spinosaurus  |           1\n\nprinting results for query 'herbivore_diet(DINO, HERBIVORE)':\n     DINO      |   HERBIVORE\n---------------+-------------\n Tyrannosaurus |           0\n  Triceratops  |           1\n Velociraptor  |           0\n Brachiosaurus |           1\n  Stegosaurus  |           1\n  Spinosaurus  |           0\n\nprinting results for query 'dinosaur_period(DINO, PERIOD)':\n     DINO      |     PERIOD\n---------------+-----------------\n Tyrannosaurus | Late Cretaceous\n  Triceratops  | Late Cretaceous\n Velociraptor  | Late Cretaceous\n Brachiosaurus |    Jurassic\n  Stegosaurus  |    Jurassic\n  Spinosaurus  |   Cretaceous\n\nprinting results for query 'dinosaur_classification(DINO, CLASS)':\n     DINO      |    CLASS\n---------------+-------------\n Tyrannosaurus |  Theropod\n  Triceratops  | Ceratopsian\n Velociraptor  |  Theropod\n Brachiosaurus |  Sauropod\n  Stegosaurus  | Stegosaurid\n  Spinosaurus  | Spinosaurid\n\n\n\n\ndef test_complex_economic_analysis() -&gt; None:\n    def calc_economic_index(interest, inflation) -&gt; Iterable[int]:\n        yield interest + inflation\n\n    calc_economic_index_dict = dict(ie_function=calc_economic_index,\n                                    ie_function_name='calc_economic_index',\n                                    in_rel=[DataTypes.integer, DataTypes.integer],\n                                    out_rel=[DataTypes.integer])\n\n    def classify_economic_index(index) -&gt; Iterable[str]:\n        if index &gt; 15:\n            yield \"Bad\"\n        else:\n            yield \"Good\"\n\n    classify_economic_index_dict = dict(ie_function=classify_economic_index,\n                                        ie_function_name='classify_economic_index',\n                                        in_rel=[DataTypes.integer],\n                                        out_rel=[DataTypes.string])\n\n    commands = \"\"\"new economy_data(int, int, int)  # year, interest, inflation\n                        economy_data(2000, 4, 3)\n                        economy_data(2008, 8, 10)\n                        economy_data(2019, 3, 2)\n\n                        complex_economic_status(YEAR, STATUS) &lt;- economy_data(YEAR, INTEREST, INFLATION), calc_economic_index(INTEREST, INFLATION) -&gt; (INDEX), classify_economic_index(INDEX) -&gt; (STATUS)\n                        \n                        ?complex_economic_status(YEAR, STATUS)\n              \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'complex_economic_status(YEAR, STATUS)':\n                                   YEAR |  STATUS\n                                --------+----------\n                                2000 |   Good\n                                2008 |   Bad\n                                2019 |   Good\n                  \"\"\"\n\n    session = run_test(commands, expected_result, [calc_economic_index_dict, classify_economic_index_dict])\n\ntest_complex_economic_analysis()\n\nprinting results for query 'complex_economic_status(YEAR, STATUS)':\n   YEAR |  STATUS\n--------+----------\n   2000 |   Good\n   2008 |   Bad\n   2019 |   Good\n\n\n\n\ndef test_complex_historical_and_technological_analysis() -&gt; None:\n    def combine_impact(year, growth) -&gt; Iterable[int]:\n        yield year + growth\n\n    combine_impact_dict = dict(ie_function=combine_impact,\n                               ie_function_name='combine_impact',\n                               in_rel=[DataTypes.integer, DataTypes.integer],\n                               out_rel=[DataTypes.integer])\n\n    def classify_impact(impact) -&gt; Iterable[str]:\n        if impact &gt; 2200:\n            yield \"Significant\"\n        else:\n            yield \"Moderate\"\n\n    classify_impact_dict = dict(ie_function=classify_impact,\n                                ie_function_name='classify_impact',\n                                in_rel=[DataTypes.integer],\n                                out_rel=[DataTypes.string])\n\n    commands = \"\"\"new year_event(int, str)  \n                        year_event(1600, \"Discovery of New Land\")\n                        year_event(1750, \"Industrial Revolution\")\n\n                        new population_year(int, int)\n                        population_year(1600, 1)\n                        population_year(1750, 3)\n\n                        complex_impact_years(YEAR, EVENT, IMPACT_TYPE) &lt;- year_event(YEAR, EVENT), population_year(YEAR, GROWTH), combine_impact(YEAR, GROWTH) -&gt; (IMPACT), classify_impact(IMPACT) -&gt; (IMPACT_TYPE)\n                        \n                        ?complex_impact_years(YEAR, EVENT, IMPACT_TYPE)\n              \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'complex_impact_years(YEAR, EVENT, IMPACT_TYPE)':\n                               YEAR |         EVENT         |  IMPACT_TYPE\n                            --------+-----------------------+---------------\n                            1600 | Discovery of New Land |   Moderate\n                            1750 | Industrial Revolution |   Moderate\n                  \"\"\"\n\n    session = run_test(commands, expected_result, [combine_impact_dict, classify_impact_dict])\n\ntest_complex_historical_and_technological_analysis()\n\nprinting results for query 'complex_impact_years(YEAR, EVENT, IMPACT_TYPE)':\n   YEAR |         EVENT         |  IMPACT_TYPE\n--------+-----------------------+---------------\n   1600 | Discovery of New Land |   Moderate\n   1750 | Industrial Revolution |   Moderate\n\n\n\n\ndef test_complex_space_exploration_analysis() -&gt; None:\n    def calc_travel_time(distance) -&gt; Iterable[int]:\n        yield int(distance / 1000)\n\n    calc_travel_time_dict = dict(ie_function=calc_travel_time,\n                                 ie_function_name='calc_travel_time',\n                                 in_rel=[DataTypes.integer],\n                                 out_rel=[DataTypes.integer])\n\n    def analyze_elements(elements) -&gt; Iterable[int]:\n        score = 0\n        if 'Carbon' in elements:\n            score += 5\n        if 'Water' in elements:\n            score += 5\n        yield score\n\n    analyze_elements_dict = dict(ie_function=analyze_elements,\n                                 ie_function_name='analyze_elements',\n                                 in_rel=[DataTypes.string],\n                                 out_rel=[DataTypes.integer])\n\n    def calculate_life_probability(age, element_score, travel_time) -&gt; Iterable[int]:\n        yield int((element_score * 2) - (age / 1000) - travel_time)\n\n    calculate_life_probability_dict = dict(ie_function=calculate_life_probability,\n                                           ie_function_name='calculate_life_probability',\n                                           in_rel=[DataTypes.integer, DataTypes.integer, DataTypes.integer],\n                                           out_rel=[DataTypes.integer])\n\n    commands = \"\"\"new exoplanet(str, int, int, str)  # name, distance, age, elements\n                        exoplanet(\"PlanetA\", 4000, 10000, \"Carbon,Water\")\n                        exoplanet(\"PlanetB\", 9000, 5000, \"Hydrogen,Silicon\")\n                        \n                        travel_time(NAME, TIME) &lt;- exoplanet(NAME, DIST, AGE, ELEM), calc_travel_time(DIST) -&gt; (TIME)\n                        element_score(NAME, SCORE) &lt;- exoplanet(NAME, DIST, AGE, ELEM), analyze_elements(ELEM) -&gt; (SCORE)\n                        life_probability(NAME, PROB) &lt;- exoplanet(NAME, DIST, AGE, ELEM), travel_time(NAME, TIME), element_score(NAME, SCORE), calculate_life_probability(AGE, SCORE, TIME) -&gt; (PROB)\n                        ?life_probability(NAME, PROB)\n              \"\"\"\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'life_probability(NAME, PROB)':\n                            NAME   |   PROB\n                            ---------+--------\n                            PlanetA |      6\n                            PlanetB |    -14\n                  \"\"\"\n\n    session = run_test(commands, expected_result, [calc_travel_time_dict, analyze_elements_dict, calculate_life_probability_dict])\n\ntest_complex_space_exploration_analysis()\n\nprinting results for query 'life_probability(NAME, PROB)':\n  NAME   |   PROB\n---------+--------\n PlanetA |      6\n PlanetB |    -14\n\n\n\n\ndef test_complex_analysis() -&gt; None:\n    def price_category(price) -&gt; Iterable[int]:\n        if price &lt; 100:\n            yield 1\n        elif price &lt; 200:\n            yield 2\n        else:\n            yield 3\n\n    price_category_dict = dict(ie_function=price_category,\n                               ie_function_name='price_category',\n                               in_rel=[DataTypes.integer],\n                               out_rel=[DataTypes.integer])\n\n    def rating_category(rating) -&gt; Iterable[str]:\n        if rating &lt; 3:\n            yield \"Low\"\n        elif rating &lt; 4:\n            yield \"Medium\"\n        else:\n            yield \"High\"\n\n    rating_category_dict = dict(ie_function=rating_category,\n                                ie_function_name='rating_category',\n                                in_rel=[DataTypes.integer],\n                                out_rel=[DataTypes.string])\n\n    def calculate_discount(price_category, rating_category) -&gt; Iterable[int]:\n        if price_category == 1 and rating_category == \"High\":\n            yield 20\n        elif price_category == 2 and rating_category == \"Medium\":\n            yield 10\n        elif price_category == 3 and rating_category == \"Low\":\n            yield 5\n        else:\n            yield 0\n\n    calculate_discount_dict = dict(ie_function=calculate_discount,\n                                   ie_function_name='calculate_discount',\n                                   in_rel=[DataTypes.integer, DataTypes.string],\n                                   out_rel=[DataTypes.integer])\n\n    commands = \"\"\"\n               new product(str, int, int)\n               product(\"Laptop\", 999, 5)\n               product(\"Phone\", 499, 4)\n               product(\"Watch\", 199, 3)\n               product(\"Earphones\", 49, 2)\n               \n               price_category(P, CAT) &lt;- product(P, PRICE, R), price_category(PRICE) -&gt; (CAT)\n               \n               rating_category(P, CAT) &lt;- product(P, PRICE, R), rating_category(R) -&gt; (CAT)\n               \n               product_discount(P, CAT, DISCOUNT) &lt;- price_category(P, CAT), rating_category(P, RCAT), calculate_discount(CAT, RCAT) -&gt; (DISCOUNT)\n               \n               ?product_discount(P, CAT, DISCOUNT)\n               \"\"\"\n\n    expected_result = {\n        'P': ['Laptop', 'Phone', 'Watch', 'Earphones'],\n        'CAT': [3, 3, 2, 1],\n        'DISCOUNT': [0, 0, 10, 0]\n    }\n\n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [price_category_dict, rating_category_dict, calculate_discount_dict])\n\ntest_complex_analysis()\n\n\ndef test_complicated_analysis1() -&gt; None:\n    def ie_func1(age) -&gt; Iterable[int]:\n        yield age * 2\n    \n    def ie_func2(age, doubled_age) -&gt; Iterable[str]:\n        if doubled_age &gt; 50:\n            yield \"Old\"\n        else:\n            yield \"Young\"\n    ie_func1_dict = dict(ie_function=ie_func1,\n                     ie_function_name='ie_func1',\n                     in_rel=[DataTypes.integer],\n                     out_rel=[DataTypes.integer])\n\n    ie_func2_dict = dict(ie_function=ie_func2,\n                        ie_function_name='ie_func2',\n                        in_rel=[DataTypes.integer, DataTypes.integer],\n                        out_rel=[DataTypes.string])\n\n        \n    commands = \"\"\"\n               new person(str, int)\n               new job(str, int)\n               new hobby(str, str)\n               \n               person(\"Alice\", 30)\n               person(\"Bob\", 25)\n               person(\"Carol\", 55)\n               \n               job(\"Alice\", 5000)\n               job(\"Bob\", 6000)\n               job(\"Carol\", 10000)\n               \n               hobby(\"Alice\", \"Reading\")\n               hobby(\"Carol\", \"Swimming\")\n               \n               sample_rule1(P, S, H) &lt;- job(P, S), person(P, A), ie_func1(A) -&gt; (AD), hobby(P, H), ie_func2(A, AD) -&gt; (AGE_GROUP)\n               \n               ?sample_rule1(P, S, H)\n               \"\"\"\n    \n    expected_result = {\n        'P': ['Alice', 'Carol'],\n        'S': [5000, 10000],\n        'H': ['Reading', 'Swimming']\n    }\n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [ie_func1_dict, ie_func2_dict])\n\ntest_complicated_analysis1()\n\n\ndef test_complicated_analysis2() -&gt; None:\n    def ie_func3(salary) -&gt; Iterable[int]:\n        yield salary - 1000\n    \n    def ie_func4(age, adjusted_salary) -&gt; Iterable[str]:\n        if age &gt; 40:\n            yield \"Senior\"\n        else:\n            yield \"Junior\"\n        \n    ie_func3_dict = dict(ie_function=ie_func3,\n                     ie_function_name='ie_func3',\n                     in_rel=[DataTypes.integer],\n                     out_rel=[DataTypes.integer])\n\n    ie_func4_dict = dict(ie_function=ie_func4,\n                        ie_function_name='ie_func4',\n                        in_rel=[DataTypes.integer, DataTypes.integer],\n                        out_rel=[DataTypes.string])\n\n    commands = \"\"\"\n               new job(str, int)\n               job(\"Alice\", 5000)\n               job(\"Bob\", 6000)\n               job(\"Carol\", 10000)\n\n               new person(str, int)\n               person(\"Alice\", 35)\n               person(\"Bob\", 30)\n               person(\"Carol\", 45)\n               \n               sample_rule2(P, S, G) &lt;- job(P, S), person(P, A), ie_func3(S) -&gt; (AS), ie_func4(A, AS) -&gt; (G)\n               \n               ?sample_rule2(P, S, G)\n               \"\"\"\n    \n    expected_result = {\n        'P': ['Alice', 'Bob', 'Carol'],\n        'S': [5000, 6000, 10000],\n        'G': ['Junior', 'Junior', 'Senior']\n    }\n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [ie_func3_dict, ie_func4_dict])\n\ntest_complicated_analysis2()\n\n\ndef test_complicated_analysis3() -&gt; None:\n    def ie_func1(age) -&gt; Iterable[int]:\n        yield age + 1\n\n    def ie_func2(course, increased_age) -&gt; Iterable[str]:\n        if increased_age &gt; 26:\n            yield f\"Senior-{course}\"\n        else:\n            yield f\"Junior-{course}\"\n\n    ie_func1_dict = dict(ie_function=ie_func1,\n                     ie_function_name='ie_func1',\n                     in_rel=[DataTypes.integer],\n                     out_rel=[DataTypes.integer])\n\n    ie_func2_dict = dict(ie_function=ie_func2,\n                        ie_function_name='ie_func2',\n                        in_rel=[DataTypes.string, DataTypes.integer],\n                        out_rel=[DataTypes.string])\n\n    commands = \"\"\"\n               new student(str, int)\n               student(\"Alice\", 25)\n               student(\"Bob\", 22)\n               student(\"Charlie\", 28)\n               \n               new course(str, str)\n               course(\"CS101\", \"Computer Science\")\n               course(\"EN101\", \"English\")\n               course(\"MA101\", \"Mathematics\")\n               \n               new enrolled(str, str)\n               enrolled(\"Alice\", \"CS101\")\n               enrolled(\"Alice\", \"EN101\")\n               enrolled(\"Bob\", \"MA101\")\n               enrolled(\"Charlie\", \"EN101\")\n               \n               sample_rule3(S, A, G) &lt;- student(S, A), enrolled(S, C), course(C, T), ie_func1(A) -&gt; (IA), ie_func2(T, IA) -&gt; (G)\n                                        \n               ?sample_rule3(S, A, G)\n               \"\"\"\n\n    expected_result = {\n        'S': ['Alice', 'Alice', 'Bob', 'Charlie'],\n        'A': [25, 25, 22, 28],\n        'G': ['Junior-Computer Science', 'Junior-English', 'Junior-Mathematics', 'Senior-English']\n    }\n\n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [ie_func1_dict, ie_func2_dict])\n\ntest_complicated_analysis3()\n\n\ndef test_ie_func_complex_analysis1() -&gt; None:\n    from typing import Iterable\n    import pandas as pd\n    \n    def ie_func1(x) -&gt; Iterable[int]:\n        yield x * 2\n\n    def ie_func2(y, z) -&gt; Iterable[int]:\n        yield y + z\n\n    ie_func1_dict = dict(ie_function=ie_func1,\n                         ie_function_name='ie_func1',\n                         in_rel=[DataTypes.integer],\n                         out_rel=[DataTypes.integer])\n\n    ie_func2_dict = dict(ie_function=ie_func2,\n                         ie_function_name='ie_func2',\n                         in_rel=[DataTypes.integer, DataTypes.integer],\n                         out_rel=[DataTypes.integer])\n\n    commands = \"\"\"\n               new rel1(int)\n               new rel2(int)\n               new rel3(int)\n               rel1(1)\n               rel2(1)\n               rel3(2)\n\n               sample_rule1(X, Y, Z) &lt;- rel1(Y), rel2(Z), ie_func1(Z) -&gt; (Y1), rel3(Y1), ie_func2(Y, Y1) -&gt; (X)\n\n               ?sample_rule1(X, Y, Z)\n               \"\"\"\n\n    expected_result = {\n        'X': [3],\n        'Y': [1],\n        'Z': [1]\n    }\n\n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [ie_func1_dict, ie_func2_dict])\n\ntest_ie_func_complex_analysis1()\n\n\ndef test_extreme_analysis1() -&gt; None:\n    def ie_func1(x) -&gt; Iterable[int]:\n        yield x * 2\n\n    def ie_func2(x, y) -&gt; Iterable[int]:\n        yield x + y\n\n    def ie_func3(x) -&gt; Iterable[int]:\n        yield x - 1\n\n    def ie_func4(x, y) -&gt; Iterable[int]:\n        yield x * y\n\n    ie_func1_dict = dict(ie_function=ie_func1,\n                         ie_function_name='ie_func1',\n                         in_rel=[DataTypes.integer],\n                         out_rel=[DataTypes.integer])\n\n    ie_func2_dict = dict(ie_function=ie_func2,\n                         ie_function_name='ie_func2',\n                         in_rel=[DataTypes.integer, DataTypes.integer],\n                         out_rel=[DataTypes.integer])\n\n    ie_func3_dict = dict(ie_function=ie_func3,\n                         ie_function_name='ie_func3',\n                         in_rel=[DataTypes.integer],\n                         out_rel=[DataTypes.integer])\n\n    ie_func4_dict = dict(ie_function=ie_func4,\n                         ie_function_name='ie_func4',\n                         in_rel=[DataTypes.integer, DataTypes.integer],\n                         out_rel=[DataTypes.integer])\n\n    commands = \"\"\"\n               new rel1(int)\n               new rel2(int)\n               new rel3(int)\n               new rel4(int)\n               rel1(1)\n               rel1(2)\n               rel2(1)\n               rel2(2)\n               rel3(2)\n               rel4(3)\n\n               sample_rule1(X, Y1, Y2, Z1, Z2, W) &lt;- rel1(Y1), rel2(Z1), ie_func1(Z1) -&gt; (Y2), rel3(Y2), ie_func2(Y1, Y2) -&gt; (Z2),rel4(Z2),ie_func3(Z2) -&gt; (W),ie_func4(Y1, W) -&gt; (X)\n\n               ?sample_rule1(X, Y1, Y2, Z1, Z2, W)\n               \"\"\"\n\n    expected_result = {\n        'X': [2],  \n        'Y1': [1],\n        'Y2': [2],\n        'Z1': [1],\n        'Z2': [3],\n        'W': [2]\n    }\n\n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, \n                                  [ie_func1_dict, ie_func2_dict, ie_func3_dict, ie_func4_dict])\n    \ntest_extreme_analysis1()\n\n\ndef test_school_complexity1() -&gt; None:\n    from typing import Iterable\n    import pandas as pd\n    \n    def calculate_average(math, science) -&gt; Iterable[int]:\n        yield (math + science) // 2\n    \n    def grade_category(avg) -&gt; Iterable[str]:\n        if avg &gt;= 90:\n            yield \"A\"\n        elif avg &gt;= 75:\n            yield \"B\"\n        else:\n            yield \"C\"\n            \n    def status(math, science, grade) -&gt; Iterable[str]:\n        if math &gt;= 90 and science &gt;= 90 and grade == \"A\":\n            yield \"Excellent\"\n        else:\n            yield \"Good\"\n    \n    calculate_average_dict = dict(ie_function=calculate_average, ie_function_name='calculate_average', in_rel=[DataTypes.integer, DataTypes.integer], out_rel=[DataTypes.integer])\n    grade_category_dict = dict(ie_function=grade_category, ie_function_name='grade_category', in_rel=[DataTypes.integer], out_rel=[DataTypes.string])\n    status_dict = dict(ie_function=status, ie_function_name='status', in_rel=[DataTypes.integer, DataTypes.integer, DataTypes.string], out_rel=[DataTypes.string])\n    \n    commands = \"\"\"\n               new student(str, int, int)\n               student(\"Alice\", 85, 92)\n               student(\"Bob\", 76, 88)\n               student(\"Carol\", 95, 91)\n               \n               average_grade(S, M, SC, AVG) &lt;- student(S, M, SC), calculate_average(M, SC) -&gt; (AVG)\n               \n               grade_status(S, AVG, GRADE, STATUS) &lt;- average_grade(S, M, SC, AVG), grade_category(AVG) -&gt; (GRADE), status(M, SC, GRADE) -&gt; (STATUS)\n               \n               ?grade_status(S, AVG, GRADE, STATUS)\n               \"\"\"\n    expected_result = {\n        'S': ['Alice', 'Bob', 'Carol'],\n        'AVG': [88, 82, 93],\n        'GRADE': ['B', 'B', 'A'],\n        'STATUS': ['Good', 'Good', 'Excellent']\n    }\n    \n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [calculate_average_dict, grade_category_dict, status_dict])\n\ntest_school_complexity1()\n\n\ndef test_school_complexity2() -&gt; None:\n    from typing import Iterable\n    import pandas as pd\n    \n    def club_member_count(members) -&gt; Iterable[int]:\n        yield len(members.split(','))\n    \n    def club_status(member_count, funding) -&gt; Iterable[str]:\n        if member_count &gt;= 10 and funding &gt;= 500:\n            yield \"Active\"\n        else:\n            yield \"Inactive\"\n    \n    club_member_count_dict = dict(ie_function=club_member_count, ie_function_name='club_member_count', in_rel=[DataTypes.string], out_rel=[DataTypes.integer])\n    club_status_dict = dict(ie_function=club_status, ie_function_name='club_status', in_rel=[DataTypes.integer, DataTypes.integer], out_rel=[DataTypes.string])\n    \n    commands = \"\"\"\n               new club(str, str, int)\n               club(\"Science\", \"Alice,Bob,Carol,Dave\", 600)\n               club(\"Art\", \"Eve,Frank\", 200)\n               \n               club_info(C, MC, STATUS) &lt;- club(C, M, F), club_member_count(M) -&gt; (MC), club_status(MC, F) -&gt; (STATUS)\n               \n               ?club_info(C, MC, STATUS)\n               \"\"\"\n    \n    expected_result = {\n        'C': ['Science', 'Art'],\n        'MC': [4, 2],\n        'STATUS': ['Inactive', 'Inactive']\n    }\n    \n    expected_result_df = pd.DataFrame(expected_result)\n    session = run_test_df_compare(commands, expected_result_df, [club_member_count_dict, club_status_dict])\n\ntest_school_complexity2()\n\n\ndef test_school_complexity3() -&gt; None:\n    from typing import Iterable\n    import pandas as pd\n    \n    def calculate_fine(days) -&gt; Iterable[int]:\n        yield days * 2\n    \n    def book_status(fine, condition) -&gt; Iterable[str]:\n        if fine &gt; 0 and condition == \"Damaged\":\n            yield \"Pay Fine and Repair\"\n        elif fine &gt; 0:\n            yield \"Pay Fine\"\n        elif condition == \"Damaged\":\n            yield \"Repair\"\n        else:\n            yield \"OK\"\n    \n    calculate_fine_dict = dict(ie_function=calculate_fine, ie_function_name='calculate_fine', in_rel=[DataTypes.integer], out_rel=[DataTypes.integer])\n    book_status_dict = dict(ie_function=book_status, ie_function_name='book_status', in_rel=[DataTypes.integer, DataTypes.string], out_rel=[DataTypes.string])\n    \n    commands = \"\"\"\n               new book(str, int, str)\n               book(\"Math101\", 5, \"Good\")\n               book(\"Science101\", 3, \"Damaged\")\n               book(\"Art101\", 0, \"Good\")\n               \n               book_info(B, F, STATUS) &lt;- book(B, D, C), calculate_fine(D) -&gt; (F), book_status(F, C) -&gt; (STATUS)\n               \n               ?book_info(B, F, STATUS)\n               \"\"\"\n    \n    expected_result = {\n        'B': ['Math101', 'Science101', 'Art101'],\n        'F': [10,6,0],\n        'STATUS': [\"Pay Fine\",\"Pay Fine and Repair\", \"OK\"]\n    }\n\n    expected_result_df = pd.DataFrame(expected_result)\n\n    session = run_test_df_compare(commands, expected_result_df, [calculate_fine_dict, book_status_dict])\n\ntest_school_complexity3()"
  },
  {
    "objectID": "tests/05p_test_corenlp.html",
    "href": "tests/05p_test_corenlp.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import pytest\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\n\n\n@pytest.mark.long\ndef test_quote() -&gt; None:\n    quoted_phrase = r'''\\\"I'm going to Hawaii.\\\"'''\n    commands = f\"\"\"sentence = \"In the summer Joe Smith decided to go on vacation.  He said, {quoted_phrase}. That July, vacationer Joe went to Hawaii.\"\n               cool_quote(A,S,D,F,G,H,J,K,L,P) &lt;- Quote(sentence) -&gt; (A,S,D,F,G,H,J,K,L,P)\n               ?cool_quote(A,S,D,F,G,H,J,K,L,P)\"\"\"\n\n    expected_result = (fr\"\"\"{QUERY_RESULT_PREFIX}'cool_quote(A, S, D, F, G, H, J, K, L, P)':\n                       A |           S           |   D |   F |   G |   H |   J |   K |     L     |     P\n                    -----+-----------------------+-----+-----+-----+-----+-----+-----+-----------+-----------\n                       0 | I'm going to Hawaii.\\ |  62 |  85 |  15 |  23 |   1 |   2 | Joe Smith | Joe Smith\n                       \"\"\")\n\n    run_test(commands, expected_result)\n\n# test_quote()\n\n\ndef test_tokenize() -&gt; None:\n    commands = \"\"\"\n                    sentence = \"Hello world. Hello world again.\"\n                    tokens(X, Y) &lt;- Tokenize(sentence) -&gt; (X, Y)\n                    ?tokens(Token, Span)\n                \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'tokens(Token, Span)':\n  Token  |   Span\n---------+----------\n    .    | [30, 31)\n  again  | [25, 30)\n  world  | [19, 24)\n  Hello  | [13, 18)\n    .    | [11, 12)\n  world  | [6, 11)\n  Hello  |  [0, 5)\n\"\"\"\n\n    run_test(commands, expected_result)\n\n# test_tokenize()\n\n\ndef test_ssplit() -&gt; None:\n    commands = \"\"\"\n                sentence = \"Hello world. Hello world again.\"\n                sentences(X) &lt;- SSplit(sentence) -&gt; (X)\n                ?sentences(Sentences)\n                \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'sentences(Sentences)':\n                          Sentences\n                    ---------------------\n                     Hello world again .\n                        Hello world .\n                    \"\"\"\n\n    run_test(commands, expected_result)\n\n# test_ssplit()\n\n\ndef test_pos() -&gt; None:\n    commands = \"\"\"\n                sentence = \"Marie was born in Paris.\"\n                pos(X, Y, Z) &lt;- POS(sentence) -&gt; (X, Y, Z)\n                ?pos(Token, POS, Span)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'pos(Token, POS, Span)':\n                      Token  |  POS  |   Span\n                    ---------+-------+----------\n                        .    |   .   | [23, 24)\n                      Paris  |  NNP  | [18, 23)\n                       in    |  IN   | [15, 17)\n                      born   |  VBN  | [10, 14)\n                       was   |  VBD  |  [6, 9)\n                      Marie  |  NNP  |  [0, 5)\n                    \"\"\"\n\n    run_test(commands, expected_result)\n\n# test_pos()\n\n\ndef test_lemma() -&gt; None:\n    commands = \"\"\"\n                sentence = \"I've been living a lie, there's nothing inside.\"\n                lemma(X, Y, Z) &lt;- Lemma(sentence) -&gt; (X, Y, Z)\n                ?lemma(Token, Lemma, Span)\n                 \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'lemma(Token, Lemma, Span)':\n                      Token  |  Lemma  |   Span\n                    ---------+---------+----------\n                        .    |    .    | [46, 47)\n                     inside  | inside  | [40, 46)\n                     nothing | nothing | [32, 39)\n                       's    |   be    | [29, 31)\n                      there  |  there  | [24, 29)\n                        ,    |    ,    | [22, 23)\n                       lie   |   lie   | [19, 22)\n                        a    |    a    | [17, 18)\n                     living  |  live   | [10, 16)\n                      been   |   be    |  [5, 9)\n                       've   |  have   |  [1, 4)\n                        I    |    I    |  [0, 1)\n                    \"\"\"\n\n    run_test(commands, expected_result)\n\n# test_lemma()\n\n\n@pytest.mark.long\ndef test_ner() -&gt; None:\n    commands = (\"\"\"sentence = \"While in France, Christine Lagarde discussed short-term stimulus \"\"\"\n                \"\"\"efforts in a recent interview with the Wall Street Journal.\"\n               ner(X, Y, Z) &lt;- NER(sentence) -&gt; (X, Y, Z)\n               ?ner(Token, NER, Span)\"\"\")\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'ner(Token, NER, Span)':\n                       Token   |     NER      |    Span\n                    -----------+--------------+------------\n                      Journal  | ORGANIZATION | [116, 123)\n                      Street   | ORGANIZATION | [109, 115)\n                       Wall    | ORGANIZATION | [104, 108)\n                      Lagarde  |    PERSON    |  [27, 34)\n                     Christine |    PERSON    |  [17, 26)\n                      France   |   COUNTRY    |  [9, 15)\n                    \"\"\"\n\n    run_test(commands, expected_result)\n\n# test_ner()\n\n\n@pytest.mark.long\ndef test_entity_mentions() -&gt; None:\n    commands = \"\"\"sentence = \"New York Times newspaper is distributed in California.\"\n            em(X, Y, Z, W, A, B, C, D, E) &lt;- EntityMentions(sentence) -&gt; (X, Y, Z, W, A, B, C, D, E)\n            ?em(DocTokenBegin, DocTokenEnd, TokenBegin, TokenEnd, Text, \\\n            CharacterOffsetBegin, CharacterOffsetEnd, Ner, NerConfidences) \"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'em(DocTokenBegin, DocTokenEnd, TokenBegin, TokenEnd, Text,\"\"\"\n                       \"\"\" CharacterOffsetBegin, CharacterOffsetEnd, Ner, NerConfidences)':\n                       DocTokenBegin |   DocTokenEnd |   TokenBegin |   TokenEnd |      Text      |   \"\"\"\n                       \"\"\"CharacterOffsetBegin |   CharacterOffsetEnd |        Ner        |           NerConfidences\n                    -----------------+---------------+--------------+------------+----------------+-----------\"\"\"\n                       \"\"\"-------------+----------------------+-------------------+------------------------------------\n                                   7 |             8 |            7 |          8 |   California   |             \"\"\"\n                       \"\"\"        43 |                   53 | STATE_OR_PROVINCE |   {'LOCATION': 0.99823619336706}\n                                   0 |             3 |            0 |          3 | New York Times |             \"\"\"\n                       \"\"\"         0 |                   14 |   ORGANIZATION    | {'ORGANIZATION': 0.98456891831803}\n                    \"\"\")\n\n    run_test(commands, expected_result)\n\n# test_entity_mentions()\n\n\ndef test_parse() -&gt; None:\n    commands = \"\"\"sentence = \"the quick brown fox jumps over the lazy dog\"\n           parse(X) &lt;- Parse(sentence) -&gt; (X)\n           ?parse(X)\"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'parse(X)':\n                                                                                                    X\n                    ----------------------------------------------------------------------------------------\"\"\"\n                       \"\"\"-------------------------------------------------------------------------\n                     (ROOT&lt;nl&gt;  (S&lt;nl&gt;    (NP (DT the) (JJ quick) (JJ brown) (NN fox))&lt;nl&gt;    (VP (VBZ jumps)&lt;nl&gt;\"\"\"\n                       \"\"\"      (PP (IN over)&lt;nl&gt;        (NP (DT the) (JJ lazy) (NN dog))))))\"\"\")\n\n    run_test(commands, expected_result)\n\n# test_parse()\n\n\n@pytest.mark.long\ndef test_depparse() -&gt; None:\n    commands = \"\"\"sentence = \"the quick brown fox jumps over the lazy dog\"\n                depparse(X, Y, Z, W, U) &lt;- DepParse(sentence) -&gt; (X, Y, Z, W, U)\n                ?depparse(Dep, Governor, GovernorGloss, Dependent, DependentGloss)\"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'depparse(Dep, Governor, GovernorGloss, Dependent, DependentGloss)':\n                          Dep  |   Governor |  GovernorGloss  |   Dependent |  DependentGloss\n                        -------+------------+-----------------+-------------+------------------\n                          obl  |          5 |      jumps      |           9 |       dog\n                         amod  |          9 |       dog       |           8 |       lazy\n                          det  |          9 |       dog       |           7 |       the\n                         case  |          9 |       dog       |           6 |       over\n                         nsubj |          5 |      jumps      |           4 |       fox\n                         amod  |          4 |       fox       |           3 |      brown\n                         amod  |          4 |       fox       |           2 |      quick\n                          det  |          4 |       fox       |           1 |       the\n                         ROOT  |          0 |      ROOT       |           5 |      jumps\n                         \"\"\"\n\n    run_test(commands, expected_result)\n\n# test_depparse()\n\n\n@pytest.mark.long\ndef test_coref() -&gt; None:\n    commands = \"\"\"sentence = \"The atom is a basic unit of matter, \\\n                    it consists of a dense central nucleus surrounded by a cloud of negatively charged electrons.\"\n            coref(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12) &lt;- \\\n            Coref(sentence) -&gt; (X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12)\n            ?coref(Id, Text, Type, Number, Gender, Animacy, StartIndex, \\\n            EndIndex, HeadIndex, SentNum, Position, IsRepresentativeMention)\"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'coref(Id, Text, Type, Number, Gender, Animacy, StartIndex,\"\"\"\n                       \"\"\" EndIndex, HeadIndex, SentNum, Position, IsRepresentativeMention)':\n                           Id |   Text   |    Type    |  Number  |  Gender  |  Animacy  |   StartIndex |   EndIndex\"\"\"\n                       \"\"\" |   HeadIndex |   SentNum |  Position  |  IsRepresentativeMention\n                        ------+----------+------------+----------+----------+-----------+--------------+------------\"\"\"\n                       \"\"\"+-------------+-----------+------------+---------------------------\n                            3 |    it    | PRONOMINAL | SINGULAR | NEUTRAL  | INANIMATE |           10 |         11\"\"\"\n                       \"\"\" |          10 |         1 |   [1, 4)   |           False\n                            0 | The atom |  NOMINAL   | SINGULAR | NEUTRAL  | INANIMATE |            1 |          3\"\"\"\n                       \"\"\" |           2 |         1 |   [1, 1)   |           True\"\"\")\n\n    run_test(commands, expected_result)\n\n# test_coref()\n\n\n@pytest.mark.long\ndef test_openie() -&gt; None:\n    commands = \"\"\"sentence = \"the quick brown fox jumps over the lazy dog\"\n               openie(X1, X2, X3, X4, X5, X6) &lt;- OpenIE(sentence) -&gt; (X1, X2, X3, X4, X5, X6)\n               ?openie(Subject, SubjectSpan, Relation, RelationSpan, Object, ObjectSpan)\"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'openie(Subject, SubjectSpan, Relation, RelationSpan,\"\"\"\n                       \"\"\" Object, ObjectSpan)':\n                             Subject     |  SubjectSpan  |  Relation  |  RelationSpan  |  Object  |  ObjectSpan\n                        -----------------+---------------+------------+----------------+----------+--------------\n                            brown fox    |    [2, 4)     | jumps over |     [4, 6)     | lazy dog |    [7, 9)\n                               fox       |    [3, 4)     | jumps over |     [4, 6)     | lazy dog |    [7, 9)\n                            quick fox    |    [1, 4)     | jumps over |     [4, 6)     | lazy dog |    [7, 9)\n                            quick fox    |    [1, 4)     | jumps over |     [4, 6)     |   dog    |    [8, 9)\n                         quick brown fox |    [1, 4)     | jumps over |     [4, 6)     |   dog    |    [8, 9)\n                            brown fox    |    [2, 4)     | jumps over |     [4, 6)     |   dog    |    [8, 9)\n                               fox       |    [3, 4)     | jumps over |     [4, 6)     |   dog    |    [8, 9)\n                         quick brown fox |    [1, 4)     | jumps over |     [4, 6)     | lazy dog |    [7, 9)\n                        \"\"\")\n\n    run_test(commands, expected_result)\n\n# test_openie()\n\n\n# note: this test uses 3+ GB of RAM\n@pytest.mark.long\ndef test_kbp() -&gt; None:\n    commands = \"\"\"sentence = \"Joe Smith was born in Oregon.\"\n              kbp(X1, X2, X3, X4, X5, X6) &lt;- KBP(sentence) -&gt; (X1, X2, X3, X4, X5, X6)\n              ?kbp(Subject, SubjectSpan, Relation, RelationSpan, Object, ObjectSpan)\"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'kbp(Subject, SubjectSpan, Relation, RelationSpan, Object,\"\"\"\n                       \"\"\" ObjectSpan)':\n                          Subject  |  SubjectSpan  |           Relation           |  RelationSpan  |  Object  |\"\"\"\n                       \"\"\"  ObjectSpan\n                        -----------+---------------+------------------------------+----------------+----------+---\"\"\"\n                       \"\"\"-----------\n                         Joe Smith |    [0, 2)     | per:stateorprovince_of_birth |    [-2, -1)    |  Oregon  |    \"\"\"\n                       \"\"\"[5, 6)\"\"\")\n\n    # run_test(commands, expected_result)\n\n\n\ndef test_sentiment() -&gt; None:\n    commands = \"\"\"sentence = \"But I do not want to go among mad people, Alice remarked.\\\n                Oh, you can not help that, said the Cat: we are all mad here. I am mad. You are mad.\\\n                How do you know I am mad? said Alice.\\\n                You must be, said the Cat, or you would not have come here. This is awful, bad, disgusting\"\n               sentiment(X, Y, Z) &lt;- Sentiment(sentence) -&gt; (X, Y, Z)\n               ?sentiment(SentimentValue, Sentiment, SentimentDistribution)\"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'sentiment(SentimentValue, Sentiment, SentimentDistribution)':\n                           SentimentValue |  Sentiment  |                                   SentimentDistribution\n                        ------------------+-------------+-------------------------------------------------------\"\"\"\n                       \"\"\"-------------------------------------\n                                        1 |  Negative   | [0.38530939547592, 0.40530204971517, 0.15108253421994, \"\"\"\n                       \"\"\"0.0344418112832, 0.02386420930578]\n                                        1 |  Negative   | [0.12830923590495, 0.37878858881094, 0.30518256399302,\"\"\"\n                       \"\"\" 0.1718067054989, 0.01591290579219]\n                                        2 |   Neutral   |  [4.32842857e-06, 0.00178165446312, 0.99514483788581,\"\"\"\n                       \"\"\" 0.00300054886868, 6.863035382e-05]\n                                        2 |   Neutral   | [0.05362880533407, 0.34651236390176, 0.49340993668151,\"\"\"\n                       \"\"\" 0.10427916689283, 0.00216972718983]\n                                        2 |   Neutral   | [0.02439193942712, 0.30967820316829, 0.58893236904834,\"\"\"\n                       \"\"\" 0.0763362330424, 0.00066125531385]\n                                        2 |   Neutral   | [0.01782223769627, 0.29955831565507, 0.61735992863662,\"\"\"\n                       \"\"\" 0.06487534397678, 0.00038417403527]\n                                        1 |  Negative   | [0.12830922491145, 0.37878858297753, 0.30518256852813,\"\"\"\n                       \"\"\" 0.17180671895586, 0.01591290462702]\n                                        1 |  Negative   | [0.10061981563996, 0.47061477615492, 0.34369414180068,\"\"\"\n                       \"\"\" 0.0811364260425, 0.00393484036195]\n                        \"\"\")\n\n    run_test(commands, expected_result)\n\n# test_sentiment()\n\n\n@pytest.mark.long\ndef test_truecase() -&gt; None:\n    commands = \"\"\"sentence = \"lonzo ball talked about kobe bryant after the lakers game.\"\n              truecase(X, Y, Z, W) &lt;- TrueCase(sentence) -&gt; (X, Y, Z, W)\n              ?truecase(Token, Span, Truecase, TruecaseText)\"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'truecase(Token, Span, Truecase, TruecaseText)':\n                          Token  |   Span   |  Truecase  |  TruecaseText\n                        ---------+----------+------------+----------------\n                            .    | [57, 58) |     O      |       .\n                          game   | [53, 57) |   LOWER    |      game\n                         lakers  | [46, 52) | INIT_UPPER |     Lakers\n                           the   | [42, 45) |   LOWER    |      the\n                          after  | [36, 41) |   LOWER    |     after\n                         bryant  | [29, 35) | INIT_UPPER |     Bryant\n                          kobe   | [24, 28) | INIT_UPPER |      Kobe\n                          about  | [18, 23) |   LOWER    |     about\n                         talked  | [11, 17) |   LOWER    |     talked\n                          ball   | [6, 10)  | INIT_UPPER |      Ball\n                          lonzo  |  [0, 5)  | INIT_UPPER |     Lonzo\n                        \"\"\"\n\n    run_test(commands, expected_result)\n\n# test_truecase()\n\n\ndef test_clean_xml() -&gt; None:\n    commands = \"\"\"sentence = \"&lt;xml&gt;&lt;to&gt;Tove&lt;/to&gt;\\\n       &lt;from&gt;Jani&lt;/Ffrom&gt;\\\n       &lt;heading&gt;Reminder&lt;/heading&gt;\\\n       &lt;body&gt;Don't forget me this weekend!&lt;/body&gt;&lt;/xml&gt;\"\n           clean_xml(X, Y, Z, W, U) &lt;- CleanXML(sentence) -&gt; (X, Y, Z, W, U)\n           ?clean_xml(Index, Word, OriginalText, CharacterOffsetBegin, CharacterOffsetEnd)\"\"\"\n\n    expected_result = (f\"\"\"{QUERY_RESULT_PREFIX}'clean_xml(Index, Word, OriginalText, CharacterOffsetBegin,\"\"\"\n                       \"\"\" CharacterOffsetEnd)':\n                           Index |   Word   |  OriginalText  |   CharacterOffsetBegin |   CharacterOffsetEnd\n                        ---------+----------+----------------+------------------------+----------------------\n                              -1 |    !     |       !        |                    118 |                  119\n                              -1 | weekend  |    weekend     |                    111 |                  118\n                              -1 |   this   |      this      |                    106 |                  110\n                              -1 |    me    |       me       |                    103 |                  105\n                              -1 |  forget  |     forget     |                     96 |                  102\n                              -1 |   n't    |      n't       |                     92 |                   95\n                              -1 |    Do    |       Do       |                     90 |                   92\n                              -1 | Reminder |    Reminder    |                     59 |                   67\n                              -1 |   Jani   |      Jani      |                     31 |                   35\n                              -1 |   Tove   |      Tove      |                      9 |                   13\n                        \"\"\")\n\n    run_test(commands, expected_result)\n# test_clean_xml()"
  },
  {
    "objectID": "tests/05n_test_optimizations.html",
    "href": "tests/05n_test_optimizations.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from spannerlib.optimizations_passes import PruneUnnecessaryProjectNodes, RemoveUselessRelationsFromRule\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test, get_session_with_optimizations\n\n\n\ndef test_prune_project_nodes() -&gt; None:\n    commands = \"\"\"\n               new B(int)\n               new C(int)\n               B(1)\n               B(2)\n               B(4)\n               C(0)\n\n               A(X) &lt;- B(X), C(0)\n               ?A(X)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n       X\n    -----\n       1\n       2\n       4\n    \"\"\"\n\n    session = get_session_with_optimizations(term_graph_optimization_passes=(PruneUnnecessaryProjectNodes,))\n    run_test(commands, expected_result, session=session)\n\ntest_prune_project_nodes()\n\nprinting results for query 'A(X)':\n   X\n-----\n   1\n   2\n   4\n\n\n\n\ndef test_prune_project_nodes() -&gt; None:\n    commands = \"\"\"\n               new B(int)\n               new C(int)\n               B(1)\n               B(2)\n               B(4)\n               C(0)\n\n               A(X) &lt;- B(X), C(0)\n               ?A(X)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n       X\n    -----\n       1\n       2\n       4\n    \"\"\"\n\n    session = get_session_with_optimizations(term_graph_optimization_passes=(PruneUnnecessaryProjectNodes,))\n    run_test(commands, expected_result, session=session)\n\ntest_prune_project_nodes()\n\nprinting results for query 'A(X)':\n   X\n-----\n   1\n   2\n   4"
  },
  {
    "objectID": "tests/05b_test_basic_cases.html",
    "href": "tests/05b_test_basic_cases.html",
    "title": "Spannerlib",
    "section": "",
    "text": "from spannerlib.session import Session\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\nInstallation NLP failed\n\n\n\n\ndef test_assignment() -&gt; None:\n    commands = \"\"\"\n            new Relation(int, int)\n            x = 1\n            y = 2\n            z = y\n            Relation(x, y)\n            Relation(y, x)\n            ?Relation(X, x)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'Relation(X, 1)':\n       X\n    -----\n       2\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_assignment()\n\nprinting results for query 'Relation(X, 1)':\n   X\n-----\n   2\n\n\n\n\ndef test_copy_table_rule() -&gt; None:\n    commands = \"\"\"\n            new B(int, int)\n            B(1, 1)\n            B(1, 2)\n            B(2, 3)\n            A(X, Y) &lt;- B(X, Y)\n            ?A(X, Y)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, Y)':\n       X |   Y\n    -----+-----\n       1 |   1\n       1 |   2\n       2 |   3\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_copy_table_rule()\n\nprinting results for query 'A(X, Y)':\n   X |   Y\n-----+-----\n   1 |   1\n   1 |   2\n   2 |   3\n\n\n\n\ndef test_join_two_tables() -&gt; None:\n    commands = \"\"\"\n        new B(int, int)\n        new C(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        C(2, 2)\n        C(1, 1)\n        D(X, Y, Z) &lt;- B(X, Y), C(Y, Z)\n        ?D(X, Y, Z)\n    \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'D(X, Y, Z)':\n       X |   Y |   Z\n    -----+-----+-----\n       1 |   2 |   2\n       1 |   1 |   1\n    \"\"\"\n\n    run_test(commands, expected_result)\ntest_join_two_tables()\n\nprinting results for query 'D(X, Y, Z)':\n   X |   Y |   Z\n-----+-----+-----\n   1 |   2 |   2\n   1 |   1 |   1\n\n\n\n\ndef test_relation_with_same_free_var() -&gt; None:\n    commands = \"\"\"\n        new B(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 2)\n        A(X) &lt;- B(X, X)\n        ?A(X)\n    \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n       X\n    -----\n       1\n       2\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_relation_with_same_free_var()\n\nprinting results for query 'A(X)':\n   X\n-----\n   1\n   2\n\n\n\n\ndef test_union_rule_with_same_vars() -&gt; None:\n    commands = \"\"\"\n        new B(int, int)\n        new C(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        C(2, 2)\n        C(1, 1)\n\n        A(X, Y) &lt;- B(X, Y)\n        A(X, Y) &lt;- C(X, Y)\n        ?A(X, Y)\n    \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, Y)':\n       X |   Y\n    -----+-----\n       1 |   1\n       1 |   2\n       2 |   2\n       2 |   3\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_union_rule_with_same_vars()\n\nprinting results for query 'A(X, Y)':\n   X |   Y\n-----+-----\n   1 |   1\n   1 |   2\n   2 |   2\n   2 |   3\n\n\n\n\ndef test_union_rule_with_different_vars() -&gt; None:\n    commands = \"\"\"\n        new B(int, int)\n        new C(int, int)\n        B(1, 1)\n        B(1, 2)\n        B(2, 3)\n        C(2, 2)\n        C(1, 1)\n\n        A(X, Y) &lt;- B(X, Y)\n        A(Z, W) &lt;- C(Z, W)\n        ?A(X, Y)\n    \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, Y)':\n       X |   Y\n    -----+-----\n       1 |   1\n       1 |   2\n       2 |   2\n       2 |   3\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_union_rule_with_different_vars()\n\nprinting results for query 'A(X, Y)':\n   X |   Y\n-----+-----\n   1 |   1\n   1 |   2\n   2 |   2\n   2 |   3\n\n\n\n\ndef test_project() -&gt; None:\n    commands = \"\"\"\n            new B(int, int)\n            B(1, 1)\n            B(1, 2)\n\n            A(X) &lt;- B(X, Y)\n            ?A(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n       X\n    -----\n       1\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_project()\n\nprinting results for query 'A(X)':\n   X\n-----\n   1\n\n\n\n\ndef test_add_fact_after_rule() -&gt; None:\n    commands = \"\"\"\n            new B(int, int)\n            B(1, 1)\n            A(X, Y) &lt;- B(X, Y)\n            B(1, 2)\n            ?A(Z, W)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(Z, W)':\n       Z |   W\n    -----+-----\n       1 |   1\n       1 |   2\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_add_fact_after_rule()\n\nprinting results for query 'A(Z, W)':\n   Z |   W\n-----+-----\n   1 |   1\n   1 |   2\n\n\n\n\ndef test_datatypes() -&gt; None:\n    commands = \"\"\"\n            new B(int, str, span)\n            B(1, \"2\", [1, 2))\n            ?B(X, Y, Z)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'B(X, Y, Z)':\n       X |   Y |   Z\n    -----+-----+--------\n       1 |   2 | [1, 2)\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_datatypes()\n\nprinting results for query 'B(X, Y, Z)':\n   X |   Y |   Z\n-----+-----+--------\n   1 |   2 | [1, 2)\n\n\n\n\ndef test_join_same_relation() -&gt; None:\n    commands = \"\"\"\n            new Parent(str, str)\n            Parent(\"Sam\", \"Noah\")\n            Parent(\"Noah\", \"Austin\")\n            Parent(\"Austin\", \"Stephen\")\n\n\n            GrandParent(G, C) &lt;- Parent(G, M), Parent(M, C)\n            ?GrandParent(X, \"Austin\")\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'GrandParent(X, \"Austin\")':\n      X\n    -----\n     Sam\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_join_same_relation()\n\nprinting results for query 'GrandParent(X, \"Austin\")':\n  X\n-----\n Sam\n\n\n\n\ndef test_rule_with_constant() -&gt; None:\n    commands = \"\"\"\n              new B(int, int)\n              new C(int, int)\n              B(1, 1)\n              B(1, 2)\n              B(2, 3)\n              C(2, 2)\n              C(1, 1)\n\n              A(X) &lt;- B(1, X)\n              ?A(X)\n           \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n       X\n    -----\n       1\n       2\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_rule_with_constant()\n\nprinting results for query 'A(X)':\n   X\n-----\n   1\n   2\n\n\n\n\ndef test_rule_with_true_value() -&gt; None:\n    commands = \"\"\"\n               new B(int, int)\n               new C(int, int)\n               B(1, 1)\n               B(1, 2)\n               B(2, 3)\n               C(2, 2)\n               C(1, 1)\n\n               A(X, Y) &lt;- B(X, Y), C(1, 1)\n               ?A(X, Y)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, Y)':\n       X |   Y\n    -----+-----\n       1 |   1\n       1 |   2\n       2 |   3\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_rule_with_true_value()\n\nprinting results for query 'A(X, Y)':\n   X |   Y\n-----+-----\n   1 |   1\n   1 |   2\n   2 |   3\n\n\n\n\ndef test_rule_with_false_value() -&gt; None:\n    commands = \"\"\"\n               new B(int, int)\n               new C(int, int)\n               B(1, 1)\n               B(1, 2)\n               B(2, 3)\n               C(2, 2)\n               C(1, 1)\n\n               A(X, Y) &lt;- B(X, Y), C(0, 0)\n               ?A(X, Y)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, Y)':\n    []\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_rule_with_false_value()\n\nprinting results for query 'A(X, Y)':\n[]\n\n\n\n\ndef test_query_with_same_var() -&gt; None:\n    commands = \"\"\"\n              new B(int, int)\n              B(1, 1)\n              B(1, 2)\n              B(2, 3)\n\n              A(X, Y) &lt;- B(X, Y)\n              ?A(X, X)\n           \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, X)':\n       X |   X\n    -----+-----\n       1 |   1\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_query_with_same_var()\n\nprinting results for query 'A(X, X)':\n   X |   X\n-----+-----\n   1 |   1\n\n\n\n\ndef test_query_with_constant_value() -&gt; None:\n    commands = \"\"\"\n               new B(int, int)\n               B(1, 1)\n               B(1, 2)\n               B(2, 3)\n\n               A(X, Y) &lt;- B(X, Y)\n               ?A(1, X)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(1, X)':\n       X\n    -----\n       1\n       2\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_query_with_constant_value()\n\nprinting results for query 'A(1, X)':\n   X\n-----\n   1\n   2\n\n\n\n\ndef test_remove_rule() -&gt; None:\n    commands = \"\"\"\n               new B(int, int)\n               new C(int, int)\n               B(1, 1)\n               B(1, 2)\n               B(2, 3)\n               C(2, 2)\n               C(1, 1)\n\n               A(X, Y) &lt;- B(X, Y)\n               A(X, Y) &lt;- C(X, Y)\n\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X, Y)':\n       X |   Y\n    -----+-----\n       1 |   1\n       1 |   2\n       2 |   3\n     \"\"\"\n\n    session = run_test(commands)\n\n    session.remove_rule(\"A(X, Y) &lt;- C(X, Y)\")\n\n    run_test(\"?A(X, Y)\", expected_result, session=session)\n\ntest_remove_rule()\n\nprinting results for query 'A(X, Y)':\n   X |   Y\n-----+-----\n   1 |   1\n   1 |   2\n   2 |   3\n\n\n\n\ndef test_select_and_join() -&gt; None:\n    commands = \"\"\"\n            new B(int)\n            new C(int, int)\n            B(2)\n            C(1, 4)\n            C(2, 5)\n            A(X) &lt;- B(X), C(X, 5)\n            ?A(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(X)':\n       X\n    -----\n       2\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_select_and_join()\n\nprinting results for query 'A(X)':\n   X\n-----\n   2\n\n\n\n\ndef test_query_true_value() -&gt; None:\n    commands = \"\"\"\n            new A(int)\n            A(1)\n            ?A(1)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(1)':\n    [()]\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_query_true_value()\n\nprinting results for query 'A(1)':\n[()]\n\n\n\n\ndef test_query_false_value() -&gt; None:\n    commands = \"\"\"\n            new A(int)\n            A(1)\n            ?A(2)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'A(2)':\n    []\n    \"\"\"\n\n    run_test(commands, expected_result)\n\ntest_query_false_value()\n\nprinting results for query 'A(2)':\n[]\n\n\n\n\ndef test_nothing() -&gt; None:\n    # we can't use run_test when there is no output\n    commands = \"\"\n\n    expected_result = \"[]\"\n\n    commands_result = str(Session().run_commands(commands, print_results=True))\n    assert expected_result == commands_result, \"expected string != result string\"\n\ntest_nothing()\n\n\ndef test_add_remove_fact() -&gt; None:\n    commands = \"\"\"\n                new rel(int)\n                rel(8) &lt;- True\n                rel(16)\n                rel(16) &lt;- False\n                rel(32)\n                rel(16)\n                rel(32) &lt;- False\n                ?rel(X)\n                \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'rel(X)':\n                           X\n                        -----\n                           8\n                          16\n                        \"\"\"\n\n    run_test(commands, expected_result)\ntest_add_remove_fact()\n\nprinting results for query 'rel(X)':\n   X\n-----\n   8\n  16"
  },
  {
    "objectID": "tests/05q_test_regex.html",
    "href": "tests/05q_test_regex.html",
    "title": "Spannerlib",
    "section": "",
    "text": "import tempfile\nfrom pathlib import Path\nfrom spannerlib.general_utils import QUERY_RESULT_PREFIX\nfrom spannerlib.tests.utils import run_test\n\n\n\ndef test_rust_regex() -&gt; None:\n    commands = \"\"\"\n        string_rel(X) &lt;- rgx_string(\"aa\",\".+\") -&gt; (X)\n        span_rel(X) &lt;- rgx_span(\"aa\",\".+\") -&gt; (X)\n        ?string_rel(X)\n        ?span_rel(X)\n        \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'string_rel(X)':\n          X\n        -----\n          a\n         aa\n\n        {QUERY_RESULT_PREFIX}'span_rel(X)':\n           X\n        --------\n         [0, 1)\n         [0, 2)\n         [1, 2)\n        \"\"\"\n\n    run_test(commands, expected_result)\n# test_rust_regex()\n\nprinting results for query 'string_rel(X)':\n  X\n-----\n  a\n aa\n\nprinting results for query 'span_rel(X)':\n   X\n--------\n [1, 2)\n [0, 2)\n [0, 1)\n\n\n\n\ndef test_rust_regex_from_file() -&gt; None:\n    with tempfile.TemporaryDirectory() as temp_dir:\n        rgx_text_file = Path(temp_dir) / \"temp\"\n        with open(rgx_text_file, \"w\") as f:\n            f.write(\"aa\")\n\n        commands = f\"\"\"\n            string_rel(X) &lt;- rgx_string_from_file(\"{rgx_text_file}\",\".+\") -&gt; (X)\n            span_rel(X) &lt;- rgx_span_from_file(\"{rgx_text_file}\",\".+\") -&gt; (X)\n            ?string_rel(X)\n            ?span_rel(X)\n            \"\"\"\n\n        expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'string_rel(X)':\n              X\n            -----\n              a\n             aa\n\n            {QUERY_RESULT_PREFIX}'span_rel(X)':\n               X\n            --------\n             [0, 1)\n             [0, 2)\n             [1, 2)\n            \"\"\"\n\n        run_test(commands, expected_result)\n# test_rust_regex_from_file()\n\nprinting results for query 'string_rel(X)':\n  X\n-----\n  a\n aa\n\nprinting results for query 'span_rel(X)':\n   X\n--------\n [1, 2)\n [0, 2)\n [0, 1)\n\n\n\n\ndef test_rust_regex_groups() -&gt; None:\n    text = \"aab\"\n    pattern = \"(?P&lt;group_all&gt;(?P&lt;group_a&gt;a+)(?P&lt;group_b&gt;b+))\"\n\n    commands = f\"\"\"\n            group_string_rel(X,Y,Z) &lt;- rgx_string(\"{text}\",\"{pattern}\") -&gt; (X,Y,Z)\n            ?group_string_rel(X, Y, Z)\n            \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'group_string_rel(X, Y, Z)':\n          X  |  Y  |  Z\n        -----+-----+-----\n         aa  |  b  | aab\n          a  |  b  | ab\"\"\"\n\n    run_test(commands, expected_result)\n# test_rust_regex_groups()\n\nprinting results for query 'group_string_rel(X, Y, Z)':\n  X  |  Y  |  Z\n-----+-----+-----\n  a  |  b  | ab\n aa  |  b  | aab\n\n\n\n\ndef test_python_regex() -&gt; None:\n    commands = \"\"\"\n           py_string_rel(X) &lt;- py_rgx_string(\"aa\",\".+\") -&gt; (X)\n           py_span_rel(X) &lt;- py_rgx_span(\"aa\",\".+\") -&gt; (X)\n           ?py_string_rel(X)\n           ?py_span_rel(X)\n           \"\"\"\n\n    expected_result = f\"\"\"{QUERY_RESULT_PREFIX}'py_string_rel(X)':\n              X\n            -----\n             aa\n\n            {QUERY_RESULT_PREFIX}'py_span_rel(X)':\n               X\n            --------\n             [0, 2)\"\"\"\n\n    run_test(commands, expected_result)\n\n# test_python_regex()\n\nprinting results for query 'py_string_rel(X)':\n  X\n-----\n aa\n\nprinting results for query 'py_span_rel(X)':\n   X\n--------\n [0, 2)"
  },
  {
    "objectID": "building_your_optimization.html#general-passes",
    "href": "building_your_optimization.html#general-passes",
    "title": "Building your own optimization",
    "section": "General Passes",
    "text": "General Passes\nBefore reading this section, we will briefly explain how passes work. There are five kinds of passes: 1. AST transformation passes - These passes convert the input program into AST. 2. semantic checks passes - These passes check the corectness of the program (i.e. one of the passes asserts that all the relations used in the program were registerred before). 3. AST execution passes - These passes traverse the AST and covert it to a parse graph. In addition they register new relations and handle variables assingments. 4. term graph passes - These passes adds rules into the term graph. 5. execution pass - This pass traverses the parse graph and finds queries. Then it computes them using the term graph.",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#optimization-passes",
    "href": "building_your_optimization.html#optimization-passes",
    "title": "Building your own optimization",
    "section": "Optimization Passes",
    "text": "Optimization Passes\nThere are two kinds of optimization passes: 1. The first one, manipulates rules before they are added to the term graph. 2. The second one, manipulates the structure of the term graph.\nnote: It’s also possible to optimize the execution function/pass (but we won’t discuss it in this tutorial).\nIn this section, we will implement two simple optimization passes, one of each kind.",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#rule-manipulation-optimization",
    "href": "building_your_optimization.html#rule-manipulation-optimization",
    "title": "Building your own optimization",
    "section": "Rule-Manipulation Optimization",
    "text": "Rule-Manipulation Optimization\nOptimizations of this kind traverse the parse_graph and find rules that weren’t added to the term graph. Then, they update each rule - by modifying its body relations list.\nHere are some examples of possible optimization passes of this kind: 1. An optimization that removes duplicated relations from a rule. i.e., the rule A(X) &lt;- B(X), C(X), B(X) contains the relation B(X) twice. The optimization will transform the rule into A(X) &lt;- B(X), C(X).\n\nAn optimization that removes useless relations from a rule. i.e. the rule A(X) &lt;- B(X), C(Y) contains the useless relation C(Y). The optimization will transform the rule into A(X) &lt;- B(X).\n\nBelow is an implementation of the latter example:\n\nOptimization Example: Remove Useless Relations\nBefore jumping into the actual implementation, we will implement it in psuedo code:\n1. a. Add the free variables inside the rule head into a relevant_free_variables set.\n   b. Mark all relations as useless, except those with no free variables (they are always relevant).\n   \n2. Find all relations which contain at least one free variable inside the relevant_free_variables set.\n\n3. Unmark these relations (since they are relevant).\n\n4. Add all free variables of the unmarked relations into the relevant_free_variables set.\n\n5. Repeat steps 2, 3 and 4 until the set of the marked relations converge.\nnote that this is a [`fixed_point`](https://DeanLight.github.io/spannerlib/general_utils.html#fixed_point) algorithm.\n\nfrom spannerlib.general_utils import fixed_point\nfrom spannerlib.general_utils import get_output_free_var_names\nfrom spannerlib.general_utils import get_input_free_var_names\n\nInstallation NLP failed\ncargo or rustup are not installed in $PATH. please install rust: https://rustup.rs/\n\n\nget_output_free_var_names documentation get_input_free_var_names documentation\n\n# first, lets implement the logic that removes useless relations from a rule\ndef remove_useless_relations(rule):\n        \"\"\"\n        Finds redundant relations and removes them from the rule.\n        \n        @param rule: a rule.\n        \"\"\"\n        # step 1.a. Add the free variables inside the rule head into a relevant_free_variables set.\n        relevant_free_vars = set(rule.head_relation.get_term_list())  \n\n        # step 1.b. Mark all relations as useless, except those with no free variables (they are always relevant).\n        initial_useless_relations_and_types = [(rel, rel_type) for rel, rel_type in zip(rule.body_relation_list, rule.body_relation_type_list)\n                                               if len(get_output_free_var_names(rel)) != 0]\n        # implement steps 2, 3 and 4\n        def step_function(current_useless_relations_and_types):\n            \"\"\"\n            Used by fixed pont algorithm.\n\n            @param current_useless_relations_and_types: current useless relations and their types\n            @return: useless relations after considering the new relevant free vars.\n            \"\"\"\n\n            next_useless_relations_and_types = []\n            \n            # step 2 - Find all relations that has at least on free variable inside the relevant_free_variables set.\n            for relation, rel_type in current_useless_relations_and_types:\n                term_list = get_output_free_var_names(relation)\n                if len(relevant_free_vars.intersection(term_list)) == 0:\n                    next_useless_relations_and_types.append((relation, rel_type))\n                else:\n                    # step 3 - Unmark relation. The relations isn't added to the useless list, and thus it's unmarked.\n                    # step 4 - Add all the free variables of the unmarked relation into the relevant_free_variables set.\n                    relevant_free_vars.update(term_list)\n                    relevant_free_vars.update(get_input_free_var_names(relation))\n\n            return next_useless_relations_and_types\n\n        # step 5 - fixed ponint. note that the distance function returns zero if and only if len(x) equals len(y).\n        useless_relations_and_types = fixed_point(start=initial_useless_relations_and_types, step=step_function, distance=lambda x, y: int(len(x) != len(y)))\n        \n        # this part filters the useless relation from the rule\n        relevant_relations_and_types = set(zip(rule.body_relation_list, rule.body_relation_type_list)).difference(useless_relations_and_types)\n        new_body_relation_list, new_body_relation_type_list = zip(*relevant_relations_and_types)\n        rule.body_relation_list = list(new_body_relation_list)\n        rule.body_relation_type_list = list(new_body_relation_type_list)\n\n\nfrom spannerlib.graphs import GraphBase, EvalState, STATE, TYPE, VALUE\nfrom spannerlib.passes_utils import ParseNodeType\nfrom spannerlib.lark_passes import GenericPass  # base class of all the passes\n    \n\n# finally, the implementation of the optimization pass\nclass RemoveUselessRelationsFromRule(GenericPass):\n    \"\"\"\n    This pass removes duplicated relations from a rule.\n    For example, the rule A(X) &lt;- B(X), C(Y) contains a redundant relation (C(Y)).\n    After this pass the rule will be A(X) &lt;- B(X).\n\n    @note: in the rule A(X) &lt;- B(X, Y), C(Y); C(Y) is not redundant!\n    \"\"\"\n    \n    def __init__(self, parse_graph: GraphBase, **kwargs):\n        self.parse_graph = parse_graph\n            \n    def run_pass(self, **kwargs):\n        # get the new rules in the parse graph\n        rules = self.parse_graph.get_all_nodes_with_attributes(type=ParseNodeType.RULE, state=EvalState.NOT_COMPUTED)\n        for rule_node_id in rules:\n            rule_node = self.parse_graph[rule_node_id]\n            rule = rule_node[VALUE]\n            remove_useless_relations(rule)\n\nModifying the pass stack looks like this:\n\ndef print_pass_stack(pass_stack):\n    \"\"\"prints pass stack in a nice format\"\"\"\n    \n    for pass_ in pass_stack:\n        print(\"\\t\" + pass_.__name__)\n        \nmagic_session = Session()  # reset the magic session\n\noriginal_pass_stack = magic_session.get_pass_stack()  # save the original pass stack\n\nnew_pass_stack = original_pass_stack.copy()\nterm_graph_pass = new_pass_stack.pop()  # remove last pass (this pass adds rules to term graph)\nnew_pass_stack.extend([RemoveUselessRelationsFromRule, term_graph_pass])\n\nmagic_session.set_pass_stack(new_pass_stack)\n\nprint(f\"Pass stack before:\")\nprint_pass_stack(original_pass_stack)\n\nprint(\"\\nPass stack after:\")\nprint_pass_stack(magic_session.get_pass_stack())\n\nPass stack before:\n    RemoveTokens\n    FixStrings\n    CheckReservedRelationNames\n    ConvertSpanNodesToSpanInstances\n    ConvertStatementsToStructuredNodes\n    CheckDefinedReferencedVariables\n    CheckReferencedRelationsExistenceAndArity\n    CheckReferencedIERelationsExistenceAndArity\n    CheckRuleSafety\n    TypeCheckAssignments\n    TypeCheckRelations\n    SaveDeclaredRelationsSchemas\n    ResolveVariablesReferences\n    ExecuteAssignments\n    AddStatementsToNetxParseGraph\n    AddRulesToTermGraph\n\nPass stack after:\n    RemoveTokens\n    FixStrings\n    CheckReservedRelationNames\n    ConvertSpanNodesToSpanInstances\n    ConvertStatementsToStructuredNodes\n    CheckDefinedReferencedVariables\n    CheckReferencedRelationsExistenceAndArity\n    CheckReferencedIERelationsExistenceAndArity\n    CheckRuleSafety\n    TypeCheckAssignments\n    TypeCheckRelations\n    SaveDeclaredRelationsSchemas\n    ResolveVariablesReferences\n    ExecuteAssignments\n    AddStatementsToNetxParseGraph\n    RemoveUselessRelationsFromRule\n    AddRulesToTermGraph\n\n\nNow let’s look at the effect of this pass on the parse graph:\n\ncommands = \"\"\"\nnew Good(int)\nnew Bad(int)\n\nExample(X) &lt;- Good(X), Bad(Y)\n\"\"\"\n\ndef run_commands_and_print_parse_graph(session):\n    session.run_commands(commands)\n    print(session._parse_graph)\n    \n\nprint(\"Parse graph of unmodified pass stack:\\n\")\nrun_commands_and_print_parse_graph(Session()) \n\nprint(\"\\nParse graph after adding optimization pass:\\n\")\nrun_commands_and_print_parse_graph(magic_session) \n\nParse graph of unmodified pass stack:\n\n(__spannerlog_root) (computed) root\n    (0) (computed) relation_declaration: Good(int)\n    (1) (computed) relation_declaration: Bad(int)\n    (2) (computed) rule: Example(X) &lt;- Good(X), Bad(Y)\n\n\nParse graph after adding optimization pass:\n\n(__spannerlog_root) (computed) root\n    (0) (computed) relation_declaration: Good(int)\n    (1) (computed) relation_declaration: Bad(int)\n    (2) (computed) rule: Example(X) &lt;- Good(X)\n\n\n\nNotice the difference in the rule node!",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#term-graph-structure-optimization",
    "href": "building_your_optimization.html#term-graph-structure-optimization",
    "title": "Building your own optimization",
    "section": "Term Graph Structure Optimization",
    "text": "Term Graph Structure Optimization\nOptimizations of this kind traverse the term_graph and modify its structure.",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#term-graph-structure",
    "href": "building_your_optimization.html#term-graph-structure",
    "title": "Building your own optimization",
    "section": "Term Graph Structure",
    "text": "Term Graph Structure\nBefore reading on, it is important to understand how the [`TermGraph`](https://DeanLight.github.io/spannerlib/graphs.html#termgraph) looks like in order to understand the terminology used - there is detailed documentation inside the class docstring.",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#structure-optimization",
    "href": "building_your_optimization.html#structure-optimization",
    "title": "Building your own optimization",
    "section": "Structure Optimization",
    "text": "Structure Optimization\nHere are some examples of possible optimization passes of this kind: 1. An optimization that removes join nodes which have only one child relation. Note: this optimization already exists so there is no need to implement it.\n\nAn optimization that removes project nodes whose input is a single-column relation.\n\nHere’s the implementation of the second example:",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#optimization-example-remove-redundant-project-nodes",
    "href": "building_your_optimization.html#optimization-example-remove-redundant-project-nodes",
    "title": "Building your own optimization",
    "section": "Optimization Example: Remove Redundant Project Nodes",
    "text": "Optimization Example: Remove Redundant Project Nodes\nThe following optimization will traverse the term graph and find all project nodes that has input relation with arity of one. In this case, the project node is redundant and therefore, we remove it from the term graph.\nBefore jumping into the actual implementation, we will implement it in a psuedo code:\n1. Find all project nodes and their union nodes parents (inside the term graph).\n\n2. For each project node\n\n    2.1. Check if the arity of the project node's input relation is one, using the following steps:\n        a. get project's node child - we will denote it as child_node.\n        b. if type of child_node is GET_REL or RULE_REL or CALC node child, return true if arity of the relation stored in child_node is one.\n        c. if type of child_node is SELECT, return true if there is only one free variable in the relation stored in the child of the child_node. \n        d. if type of child_node is project:\n              (i). get input relaations from all of it's children nodes\n              (ii). return true if the arity of the join of all the input relations is one.\n              \n    2.2 if has arity of one, remove the node from the graph by connecting it's child to it's parent.\n\n# helper function\ndef is_relation_has_one_free_var(relation) -&gt; bool:\n    \"\"\"\n    Check whether relation is only one free variable.\n\n    @param relation_: a relation or an ie_relation.\n    \"\"\"\n\n    return len(relation.get_term_list()) == 1\n\n\nfrom spannerlib.graphs import *\n# this function implements step 2 in the pseudo code\ndef is_input_relation_of_node_has_arity_of_one(term_graph: TermGraphBase, node_id) -&gt; bool:\n    \"\"\"\n    @param node_id: id of the node.\n    @note: we expect id of project/join node.\n    @return: the arity of the relation that the node gets during the execution.\n    \"\"\"\n\n    # staep 2.1.a: note that this methods suppose to work for both project nodes and join nodes.\n    # project nodes always have one child while join nodes always have more than one child.\n    # for that reason, we traverse all the children of the node.\n    node_ids = term_graph.get_children(node_id)\n    \n    # used to compute arity of final relation\n    free_vars: Set[str] = set()\n\n    for node_id in node_ids:\n        node_attrs = term_graph[node_id]\n        node_type = node_attrs[TYPE]\n        \n        # step 2.1.b\n        if node_type in (TermNodeType.GET_REL, TermNodeType.RULE_REL, TermNodeType.CALC):\n            relation = node_attrs[VALUE]\n            # if relation has more than one free var we can't prune the project\n            if not is_relation_has_one_free_var(relation):\n                return False\n\n            free_vars |= set(relation.get_term_list())\n            \n        # step 2.1.c\n        elif node_type is TermNodeType.SELECT:\n            relation_child_id = next(iter(term_graph.get_children(node_id)))\n            relation = term_graph[relation_child_id][VALUE]\n            if not is_relation_has_one_free_var(relation):\n                return False\n\n            relation_free_vars = [var for var, var_type in zip(relation.get_term_list(), relation.get_type_list()) if var_type is DataTypes.free_var_name]\n            free_vars |= set(relation_free_vars)\n        \n        # step 2.1.d\n        elif node_type is TermNodeType.JOIN:\n            # the input of project node is the same as the input of the join node\n            return is_input_relation_of_node_has_arity_of_one(term_graph, node_id)\n\n    return len(free_vars) == 1\n\n\n# finally, lets implement the optimization pass class\nclass PruneUnnecessaryProjectNodes(GenericPass):\n    \"\"\"\n    This class prunes project nodes that gets a relation with one column (therefore, the project is redundant).\n\n    For example, the rule A(X) &lt;- B(X) will yield the following term graph:\n\n        rule_rel node (of A)\n            union node\n                project node (on X)\n                   get_rel node (get B)\n\n        since we project a relation with one column, after this pass the term graph will be:\n\n        rule_rel node (of A)\n            union node\n                get_rel node (get B)\n\n    \"\"\"\n\n    def __init__(self, term_graph: TermGraphBase, **kwargs):\n        self.term_graph = term_graph\n\n    def run_pass(self, **kwargs):\n        self.prune_project_nodes()\n        \n    def prune_project_nodes(self) -&gt; None:\n        \"\"\"\n        Prunes the redundant project nodes.\n        \"\"\"\n\n        project_nodes = self.term_graph.get_all_nodes_with_attributes(type=TermNodeType.PROJECT)\n        for project_id in project_nodes:\n            if is_input_relation_of_node_has_arity_of_one(self.term_graph, project_id):\n                # step 2.2\n                self.term_graph.add_edge(self.term_graph.get_parent(project_id), self.term_graph.get_child(project_id))\n                self.term_graph.remove_node(project_id)\n                \n\nThe next step is adding this pass to the pass stack:\n\nmagic_session = Session()  # reset the magic_session\n\nnew_pass_stack = magic_session.get_pass_stack()\nnew_pass_stack.append(PruneUnnecessaryProjectNodes)\nmagic_session.set_pass_stack(new_pass_stack)\n\nprint(\"New pass stack:\")\nprint_pass_stack(magic_session.get_pass_stack())\n\nNew pass stack:\n    RemoveTokens\n    FixStrings\n    CheckReservedRelationNames\n    ConvertSpanNodesToSpanInstances\n    ConvertStatementsToStructuredNodes\n    CheckDefinedReferencedVariables\n    CheckReferencedRelationsExistenceAndArity\n    CheckReferencedIERelationsExistenceAndArity\n    CheckRuleSafety\n    TypeCheckAssignments\n    TypeCheckRelations\n    SaveDeclaredRelationsSchemas\n    ResolveVariablesReferences\n    ExecuteAssignments\n    AddStatementsToNetxParseGraph\n    AddRulesToTermGraph\n    PruneUnnecessaryProjectNodes\n\n\nFinally, lets see how this pass modifies the term graph:\n\ncommands = \"\"\"\nnew B(int)\nA(X) &lt;- B(X)\n\"\"\"\n\ndef run_commands_and_print_term_graph(session):\n    session.run_commands(commands)\n    print(session._term_graph)\n    \n\nprint(\"Term graph of unmodified pass stack:\\n\")\nrun_commands_and_print_term_graph(Session()) \n\nprint(\"\\nTerm graph after adding optimization pass:\\n\")\nrun_commands_and_print_term_graph(magic_session) \n\nTerm graph of unmodified pass stack:\n\n(__spannerlog_root) (not_computed) root\n    (A) (not_computed) rule_rel: A(X)\n        (0) (not_computed) union\n            (1) (not_computed) project: ['X']\n                (2) (not_computed) get_rel: B(X)\n\nDependencyGraph is:\n__spannerlog_root\n    A\n\n\nTerm graph after adding optimization pass:\n\n(__spannerlog_root) (not_computed) root\n    (A) (not_computed) rule_rel: A(X)\n        (0) (not_computed) union\n            (2) (not_computed) get_rel: B(X)\n\nDependencyGraph is:\n__spannerlog_root\n    A\n\n\n\nNotice the changes in the term_graph’s structure!",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "building_your_optimization.html#optimization-example-overlapping-rules",
    "href": "building_your_optimization.html#optimization-example-overlapping-rules",
    "title": "Building your own optimization",
    "section": "Optimization Example: Overlapping Rules",
    "text": "Optimization Example: Overlapping Rules\nAnother optimization example, this time without an implementation. It does the following: 1. Finds overlapping structure of rules. 2. Merges the overlapping structure and adds it to the term graph.\n\nDetailed Example Explanation\nLet’s look at the following example:\n&gt;&gt;&gt; D(X,Y) &lt;- A(X),B(Y),C(X,Y,Z)\n&gt;&gt;&gt; E(X,Y) &lt;- A(X),C(X,Y,Z), F(Z)\n&gt;&gt;&gt; x(X,Y) &lt;- E(X,Y)\n&gt;&gt;&gt; x(X,Y) &lt;- D(X,Y)\nWithout merging terms with overlapping structures, this would naively generate something that abstractly looks like this: \n\n\n\n\n\n\n\n\n\nThe weakness with this approach is that A AND C is computed twice.\nA version of the term graph that takes care to merge terms with overlapping structures would look more like this: \n\n\n\n\n\n\n\n\n\nHere, we realized that A,C is a joint component and that we need only compute it once. This would be the automatic equivalent of a smart programmer, refactoring the query above to look like\n&gt;&gt;&gt; TEMP(X,Y,Z) &lt;- A(X), C(X,Y,Z)\n&gt;&gt;&gt; D(X,Y) &lt;- B(Y),TEMP(X,Y,Z)\n&gt;&gt;&gt; E(X,Y) &lt;- TEMP(X,Y,Z), F(Z)\n&gt;&gt;&gt; x(X,Y) &lt;- E(X,Y)\n&gt;&gt;&gt; x(X,Y) &lt;- D(X,Y)\nHere’s a pseudo implementation of this pass:\n1. get all the registered rules by using term_graph.get_all_rules).\n\n2. find overlapping structure between rules (this step can be implemented in many different ways).\n\n3. create new rule that consists of the overlapping structure.\n\n4. add this new rule to the term graph by using term_graph.add_rule_to_term_graph.\n\n5. updated the previous rule to use the newly created rule.\n\n6. added the rules to the term graph.\n\n7. delete the previous versions of the rule from the term graph by using term_graph.remove_rule.\nFor example, if the following rules were registered: 1. D(X,Y) &lt;- A(X),B(Y),C(X,Y,Z) 2. E(X,Y) &lt;- A(X),C(X,Y,Z), F(Z)\nIn the second step of the algorithm, we will find that both rules share the structure A(X),C(X,Y,Z). In the third step we will create a new relation TEMP(X,Y,Z) &lt;- A(X), C(X,Y,Z), and add it to the term graph. In the fifth step we will modify to original rules in the following way: 1. D(X,Y) &lt;- B(Y),TEMP(X,Y,Z) 2. E(X,Y) &lt;- TEMP(X,Y,Z), F(Z)\nAnd then we will add them to the term graph. The last step will delete the old rules from the term graph.",
    "crumbs": [
      "Tutorials",
      "Building your own optimization"
    ]
  },
  {
    "objectID": "adding_inference_rules_to_term_graph.html",
    "href": "adding_inference_rules_to_term_graph.html",
    "title": "Adding inference rules to term graph",
    "section": "",
    "text": "source\n\nAddRulesToTermGraph\n\n AddRulesToTermGraph (parse_graph:GraphBase, term_graph:TermGraphBase,\n                      **kwargs:Any)\n\nThis class traverses the parse graph and finds all the new rules. It adds these rules to the term graph (using term_graph’s add rule method).\n\nsource\n\n\nAddRulesToTermGraph._add_rules_to_computation_graph\n\n AddRulesToTermGraph._add_rules_to_computation_graph ()\n\nGenerates and adds all the execution trees to the term graph.",
    "crumbs": [
      "src",
      "Graphs",
      "Adding inference rules to term graph"
    ]
  }
]