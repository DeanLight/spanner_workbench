# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/010_engine.ipynb.

# %% auto 0
__all__ = ['logger', 'PrimitiveType', 'Type', 'STRING_PATTERN', 'op_to_func', 'Var', 'FreeVar', 'RelationDefinition', 'Relation',
           'IEFunction', 'IERelation', 'Rule', 'pretty', 'graph_compose', 'merge_term_graphs_pair', 'merge_term_graphs',
           'DB', 'Engine', 'select', 'project', 'rename', 'union', 'intersection', 'difference', 'join', 'calc',
           'get_rel', 'compute_node']

# %% ../nbs/010_engine.ipynb 3
from abc import ABC, abstractmethod
import pytest
from collections import defaultdict

import pandas as pd
from pathlib import Path
from typing import no_type_check, Set, Sequence, Any,Optional,List,Callable,Dict,Union
from pydantic import BaseModel
import networkx as nx
import itertools
from graph_rewrite import draw, draw_match, rewrite, rewrite_iter
from .utils import serialize_graph,serialize_df_values,checkLogs,get_new_node_name
from .span import Span,SpanParser

import logging
logger = logging.getLogger(__name__)

# %% ../nbs/010_engine.ipynb 5
from enum import Enum
from typing import Any
from pydantic import ConfigDict


class Var(BaseModel):
    name: str
    def __hash__(self):
        return hash(self.name)

class FreeVar(BaseModel):
    name: str
    def __hash__(self):
        return hash(self.name)

PrimitiveType=Union[str,int,Span]
Type = Union[str,int,Span,Var,FreeVar]

class RelationDefinition(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    name: str
    scheme: List[type]

class Relation(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    name: str
    terms: List[Type]

class IEFunction(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    name: str
    func: Callable
    in_schema: List[type]
    out_schema: List[type]


class IERelation(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    name: str
    in_terms: List[Type]
    out_terms: List[Type]
    def __hash__(self):
        hash_str = f'''{self.name}_in_{'_'.join([str(x) for x in self.in_terms])}_out_{'_'.join([str(x) for x in self.out_terms])}'''
        return hash(hash_str)
class Rule(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    head: Relation
    body: List[Union[Relation,IERelation]]

# %% ../nbs/010_engine.ipynb 6
def pretty(obj):
    """pretty printing dataclasses for user messages,
    making them look like spannerlog code instead of python code"""
    
    if isinstance(obj,Span):
        return f"[{obj.start},{obj.end})"
    elif isinstance(obj,(Var,FreeVar)):
        return obj.name
    elif isinstance(obj,RelationDefinition):
        return f"{obj.name}({','.join(pretty(o) for o in obj.scheme)})"
    elif isinstance(obj,Relation):
        return f"{obj.name}({','.join(pretty(o) for o in obj.terms)})"
    elif isinstance(obj,IERelation):
        return f"{obj.name}({','.join(pretty(o) for o in obj.in_terms)}) -> ({','.join(pretty(o) for o in obj.out_terms)})"
    elif isinstance(obj,IEFunction):
        return f"{obj.name}({','.join(pretty(o) for o in obj.in_schema)}) -> ({','.join(pretty(o) for o in obj.out_schema)})"
    elif isinstance(obj,Rule):
        return f"{pretty(obj.head)} <- {','.join(pretty(o) for o in obj.body)}"
    elif isinstance(obj,type):
        return obj.__name__
    else:
        return str(obj)

# %% ../nbs/010_engine.ipynb 11
def _get_bounding_order(rule:Rule)->List[Union[Relation,IERelation]]:
    """Get an order of evaluation for the body of a rule
    this is a very naive ordering that can be heavily optimized"""

    # we start with all relations since they can be bound at once
    order = list()
    bounded_vars = set()
    for rel in rule.body:
        if isinstance(rel,Relation):
            order.append(rel)
            for term in rel.terms:
                if isinstance(term,FreeVar):
                    bounded_vars.add(term)

    unordered_ierelations = {rel for rel in rule.body if isinstance(rel,IERelation)}
    while len(unordered_ierelations) > 0:
        for ie_rel in unordered_ierelations:
            in_free_vars = {term for term in ie_rel.in_terms if isinstance(term,FreeVar)}
            if in_free_vars.issubset(bounded_vars):
                order.append(ie_rel)
                out_free_vars = {term for term in ie_rel.out_terms if isinstance(term,FreeVar)}
                bounded_vars = bounded_vars.union(out_free_vars)
                unordered_ierelations.remove(ie_rel)
                break

    return order

# %% ../nbs/010_engine.ipynb 14
def _name_node(counter):
    if isinstance(counter,itertools.count):
        return next(counter)
    else: # if its just the name to give
        return counter

def _select_if_needed(g,node_counter,source_node,terms):
    """add a project node as a father of source_node if the terms are not all free variables
    returns the source_node if no project is needed, or the project node if it is needed
    the name of the project node should be supplied
    """

    need_select = any(not isinstance(term,FreeVar) for term in terms)
    if not need_select:
        return source_node
    
    select_pos_val = list()
    for i,term in enumerate(terms):
        if not isinstance(term,FreeVar):
            select_pos_val.append((i,term))
    
    select_name = _name_node(node_counter)
    g.add_node(select_name, op='select',theta=select_pos_val)
    g.add_edge(select_name,source_node)
    return select_name

def _product_if_needed(g,node_counter,source_node,terms):
    """add a product node as a father of source_node if the terms are not all free variables
    returns the source_node if no product is needed, or the product node if it is needed
    the name of the product node should be supplied
    """

    need_product = any(not isinstance(term,FreeVar) for term in terms)
    if not need_product:
        return source_node
    
    product_pos_val = list()
    for i,term in enumerate(terms):
        if not isinstance(term,FreeVar):
            product_pos_val.append((i,term))
    
    product_name = _name_node(node_counter)
    g.add_node(product_name, op='product',theta=product_pos_val)
    g.add_edge(product_name,source_node)
    return product_name

# TODO from here, iteratively build the joins each time using _project_if_needed on the outrel and inrel of the relations/ierelaitons
def _rule_to_term_graph(rule:Rule,rule_id) -> nx.DiGraph:
    """Convert a rule to a directed RA+IE term graph"""
    node_counter = itertools.count()
    G = nx.DiGraph()
    # add nodes for all relations
    body_term_connectors = list()
    body_rels = _get_bounding_order(rule)

    # create derivation for each rel in the body
    for rel_idx,rel in enumerate(body_rels):
        if isinstance(rel,Relation):
            G.add_node(rel.name,rel=rel.name)
            rename_node = _name_node(node_counter)
            G.add_node(rename_node, op='rename',names=[(i,term.name) for i,term in enumerate(rel.terms) if isinstance(term,FreeVar)])
            G.add_edge(rename_node,rel.name)
            top_rel_node = _select_if_needed(G,node_counter,rename_node,rel.terms)
            
            body_term_connectors.append((None,top_rel_node))

        elif isinstance(rel,IERelation):
            get_input_node_name =_name_node(node_counter)
            calc_node_name = _name_node(node_counter)
            G.add_node(get_input_node_name, op='project', on=[term.name for term in rel.in_terms if isinstance(term,FreeVar)])
            G.add_node(calc_node_name, op='calc',func=rel.name)

            product_name = _name_node(node_counter)
            calc_son = _product_if_needed(G,node_counter,get_input_node_name,rel.in_terms)
            G.add_edge(calc_node_name,calc_son)
            rename_node = _name_node(node_counter)
            G.add_node(rename_node, op='rename',names=[(i,term.name) for i,term in enumerate(rel.out_terms) if isinstance(term,FreeVar)])
            G.add_edge(rename_node,calc_node_name)
            select_name = _name_node(node_counter)
            top_rel_node = _select_if_needed(G,node_counter,rename_node,rel.out_terms)
            body_term_connectors.append((get_input_node_name,top_rel_node))

    # connect outputs of different rels via joins
    # and connect input of ie functons into the join
    for i,(connectors,rel) in enumerate(zip(body_term_connectors,body_rels)):
        if i == 0:
            prev_top = connectors[1]
            continue

        current_top = connectors[1]

        join_node_name = _name_node(node_counter)
        G.add_node(join_node_name, op='join')
        G.add_edge(join_node_name,prev_top)
        G.add_edge(join_node_name,current_top)

        if isinstance(rel,IERelation):
            ie_bottom = connectors[0]
            G.add_edge(ie_bottom,prev_top)


        prev_top = join_node_name

    # project all assignments into the head
    head_project_name = _name_node(node_counter)
    G.add_node(head_project_name, op='project', on=[term.name for term in rule.head.terms],rel=f'_{rule.head.name}_{rule_id}')
    G.add_edge(head_project_name,prev_top)

    # add a union for each rule for the given head
    G.add_node(rule.head.name,op='union',rel=rule.head.name)
    G.add_edge(rule.head.name,head_project_name)

    # add rule id for each node
    for u in G.nodes:
        G.nodes[u]['rule_id'] = {rule_id}
    return G

# %% ../nbs/010_engine.ipynb 17
def graph_compose(g1,g2,mapping_dict,debug=False):
    """compose two graphs with a mapping dict"""
    # if there is a node in g2 that is renamed but has a name collision with an existing node that is not renamed, we will rename the existing node to a uniq name
    # making new names into a digraph is a dirty hack, TODO resolve this
    save_new_names= nx.DiGraph()
    original_mapping_dict = mapping_dict.copy()
    for u2 in g2.nodes():
        if u2 not in mapping_dict and u2 in g1.nodes():
            mapping_dict[u2] = get_new_node_name(g2,avoid_names_from=[g1,save_new_names])
            save_new_names.add_node(mapping_dict[u2])
    if debug:
        return mapping_dict
    g2 = nx.relabel_nodes(g2,mapping_dict,copy=True)

    merged_graph = nx.compose(g1,g2)
    for old_name,new_name in original_mapping_dict.items():
        rule_ids1 = g1.nodes[old_name].get('rule_id',set())
        rule_ids2 = g2.nodes[new_name].get('rule_id',set())
        merged_rule_ids = rule_ids1.union(rule_ids2)
        merged_graph.nodes[new_name]['rule_id'] = merged_rule_ids



    return merged_graph


# %% ../nbs/010_engine.ipynb 22
def merge_term_graphs_pair(g1,g2,exclude_props = ['label'],debug=False):
    """merge two term graphs into one term graph
    when talking about term graphs, 2 nodes if their data is identical and all of their children are identical
    but we would also like to merge rules for the same head, so we will also nodes that have the same 'rel' attribute
    """

    def _are_nodes_equal(g1,u1,g2,u2):

        u1_data = g1.nodes[u1]
        u2_data = g2.nodes[u2]
        
        if 'rel' in u1_data and 'rel' in u2_data:
            return u1_data['rel'] == u2_data['rel']


        
        return False
        # TODO this old code tries to merge nodes, but then its hard to remember which belong to which rules so we only merge
        # so we will do this merging per query
        u1_clean_data = {k:v for k,v in u1_data.items() if k not in exclude_props}
        u2_clean_data = {k:v for k,v in u2_data.items() if k not in exclude_props}

        are_equal = u1_clean_data == u2_clean_data and all(v2 in node_mappings for v2 in g2.successors(u2))
        return are_equal
        

    # we will check for each node in g2 if it has a node in g1 which is it's equal.
    # and save that in a mapping
    node_mappings=dict()# g2 node name to g1 node name
    # we use the fact that g2 is going to be acyclic to travers it in postorder
    for u2 in nx.dfs_postorder_nodes(g2):
        for u1 in g1.nodes():
            if _are_nodes_equal(g1,u1,g2,u2):
                node_mappings[u2] = u1
                break



    if debug:
        return node_mappings
    else:
        return graph_compose(g1,g2,node_mappings)



def merge_term_graphs(gs,exclude_props = ['label'],debug=False):
    """merge a list of term graphs into one term graph
    """
    merge = gs[0]
    for g in gs[1:-1]:
        merge = merge_term_graphs_pair(merge,g,exclude_props,debug=False)
    # if debug, we run debug only on the last merge so we can iteratively debug a list of merges
    return merge_term_graphs_pair(merge,gs[-1],exclude_props,debug=debug)


# %% ../nbs/010_engine.ipynb 32
import re
STRING_PATTERN = re.compile(r"^[^\r\n]+$")


def _infer_relation_schema(row) -> Sequence[type]: # Inferred type list of the given relation
    """
    Guess the relation type based on the data.
    We support both the actual types (e.g. 'Span'), and their string representation ( e.g. `"[0,8)"`).

    **@raise** ValueError: if there is a cell inside `row` of an illegal type.
    """
    relation_types = []
    for cell in row:
        try:
            int(cell)  # check if the cell can be converted to integer
            relation_types.append(int)
        except (ValueError, TypeError):
            if isinstance(cell, Span) or SpanParser.parse(cell):
                relation_types.append(Span)
            elif re.match(STRING_PATTERN, cell):
                relation_types.append(str)
            else:
                raise ValueError(f"value doesn't match any datatype: {cell}")

    return relation_types

# %% ../nbs/010_engine.ipynb 34
def _pd_drop_row(df,row_vals):
    new_df = df[(df!=row_vals).all(axis=1)]
    return new_df

def _pd_append_row(df,row_vals):
    return pd.concat([df,pd.DataFrame([row_vals],columns=df.columns)])

# %% ../nbs/010_engine.ipynb 36
class DB(dict):
    def __repr__(self):
        key_str=', '.join(self.keys())
        return f'DB({key_str})'

# %% ../nbs/010_engine.ipynb 37
from copy import deepcopy
class Engine():
    def __init__(self,rewrites=None):
        if rewrites is None:
            self.rewrites = []
        self.symbol_table={
            # key : type,val
        }
        self.Relation_defs={
            # key : RelationDefinition for both real and derived relations
        }
        self.ie_functions={
            # name : IEFunction class
        }

        self.term_graph = nx.DiGraph()
        
        self.node_counter = itertools.count()
        self.rule_counter = itertools.count()

        self.db = DB(
            # relation_name: dataframe
        )

        # lets skip this for now and keep it a an attribute in the node graph
        self.rules_to_ids = {
            # rule pretty string, to node id in term_graph
        }

        # self.rels_to_nodes() = {
        #     # relation name to node that represents it
        # }

    @classmethod
    def _col_names(self,length):
        # these names wont conflixt with logical variables since they must always start with Uppercase letters
        return [f'col{i}' for i in range(length)]
    

    def set_var(self,var_name,value,read_from_file=False):
        symbol_table = self.symbol_table
        if var_name in symbol_table:
            existing_type,existing_value = symbol_table[var_name]
            if type(value) != existing_type:
                raise ValueError(f"Variable {var_name} was previously defined with {existing_value}({pretty(existing_type)})"
                                f" but is trying to be redefined to {value}({pretty(type(value))}) of a different type which might interfere with previous rule definitions")    
        symbol_table[var_name] = type(value),value
        return
    def get_var(self,var_name):
        return self.symbol_table.get(var_name,None)
    
    def del_var(self,var_name):
        del self.symbol_table[var_name]

    def get_relation(self,rel_name:str):
        return self.Relation_defs.get(rel_name,None)

    def set_relation(self,rel_def:RelationDefinition):
        if rel_def.name in self.Relation_defs:
            existing_def = self.Relation_defs[rel_def.name]
            if existing_def != rel_def:
                raise ValueError(f"Relation {rel_def.name} was previously defined with {existing_def}"
                                f"but is trying to be redefined to {rel_def} which might interfere with previous rule definitions")
        else:
            self.Relation_defs[rel_def.name] = rel_def
            #TODO fix make sure that the empty df has the correct types based on the rel_def
            empty_df = pd.DataFrame(columns=self._col_names(len(rel_def.scheme)))
            self.db[rel_def.name] = empty_df
            self.term_graph.add_node(rel_def.name,rel=rel_def.name)

    def del_relation(self,rel_name:str):
        # TODO we need to think about what to do with all relations that used this rule
        raise NotImplementedError("deleting relations is not supported yet")
        return

    def add_fact(self,fact:Relation):
        self.db[fact.name] = _pd_append_row(self.db[fact.name],fact.terms)

    def add_facts(self,rel_name,facts:pd.DataFrame):
        facts= facts.copy()
        facts.columns = self._col_names(len(facts.columns))
        self.db[rel_name] = pd.concat([self.db[rel_name],facts])

    def del_fact(self,fact:Relation):
        self.db[fact.name] = _pd_drop_row(df = self.db[fact.name],row_vals=fact.terms)

    def get_ie_function(self,name:str):
        return self.ie_functions.get(name,None)

    def set_ie_function(self,ie_func:IEFunction):
        self.ie_functions[ie_func.name]=ie_func

    def del_ie_function(self,name:str):
        del self.ie_functions[name]

    def add_rule(self,rule:Rule,schema:RelationDefinition=None):
        if not self.get_relation(rule.head.name) and schema is None:
            raise ValueError(f"Relation {rule.head.name} not defined before adding the rule with it's head\n"
                             f"And an relation schema was not supplied."
                             f"existing relations are {self.Relation_defs.keys()}")

        if not schema is None:
            self.set_relation(schema)

        rule_id = next(self.rule_counter)

        self.rules_to_ids[pretty(rule)] = rule_id

        g2 = _rule_to_term_graph(rule,rule_id)

        merge_term_graph = merge_term_graphs_pair(self.term_graph,g2)
        self.term_graph = merge_term_graph
        

    def del_rule(self,rule_str:str):
        if not rule_str in self.rules_to_ids:
            raise ValueError(f"Rule {rule_str} does not exist\n"
                             f"existing rules are {self.rules_to_ids.keys()}")
        rule_id = self.rules_to_ids[rule_str]
        g = self.term_graph
        nodes_to_delete=[]
        for u in g.nodes:
            node_rule_ids = g.nodes[u].get('rule_id',set())
            if rule_id in node_rule_ids:
                node_rule_ids.remove(rule_id)
                if len(node_rule_ids) == 0:
                    nodes_to_delete.append(u)
        g.remove_nodes_from(nodes_to_delete)
            
        return

    def _inline_db_and_ies_in_graph(self,g:nx.DiGraph):
        g=deepcopy(g)
        for u in g.nodes:
            if g.out_degree(u)==0:
                g.nodes[u]['op'] = 'get_rel'
                g.nodes[u]['db'] = self.db
            elif g.nodes[u]['op'] == 'calc':
                ie_func_name = g.nodes[u]['func']
                g.nodes[u]['func'] = self.ie_functions[ie_func_name].func
                g.nodes[u]['out_schema'] = self.ie_functions[ie_func_name].out_schema
        return g


    def plan_query(self,q_rel:Relation,rewrites=None):
        if rewrites is None:
            rewrites = self.rewrites
        query_graph = self._inline_db_and_ies_in_graph(self.term_graph)

        # get the sub term graph induced by the relation head
        root_node = q_rel.name
        connected_nodes = list(nx.shortest_path(query_graph,root_node).keys())
        query_graph = nx.DiGraph(nx.subgraph(query_graph,connected_nodes))
        
        # based on the asked relation, add:
        # select node if there are constants
        # project node to project to the remaining free variables
        # rename node to rename the cols to the Free vars the query is asking for
        select_node = _select_if_needed(query_graph,get_new_node_name(query_graph),root_node,q_rel.terms)
        rename_node = get_new_node_name(query_graph)
        query_graph.add_node(rename_node, op='rename',names=[(i,term.name) for i,term in enumerate(q_rel.terms) if isinstance(term,FreeVar)])
        query_graph.add_edge(rename_node,select_node)
        project_node = get_new_node_name(query_graph)
        query_graph.add_node(project_node, op='project', on=[term.name for term in q_rel.terms if isinstance(term,FreeVar)])
        query_graph.add_edge(project_node,rename_node)

        # TODO for all rewrites, run them
        return query_graph,project_node

    def execute_plan(self,query_graph,root_node):
        results_dict = defaultdict(list)
        return compute_node(query_graph,root_node,results_dict)

    def run_query(self,q:Relation,rewrites=None):
        query_graph,root_node = self.plan_query(q,rewrites)
        return self.execute_plan(query_graph,root_node)


# %% ../nbs/010_engine.ipynb 57
def select(df,theta,**kwargs):
    if df.empty:
        return df

    if callable(theta):
        return df[theta(df)]
    else:
        pos_val_pairs = theta
        for pos,val in pos_val_pairs:
            df = df[df.iloc[:,pos] == val]
        return df

def project(df,on,**kwargs):
    if df.empty:
        return df
    return df[on]

def rename(df,names,**kwargs):
    if df.empty:
        return df
    names_mapper = {
    }
    current_col_names = list(df.columns)
    for i,name in names:
        names_mapper[current_col_names[i]] = name
    return df.rename(names_mapper,axis=1)

def union(*dfs,**kwargs):
    return pd.concat(dfs).drop_duplicates()

def intersection(df1,df2,**kwargs):
    return pd.merge(df1,df2,how='inner',on=list(df1.columns))

def difference(df1,df2,**kwargs):
    return pd.concat([df1,df2]).drop_duplicates(keep=False)

def join(df1,df2,**kwargs):
    cols1 = set(df1.columns)
    cols2 = set(df2.columns)
    on = cols1 & cols2
    # get only logical variables
    on = [ col for col in on if isinstance(col,str) and col[0].isupper()]
    if len(on)==0:
        return pd.merge(df1,df2,how='cross')
    else:
        return pd.merge(df1,df2,how='inner',on=on)

def calc(df,func,out_schema,semantics='per_row',**kwargs):
    if df.empty:
        return df
    if semantics == 'per_row':
    
        def check_output_generator(vals):
            for val in vals:
                if _infer_relation_schema(val) != out_schema:
                    pretty_actual_schema = [pretty(term) for term in _infer_relation_schema(val)]
                    pretty_expected_schema = [pretty(term) for term in out_schema]
                    raise ValueError(f"Function {func} returned a value of an unexpected schema "
                            f"{val}({pretty_actual_schema}) expected {pretty_expected_schema}")
                yield val

        vals = (list(vals) for _,vals in df.iterrows())
        vals = (func(*vals) for vals in vals)
        vals = check_output_generator(vals)

        return pd.DataFrame(vals,columns=Engine._col_names(len(out_schema)))
    
    else: # bulk
        raise NotImplementedError('non per row semantics are not supported for calculating ie functions')

def get_rel(rel,db,**kwargs):
    return db[rel]


# %% ../nbs/010_engine.ipynb 58
op_to_func = {
    'union':union,
    'intersection':intersection,
    'difference':difference,
    'select':select,
    'project':project,
    'rename':rename,
    'join':join,
    'calc':calc,
    'get_rel':get_rel
}

# %% ../nbs/010_engine.ipynb 82
def compute_node(G,u,results_dict):
    """
    """
    global op_to_func
    u_data = G.nodes[u]
    if u_data.get('final',False):
        return results_dict[u][-1]
    

    children_ids = list(G.successors(u))
    children_data = [G.nodes[child_id] for child_id in children_ids]

    if len(children_ids)==0:
        all_children_final=True
        children_results=[]
        
    else:
        all_children_final = all(child_data.get('final',False) for child_data in children_data)
        # this block helps avoid infinite recursion when we need to initialize the 0th iteration of a node in a cycle as the empty relation
        if u_data.get('visited',False):
            u_data['visited'] = True
            return pd.DataFrame()
        u_data['visited'] =True
        
        children_results = [compute_node(G,child_id,results_dict) for child_id in children_ids]



    op_code = u_data['op']
    op_func = op_to_func[op_code]
    logger.debug(f'computing node {u} with op {op_code} and children results {children_results} and data {u_data}')
    current_results = op_func(*children_results,**u_data)
    results_dict[u].append(current_results)

    # if all children are final then we can mark this node as final
    if all_children_final:
        u_data['final'] = True
    # else check for fixed point
    elif len(results_dict[u])>1:
        this_res = results_dict[u][-1]
        last_res = results_dict[u][-2]
        if pd.DataFrame.equals(this_res,last_res):
            u_data['final'] = True
        else:
            u_data['final'] = False
    # if this is the first iteration we need to wait for the second one to check for fixed point so final is still false
    else:
        u_data['final']=False

    return current_results




