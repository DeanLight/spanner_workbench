{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19 NLP pipeline\n",
    "\n",
    "### The pipline repository [link](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/)\n",
    "\n",
    "### Inroduction:\n",
    "The primary objective of the NLP pipeline is to identify individuals who have been positively diagnosed with COVID-19 by extracting pertinent information from unstructured free-text narratives found within the Electronic Health Record (EHR) of the Department of Veterans Affairs (VA). By automating this process, the pipeline streamlines the screening of a substantial volume of clinical text, significantly reducing the time and effort required for identification.\n",
    "The pipeline is built on medSpacy framework, and defines a new UI to use.\n",
    "Our goal is to write the pipline in rgxlog language so we show a real world example about the benefits of the rgxlog framework from the NLP world.\n",
    "\n",
    "### pipline stages:\n",
    "- [Concept tagger](#concept-tag-rules)\n",
    "- [Target matcher](#target-rules)\n",
    "- [Sectionizer](#section-rules)\n",
    "- [Context matcher](#context-rules)\n",
    "- [Postprocessor](#postprocess-rules)\n",
    "- [Document Classifier](#document_classifier)\n",
    "\n",
    "We will implement each stage separately later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need to install some requirements to work with [medspacy](https://github.com/medspacy/medspacy) framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /miniconda/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /miniconda/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /miniconda/lib/python3.8/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /miniconda/lib/python3.8/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /miniconda/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /miniconda/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /miniconda/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /miniconda/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /miniconda/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /miniconda/lib/python3.8/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /miniconda/lib/python3.8/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /miniconda/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /miniconda/lib/python3.8/site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /miniconda/lib/python3.8/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /miniconda/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: jinja2 in /miniconda/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /miniconda/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /miniconda/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /miniconda/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /miniconda/lib/python3.8/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /miniconda/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /miniconda/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /miniconda/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /miniconda/lib/python3.8/site-packages (from jinja2->spacy) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /miniconda/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /miniconda/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.6.2)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /miniconda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /miniconda/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /miniconda/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /miniconda/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /miniconda/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
>>>>>>> defea06 (fixing comments)
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import what we need from the rgxlog framework and define some ie functions that will be used in every stage of the pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import rgxlog\n",
    "from rgxlog import magic_session\n",
    "from rgxlog import Session\n",
    "from rgxlog.engine.datatypes.primitive_types import DataTypes\n",
    "from rgxlog.engine.datatypes.primitive_types import Span\n",
    "session = rgxlog.magic_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
<<<<<<< HEAD
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /spanner_workbench\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nbdev in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (2.3.13)\n",
      "Requirement already satisfied: pandas in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: notebook in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (6.5.4)\n",
      "Requirement already satisfied: pyDatalog in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.17.4)\n",
      "Requirement already satisfied: tabulate in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: lark-parser>=0.9.0 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.12.0)\n",
      "Requirement already satisfied: ipython>=7.18.1 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (8.12.2)\n",
      "Requirement already satisfied: setuptools>=50.2.0 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (65.6.3)\n",
      "Requirement already satisfied: networkx>=2.5 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (3.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.6.2)\n",
      "Requirement already satisfied: jsonpath-ng in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.5.3)\n",
      "Requirement already satisfied: psutil in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (5.9.5)\n",
      "Requirement already satisfied: install-jdk in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.0.4)\n",
      "Requirement already satisfied: parse in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.19.0)\n",
      "Requirement already satisfied: spanner-nlp in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (0.0.6)\n",
      "Requirement already satisfied: pytest in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (7.3.1)\n",
      "Requirement already satisfied: Jinja2 in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (3.1.2)\n",
      "Requirement already satisfied: pycodestyle in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (2.10.0)\n",
      "Requirement already satisfied: mypy in /miniconda/lib/python3.8/site-packages (from rgxlog==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (2.15.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (3.0.38)\n",
      "Requirement already satisfied: typing-extensions in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (4.6.2)\n",
      "Requirement already satisfied: pickleshare in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.7.5)\n",
      "Requirement already satisfied: backcall in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (5.9.0)\n",
      "Requirement already satisfied: decorator in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.18.2)\n",
      "Requirement already satisfied: stack-data in /miniconda/lib/python3.8/site-packages (from ipython>=7.18.1->rgxlog==0.0.1) (0.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /miniconda/lib/python3.8/site-packages (from Jinja2->rgxlog==0.0.1) (2.1.2)\n",
      "Requirement already satisfied: ply in /miniconda/lib/python3.8/site-packages (from jsonpath-ng->rgxlog==0.0.1) (3.11)\n",
      "Requirement already satisfied: six in /miniconda/lib/python3.8/site-packages (from jsonpath-ng->rgxlog==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /miniconda/lib/python3.8/site-packages (from mypy->rgxlog==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /miniconda/lib/python3.8/site-packages (from mypy->rgxlog==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: PyYAML in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (6.0)\n",
      "Requirement already satisfied: astunparse in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (1.6.3)\n",
      "Requirement already satisfied: watchdog in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: execnb>=0.1.4 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (0.1.5)\n",
      "Requirement already satisfied: ghapi>=1.0.3 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (1.0.4)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (1.5.29)\n",
      "Requirement already satisfied: ipywidgets<=8.0.4 in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (8.0.4)\n",
      "Requirement already satisfied: asttokens in /miniconda/lib/python3.8/site-packages (from nbdev->rgxlog==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (1.8.2)\n",
      "Requirement already satisfied: prometheus-client in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (0.17.0)\n",
      "Requirement already satisfied: nbformat in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (5.8.0)\n",
      "Requirement already satisfied: ipython-genutils in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (5.3.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (7.4.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (25.0.2)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (8.2.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (0.17.1)\n",
      "Requirement already satisfied: argon2-cffi in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (21.3.0)\n",
      "Requirement already satisfied: ipykernel in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (6.23.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (1.5.6)\n",
      "Requirement already satisfied: tornado>=6.1 in /miniconda/lib/python3.8/site-packages (from notebook->rgxlog==0.0.1) (6.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /miniconda/lib/python3.8/site-packages (from pandas->rgxlog==0.0.1) (2023.3)\n",
      "Requirement already satisfied: packaging in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (22.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /miniconda/lib/python3.8/site-packages (from pytest->rgxlog==0.0.1) (1.1.1)\n",
      "Requirement already satisfied: requests in /miniconda/lib/python3.8/site-packages (from spanner-nlp->rgxlog==0.0.1) (2.28.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /miniconda/lib/python3.8/site-packages (from fastcore>=1.5.27->nbdev->rgxlog==0.0.1) (22.3.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /miniconda/lib/python3.8/site-packages (from ipywidgets<=8.0.4->nbdev->rgxlog==0.0.1) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /miniconda/lib/python3.8/site-packages (from ipywidgets<=8.0.4->nbdev->rgxlog==0.0.1) (3.0.9)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /miniconda/lib/python3.8/site-packages (from ipykernel->notebook->rgxlog==0.0.1) (1.6.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /miniconda/lib/python3.8/site-packages (from ipykernel->notebook->rgxlog==0.0.1) (0.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /miniconda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.18.1->rgxlog==0.0.1) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /miniconda/lib/python3.8/site-packages (from jupyter-client>=5.3.4->notebook->rgxlog==0.0.1) (6.6.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /miniconda/lib/python3.8/site-packages (from jupyter-core>=4.6.1->notebook->rgxlog==0.0.1) (3.5.1)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /miniconda/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (2.6.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /miniconda/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.2.3)\n",
      "Requirement already satisfied: defusedxml in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (0.7.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (2.0.5)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (0.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (4.12.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (0.2.2)\n",
      "Requirement already satisfied: tinycss2 in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (1.2.1)\n",
      "Requirement already satisfied: bleach in /miniconda/lib/python3.8/site-packages (from nbconvert>=5->notebook->rgxlog==0.0.1) (6.0.0)\n",
      "Requirement already satisfied: fastjsonschema in /miniconda/lib/python3.8/site-packages (from nbformat->notebook->rgxlog==0.0.1) (2.17.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /miniconda/lib/python3.8/site-packages (from nbformat->notebook->rgxlog==0.0.1) (4.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /miniconda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.18.1->rgxlog==0.0.1) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /miniconda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.18.1->rgxlog==0.0.1) (0.2.6)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /miniconda/lib/python3.8/site-packages (from argon2-cffi->notebook->rgxlog==0.0.1) (21.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /miniconda/lib/python3.8/site-packages (from astunparse->nbdev->rgxlog==0.0.1) (0.37.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda/lib/python3.8/site-packages (from requests->spanner-nlp->rgxlog==0.0.1) (1.25.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /miniconda/lib/python3.8/site-packages (from stack-data->ipython>=7.18.1->rgxlog==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /miniconda/lib/python3.8/site-packages (from stack-data->ipython>=7.18.1->rgxlog==0.0.1) (0.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /miniconda/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=5.3.4->notebook->rgxlog==0.0.1) (3.15.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (0.19.3)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (5.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (23.1.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (3.6.2)\n",
      "Requirement already satisfied: jupyter-server-terminals in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.4.4)\n",
      "Requirement already satisfied: overrides in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (7.3.1)\n",
      "Requirement already satisfied: websocket-client in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /miniconda/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.6.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /miniconda/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->rgxlog==0.0.1) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /miniconda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=5->notebook->rgxlog==0.0.1) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /miniconda/lib/python3.8/site-packages (from bleach->nbconvert>=5->notebook->rgxlog==0.0.1) (0.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /miniconda/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /miniconda/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->rgxlog==0.0.1) (2.21)\n",
      "Requirement already satisfied: rfc3339-validator in /miniconda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /miniconda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (0.1.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /miniconda/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->rgxlog==0.0.1) (2.0.7)\n",
      "Requirement already satisfied: uri-template in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (2.1)\n",
      "Requirement already satisfied: fqdn in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.5.1)\n",
      "Requirement already satisfied: webcolors>=1.11 in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.13)\n",
      "Requirement already satisfied: isoduration in /miniconda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (20.11.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arrow>=0.15.0 in /miniconda/lib/python3.8/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook->rgxlog==0.0.1) (1.2.3)\n",
      "Building wheels for collected packages: rgxlog\n",
      "  Building wheel for rgxlog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rgxlog: filename=rgxlog-0.0.1-py3-none-any.whl size=81619 sha256=bf1d1ae42abcb98c4588097919d16672c310e801c16f2cd831735faede5bff8c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hu2r35sz/wheels/62/da/4d/48e3d3b55711eef8154983104097b04f07d8a68aa8ee1aa6ed\n",
      "Successfully built rgxlog\n",
      "Installing collected packages: rgxlog\n",
      "  Attempting uninstall: rgxlog\n",
      "    Found existing installation: rgxlog 0.0.1\n",
      "    Uninstalling rgxlog-0.0.1:\n",
      "      Successfully uninstalled rgxlog-0.0.1\n",
      "Successfully installed rgxlog-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! {current_python} -m pip install {package_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import rgxlog\n",
    "from rgxlog.session import Session\n",
    "from rgxlog.primitive_types import Span, DataTypes\n",
    "from rgxlog import magic_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some generic ie functions that will be used in every stage of the pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
>>>>>>> defea06 (fixing comments)
   "outputs": [],
   "source": [
    "def read_from_file(text_path):\n",
    "    \"\"\"\n",
    "    Reads from file and return it's content.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to read from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file.\n",
    "    \"\"\"\n",
    "    with open(f\"{text_path}\", 'r') as file:\n",
    "        content = file.read()\n",
    "    yield content\n",
    "magic_session.register(ie_function=read_from_file,\n",
    "                       ie_function_name = \"read_from_file\",\n",
    "                       in_rel=[DataTypes.string],\n",
    "                       out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "for value in read_from_file(\"sample1.txt\"):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_interval_conflicts(replacements):\n",
    "    \"\"\"\n",
    "    This function takes a list of replacements, where each replacement is represented\n",
    "    as a list containing a label and a span (interval). It checks for conflicts among\n",
    "    the intervals and returns a list of resolved replacements, ensuring that no two\n",
    "    intervals overlap.\n",
    "\n",
    "    Parameters:\n",
    "    replacements (list of lists): A list of replacements, where each replacement\n",
    "        is represented as a list [label, span].\n",
    "\n",
    "    Returns:\n",
    "    list of lists: A list of resolved replacements, where each replacement is a list\n",
    "        [label, span], ensuring that there are no conflicts among intervals.\n",
    "    \"\"\"\n",
    "    # Sort the replacements by the size of the spans in descending order\n",
    "    replacements.sort(key=lambda x: x[1].span_end - x[1].span_start, reverse=True)\n",
    "\n",
    "    # Initialize a list to keep track of intervals that have been replaced\n",
    "    resolved_replacements = []\n",
    "    \n",
    "    for label, span in replacements:\n",
    "        conflict = False\n",
    "\n",
    "        for _, existing_span in resolved_replacements:\n",
    "            existing_start = existing_span.span_start\n",
    "            existing_end = existing_span.span_end\n",
    "\n",
    "            if not (span.span_end <= existing_start or span.span_start >= existing_end):\n",
    "                conflict = True\n",
    "                break\n",
    "\n",
    "        if not conflict:\n",
    "            resolved_replacements.append([label, span])\n",
    "\n",
    "    return resolved_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spans(spans_table, paths_table):\n",
    "    \"\"\"\n",
    "    This function takes tables a spans tables and path table for the files paths,\n",
    "    it generate queries for the tables, executes the queries using a session, processes the results, \n",
    "    and replaces specific spans in a text with the corresponding labels, it first\n",
    "    resolve spans overlapping conflicts for each giving path.\n",
    "\n",
    "    Parameters:\n",
    "    spans_table (str): A string representing the spans table to process, table columns are formated as (Label, Span, Path).\n",
    "    paths_table (str): A string representing the paths table to process, table columns are formated as (Path)\n",
    "\n",
    "    Returns:\n",
    "    str: The adjusted text string with the new labels.\n",
    "    \"\"\"\n",
    "    # Get a list of all the paths\n",
    "    paths = session.run_commands(f\"?{paths_table}(Path)\", print_results=False, format_results=True)\n",
    "    paths = paths[0].values.tolist()\n",
    "    for path_list in paths:\n",
    "        path = path_list[0]\n",
    "\n",
    "        # Generate a spans query for each path, the query will be formates as (Label, Span, Path)\n",
    "        results = session.run_commands(f'?{spans_table}(Label, Span, \"{path}\")', print_results=True, format_results=True)\n",
    "        if len(results[0]) == 0:\n",
    "            continue\n",
    "        # replacments is list of lists where each list is a [Label, Span]\n",
    "        replacements = results[0].values.tolist()\n",
    "        \n",
    "        with open(f\"{path}\", 'r') as file:\n",
    "            adjusted_string = file.read()\n",
    "    \n",
    "        # Resolve spans conflicts\n",
    "        resolved_replacements = resolve_interval_conflicts(replacements)\n",
    "    \n",
    "        # Sort the resolved replacements by the starting index of each span in descending order\n",
    "        resolved_replacements.sort(key=lambda x: x[1].span_start, reverse=True)\n",
    "    \n",
    "        # iterate over the resolved query results and replace the space with the corresponding label\n",
    "        for i in range(len(resolved_replacements)):\n",
    "            replace_string, span = resolved_replacements[i]\n",
    "            replace_length = len(replace_string)\n",
    "            adjusted_string = adjusted_string[:span.span_start] + replace_string + adjusted_string[span.span_end:]\n",
    "    \n",
    "        with open(f\"{path}\", 'w') as file:\n",
    "            file.writelines(adjusted_string)"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'sampleMatches(Label, Span, \"example.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " Covid-19 | [12, 29)\n",
      "\n",
      "The boy has Covid-19\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('The boy has novel coronavirus')\n",
    "%rgxlog new samplePaths(str)\n",
    "%rgxlog samplePaths(\"example.txt\")\n",
    "%rgxlog new sampleMatches(str, span, str)\n",
    "%rgxlog sampleMatches(\"Covid-19\", [12,29), \"example.txt\")\n",
    "replace_spans('sampleMatches', 'samplePaths', magic_session)\n",
    "for value in read_from_file(\"example.txt\"):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_span_contained(span1, span2):\n",
    "    \"\"\"\n",
    "    Checks if one span is contained within the other span and returns the smaller span if yes.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (span)\n",
    "        span2 (span)\n",
    "\n",
    "    Returns:\n",
    "        span: span1 if contained within span2 or vice versa, or None if not contained.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield span1\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield span2\n",
    "\n",
    "magic_session.register(is_span_contained, \"is_span_contained\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Span: 8-9\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "span1 = Span(2, 12)\n",
    "span2 = Span(8, 9)\n",
    "for span in is_span_contained(span1, span2):\n",
    "    print(f\" Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_span(span1, span2):\n",
    "    \"\"\"\n",
    "    Computes the relative position of the conatined span within the other span.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (Span): The first span object.\n",
    "        span2 (Span): The second span object.\n",
    "\n",
    "    Returns:\n",
    "        Span: The new relative span of the contained one.\n",
    "        None: If there's no span contained within the other.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield Span(span1.span_start - span2.span_start, span1.span_end - span2.span_start)\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield Span(span2.span_start - span1.span_start, span2.span_end - span1.span_start)\n",
    "\n",
    "magic_session.register(get_relative_span, \"get_relative_span\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Span: 0-3\n"
     ]
    }
   ],
   "source": [
    "# usage example \n",
    "span1 = Span(2, 12)\n",
    "span2 = Span(2, 5)\n",
    "for span in get_relative_span(span1, span2):\n",
    "    print(f\" Span: {span.span_start}-{span.span_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenization(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    tokenizing it into sentences and returns each individual sentence in the processed text using a generator.\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        str: Individual sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        yield sentence.text\n",
    "\n",
    "magic_session.register(ie_function=sent_tokenization, ie_function_name = \"sent_tokenization\", in_rel=[DataTypes.string], out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient presents to be tested for COVID-19.\n",
      "His wife recently tested positive for novel coronavirus.\n",
      "SARS-COV-2 results came back positive.\n"
     ]
    }
   ],
   "source": [
    "# usage example \n",
    "for sentence in sent_tokenization(\"sample1.txt\"):\n",
    "    print(sentence)"
   ]
  },
  {
>>>>>>> defea06 (fixing comments)
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The paths of the text files to be classified should be written in \"files_paths.csv\" file"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 18,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1.txt\n",
      "sample2.txt\n",
      "sample3.txt\n",
      "sample4.txt\n",
      "sample5.txt"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat files_paths.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 19,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"files_paths.csv\", relation_name=\"FilesPaths\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 20,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                   Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | Patient presents to be tested for COVID-19. His wife recently tested positive for novel coronavirus. SARS-COV-2 results came back positive.\n",
      " sample2.txt |                                         The patient was tested for COVID-19. Results are positive.\n",
      " sample3.txt |                                            Problem List: 1. Pneumonia 2. Novel Coronavirus 2019\n",
      " sample4.txt |                                                            neg covid education.\n",
      " sample5.txt |                                                         positive covid precaution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "FilesContent(Path, Content) <- FilesPaths(Path), read_from_file(Path) -> (Content)\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<a id='concept-tag-rules'></a>\n",
    "### [Concept Tag Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/concept_tag_rules.py):\n",
    "Concept tag rules, also known as pattern-based rules or custom rules, are a way to specify and define patterns that an NLP (Natural Language Processing) system should recognize within text data. These rules are used to identify specific concepts or entities within text documents. In the context of MedSpaCy and medical NLP, concept tag rules are often used to identify medical entities and concepts accurately.\n",
    "\n",
    "In the orginal project they used the TargetRule class which defines a rule for identifying a specific concept or entity in text.\n",
    "each concept Target Rule looks like this:\n",
    "\n",
    "TargetRule(\n",
    "            literal=\"coronavirus\",\n",
    "            category=\"COVID-19\",\n",
    "            pattern=[{\"LOWER\": {\"REGEX\": \"coronavirus|hcov|ncov$\"}}],\n",
    "          )\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity.\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\n",
    "\n",
    "Instead what we did is to define regex patterns, we have added these pattern in concept_target_rules.csv file, there are two types of these patterns lemma and pos, that we will implement each later on.\n",
    "Each rule in the csv file is like this : regexPattern, label, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:hcov|covid(?:(?:-)?(?:\\s)?19|10)?|2019-cov|cov2|ncov-19|covd 19|no-cov|sars cov),COVID-19,lemma\n",
      "(?i)(?:coivid|(?:novel )?corona(?:virus)?(?: (?:20)?19)?|sars(?:\\s)?(?:-)?(?:\\s)?cov(?:id)?(?:-)?(?:2|19)),COVID-19,lemma\n",
      "(?i)(?:\\+(?: ve)?|\\(\\+\\)|positive|\\bpos\\b|active|confirmed),positive,lemma\n",
      "(?i)(?:pneum(?:onia)?|pna|hypoxia|septic shoc|ards\\(?(?:(?:[12])/2)\\)?|(?:hypoxemic|acute|severe)? resp(?:iratory)? failure(?:\\(?(?:[12]/2)\\)?)?)\",associated_diagnosis,lemma\n",
      "(?i)(?:(?:diagnos(?:is|ed)|dx(?:\\.)?)(?:of|with)?),diagnosis,lemma\n",
      "(?i)(?:^screen),screening,lemma\n",
      "(?i)(?:in contact with|any one|co-worker|at work|(?:the|a)(?:wo)?man|(?:another|a) (?:pt|patient|pt\\.)),other_experiencer,lemma\n",
      "(?i)(?:patient|pt(?:\\.)?|vt|veteran),patient,lemma\n",
      "(?i)(?:like_num (?:days|day|weeks|week|months|month) (?:ago|prior)),timesx,lemma\n",
      "(?i)(?:(?:antibody|antibodies|ab) test),antibody test,lemma\n",
      "(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\s)?(?:-)?(?:\\s)?(?: infection)?(?: strain)?(?:\\s)?(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63) (?:coronavirus|hcovs?|ncovs?|covs?)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:non(?:\\s)?(?:-)?(?:\\s)?(?:novel|covid|ncovid|covid-19)(?: coronavirus)?|other coronavirus),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:wife|husband|spouse|family|member|girlfriend|boyfriend|mother|father|nephew|niece|grandparent|grandparents|granddaughter|relative|relatives|caregiver),family,pos\n",
      "(?i)(?:grandchild|grandson|cousin|grandmother|grandfather|parent|son|daughter|mom|dad|brother|sister|aunt|uncle|child|children|sibling|siblings),family,pos\n",
      "(?i)(?:someone|somebody|person|anyone|anybody|people|individual|individuals|teacher|anybody|employees|employer|customer|client|residents),other_experiencer,pos\n",
      "(?i)(?:resident|pts|patients|coworker|coworkers|workers|colleague|captain|captains|pilot|pilots|sailor|sailors|meeting),other_experiencer,pos\n",
      "(?i)(?:boyfriend|persons|person|church|convention|guest|party|attendee|conference|roommate|friend|friends|coach|player|neighbor|manager|boss),other_experiencer,pos\n",
      "(?i)(?:cashier|landlord|worked|works|^mate|nobody|mates|housemate|housemates|hotel|soldier|airport|tsa|lady|ladies|lobby|staffer|staffers),other_experiencer,pos"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat concept_tags_rules.csv"
=======
    "Before we continue we have to do some pre processing to help ease the next stages, we will certain words in each list to it's lemma forms, here a list of the words that we want to lemmatize"
>>>>>>> defea06 (fixing comments)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")"
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "man\n",
      "woman\n",
      "have\n",
      "do\n",
      "emergency\n",
      "epidemic\n",
      "outbreak\n",
      "crisis\n",
      "breakout\n",
      "pandemic\n",
      "spread\n",
      "confirm\n",
      "person\n",
      "patient\n",
      "veteran\n",
      "limit\n",
      "reduce\n",
      "factor\n",
      "contact\n",
      "case\n",
      "lower\n",
      "minimize\n",
      "risk\n",
      "chance\n",
      "possibility\n",
      "care\n",
      "clean\n",
      "desire\n",
      "flight\n",
      "trip\n",
      "plan\n",
      "reschedule\n",
      "postpone\n",
      "barrier"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat lemma_words.txt"
>>>>>>> defea06 (fixing comments)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Lemma Rules:\n",
    "Lemma rules are rules that used the attribute _lemma of each token in the NLP, so what we defined this function to lemmatize the text, most of the rules used only the raw text, thats why we decided that only to lemmatize the tokens we needed.\n",
    "\n",
    "Example for a lemma rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"results positive\",\n",
    "            \"positive\",\n",
    "            pattern=[\n",
    "                {\"LOWER\": \"results\"},\n",
    "                {\"LEMMA\": \"be\", \"OP\": \"?\"},\n",
    "                {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n",
    "            ],\n",
    "        ),\n",
    "We used the py_rgx_span to capture the patterns, and will use the spans later on in replace_spans that will replace each span with the correct label"
=======
    "We will define a helper method to do that:"
>>>>>>> defea06 (fixing comments)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 22,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, lemmatizes its content using spaCy's English language model,\n",
    "    and replaces certain words with their lemmas the rest will remain the same. The updated text is then written back to the same file.\n",
    "\n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be lemmatized.\n",
    "        lemma_words_path(str): The path that contains the list of words to be lemmatized\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized text.\n",
    "    \"\"\"\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 23,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "printing results for query 'lemma_texts(Path, LemmaText)':\n",
      "    Path     |                                                                    LemmaText\n",
=======
      "The boy be sick \n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('The boy was sick')\n",
    "lemmatized_text = lemmatize_text('example.txt', 'lemma_words.txt')\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the texts to lemmatize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files_paths.csv', 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        path = row[0]\n",
    "        lemmatize_text(path, 'lemma_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for example in sample2.txt, was has changed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                     Content\n",
>>>>>>> defea06 (fixing comments)
      "-------------+--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for novel coronavirus . SARS - COV-2 results came back positive .\n",
      " sample2.txt |                                            The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                                    Problem List : like_num . Pneumonia like_num . Novel Coronavirus like_num\n",
      " sample4.txt |                                                              neg covid education .\n",
      " sample5.txt |                                                           positive covid precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 14,
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='concept-tag-rules'></a>\n",
    "### [Concept Tagger](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/concept_tag_rules.py):\n",
    "Concept tag rules, also known as pattern-based rules or custom rules, are a way to specify and define patterns that an NLP (Natural Language Processing) system should recognize within text data. These rules are used to identify specific concepts or entities within text documents. In the context of MedSpaCy and medical NLP, concept tag rules are often used to identify medical entities and concepts accurately.\n",
    "\n",
    "In the orginal project they used the TargetRule class which defines a rule for identifying a specific concept or entity in text.\n",
    "each concept Target Rule looks like this:\n",
    "\n",
    "TargetRule(\n",
    "            literal=\"coronavirus\",\n",
    "            category=\"COVID-19\",\n",
    "            pattern=[{\"LOWER\": {\"REGEX\": \"coronavirus|hcov|ncov$\"}}],\n",
    "          )\n",
    "\n",
    "**Literal** : This specifies the literal text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the category or label associated with the identified entity.\n",
    "\n",
    "**Pattern** : This defines the pattern or conditions under which the entity should be recognized. It's a list of dictionaries specifying conditions for token matching. These rules some times used lemma attribute or POS of each token. A documentation can be found at : https://spacy.io/usage/rule-based-matching.\n",
    "\n",
    "Instead what we did is to define regex patterns, we have added these pattern in concept_target_rules.csv file, there are two types of these patterns lemma and pos, that we will implement each later on.\n",
    "Each rule in the csv file is like this : regexPattern, label, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:hcov|covid(?:(?:-)?(?:\\s)?19|10)?|2019-cov|cov2|ncov-19|covd 19|no-cov|sars cov),COVID-19,lemma\n",
      "(?i)(?:coivid|(?:novel )?corona(?:virus)?(?: (?:20)?19)?|sars(?:\\s)?(?:-)?(?:\\s)?cov(?:id)?(?:-)?(?:2|19)),COVID-19,lemma\n",
      "(?i)(?:\\+(?: ve)?|\\(\\+\\)|positive|\\bpos\\b|active|confirmed),positive,lemma\n",
      "(?i)(?:pneum(?:onia)?|pna|hypoxia|septic shoc|ards\\(?(?:(?:[12])/2)\\)?|(?:hypoxemic|acute|severe)? resp(?:iratory)? failure(?:\\(?(?:[12]/2)\\)?)?)\",associated_diagnosis,lemma\n",
      "(?i)(?:(?:diagnos(?:is|ed)|dx(?:\\.)?)(?:of|with)?),diagnosis,lemma\n",
      "(?i)(?:^screen),screening,lemma\n",
      "(?i)(?:in contact with|any one|co-worker|at work|(?:the|a)(?:wo)?man|(?:another|a) (?:pt|patient|pt\\.)),other_experiencer,lemma\n",
      "(?i)(?:patient|pt(?:\\.)?|vt|veteran),patient,lemma\n",
      "(?i)(?:like_num (?:days|day|weeks|week|months|month) (?:ago|prior)),timesx,lemma\n",
      "(?i)(?:(?:antibody|antibodies|ab) test),antibody test,lemma\n",
      "(?i)(?:(?:coronavirus|hcovs?|ncovs?|covs?)(?:\\s)?(?:-)?(?:\\s)?(?: infection)?(?: strain)?(?:\\s)?(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:(?:229(?:e)?|oc(?:-)?(?:43)?|o43|0c43|43|nl(?:16(?:3|5))?|hku(?:t|-)?1|hkui|emc|63) (?:coronavirus|hcovs?|ncovs?|covs?)),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:non(?:\\s)?(?:-)?(?:\\s)?(?:novel|covid|ncovid|covid-19)(?: coronavirus)?|other coronavirus),OTHER_CORONAVIRUS,lemma\n",
      "(?i)(?:wife|husband|spouse|family|member|girlfriend|boyfriend|mother|father|nephew|niece|grandparent|grandparents|granddaughter|relative|relatives|caregiver),family,pos\n",
      "(?i)(?:grandchild|grandson|cousin|grandmother|grandfather|parent|son|daughter|mom|dad|brother|sister|aunt|uncle|child|children|sibling|siblings),family,pos\n",
      "(?i)(?:someone|somebody|person|anyone|anybody|people|individual|individuals|teacher|anybody|employees|employer|customer|client|residents),other_experiencer,pos\n",
      "(?i)(?:resident|pts|patients|coworker|coworkers|workers|colleague|captain|captains|pilot|pilots|sailor|sailors|meeting),other_experiencer,pos\n",
      "(?i)(?:boyfriend|persons|person|church|convention|guest|party|attendee|conference|roommate|friend|friends|coach|player|neighbor|manager|boss),other_experiencer,pos\n",
      "(?i)(?:cashier|landlord|worked|works|^mate|nobody|mates|housemate|housemates|hotel|soldier|airport|tsa|lady|ladies|lobby|staffer|staffers),other_experiencer,pos"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat concept_tags_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_session.import_relation_from_csv(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma Rules:\n",
    "Lemma rules are rules that used the attribute _lemma of each token in the NLP, we already lemmatized the texts, so now we can create a regex patterns for that.\n",
    "\n",
    "Example for a lemma rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"results positive\",\n",
    "            \"positive\",\n",
    "            pattern=[\n",
    "                {\"LOWER\": \"results\"},\n",
    "                {\"LEMMA\": \"be\", \"OP\": \"?\"},\n",
    "                {\"LOWER\": {\"IN\": [\"pos\", \"positive\"]}},\n",
    "            ],\n",
    "        ),\n",
    "We used the py_rgx_span to capture the patterns, and will use the spans later on in replace_spans that will replace each span with the correct label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%rgxlog\n",
    "LemmaMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                     Content\n",
      "-------------+--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for novel coronavirus . SARS - COV-2 results came back positive .\n",
      " sample2.txt |                                            The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                                    Problem List : like_num . Pneumonia like_num . Novel Coronavirus like_num\n",
      " sample4.txt |                                                             neg COVID-19 education .\n",
      " sample5.txt |                                                          positive COVID-19 precaution .\n",
      " sample6.txt |                                              The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                                Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'LemmaMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label   |    Span\n",
      "----------+------------\n",
<<<<<<< HEAD
      " COVID-19 |  [34, 42)\n",
      " COVID-19 | [103, 115)\n",
      " COVID-19 | [83, 100)\n",
      " positive | [134, 142)\n",
      " positive |  [70, 78)\n",
=======
      " positive | [134, 142)\n",
      " positive |  [70, 78)\n",
      " COVID-19 | [103, 115)\n",
      " COVID-19 | [83, 100)\n",
      " COVID-19 |  [34, 42)\n",
>>>>>>> defea06 (fixing comments)
      " patient  |   [0, 7)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample2.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [26, 34)\n",
      " positive | [48, 56)\n",
      " patient  | [4, 11)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample3.txt\")':\n",
      "        Label         |   Span\n",
      "----------------------+----------\n",
      "       COVID-19       | [47, 64)\n",
      " associated_diagnosis | [26, 35)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample4.txt\")':\n",
      "  Label   |  Span\n",
      "----------+--------\n",
      " COVID-19 | [4, 9)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample5.txt\")':\n",
      "  Label   |  Span\n",
      "----------+---------\n",
      " COVID-19 | [9, 14)\n",
      " positive | [0, 8)\n",
<<<<<<< HEAD
=======
      " COVID-19 | [9, 17)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample6.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " positive | [26, 34)\n",
      " COVID-19 | [35, 43)\n",
      " patient  | [4, 11)\n",
      "\n",
      "printing results for query 'LemmaMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 31,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                     The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                             Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                      neg COVID-19 education .\n",
      " sample5.txt |                                                   positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Rules:\n",
    "As we mentioned above these rules used the POS attribute of each token, there were a small number of rules so we only used this to the tokens we needed.\n",
    "Example of the a rule from the original NLP:\n",
    "\n",
    "        TargetRule(\n",
    "            \"other experiencer\",\n",
    "            category=\"other_experiencer\",\n",
    "            pattern=[\n",
    "                {\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]},\n",
    "                    \"LOWER\": {\n",
    "                        \"IN\": [\n",
    "                            \"someone\",\n",
    "                            \"somebody\",\n",
    "                            \"person\",\n",
    "                            \"anyone\",\n",
    "                            \"anybody\",\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "The patterns we've defined will match words listed under \"IN\", We specifically capture words if their Part-of-Speech (POS) falls into one of the categories: [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]. To accomplish this, two functions are employed: the first function determines the POS of each token, and the second one, py_rgx_span, captures the predefined patterns. After matching words, We confirm the accurate POS tags of the matched words using spans."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 32,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text_with_pos(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    and returns a tuple of (POS, Span) for each token if it's one of NOUN|PROPN|PRON|ADJ\n",
    "    otherwise an empty tuple will be returned\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, Span): The POS of the token and it's span\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "magic_session.register(ie_function=annotate_text_with_pos, ie_function_name = \"annotate_text_with_pos\", in_rel=[DataTypes.string], out_rel=[DataTypes.string, DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ, (0,4)\n",
      "NOUN, (5,8)\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "with open('example.txt', 'w') as file:\n",
    "    file.write('sick boy')\n",
    "\n",
    "for POS, span in annotate_text_with_pos('example.txt'):\n",
    "    print(f\"{POS}, ({span.span_start},{span.span_end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSTable(POS, Span, Path)':\n",
      "  POS  |    Span    |    Path\n",
      "-------+------------+-------------\n",
      "  ADJ  |   [0, 7)   | sample1.txt\n",
      "  ADJ  | [121, 129) | sample1.txt\n",
      "  ADJ  |  [70, 78)  | sample1.txt\n",
      " NOUN  | [103, 110) | sample1.txt\n",
      " NOUN  |  [49, 53)  | sample1.txt\n",
      " NOUN  |  [8, 16)   | sample1.txt\n",
<<<<<<< HEAD
      " PROPN |  [83, 91)  | sample1.txt\n",
=======
      " PRON  |  [45, 48)  | sample1.txt\n",
      " PROPN |  [34, 42)  | sample1.txt\n",
      " PROPN |  [83, 91)  | sample1.txt\n",
      " PROPN | [94, 102)  | sample1.txt\n",
>>>>>>> defea06 (fixing comments)
      "  ADJ  |  [48, 56)  | sample2.txt\n",
      " NOUN  |  [26, 34)  | sample2.txt\n",
      " NOUN  |  [37, 44)  | sample2.txt\n",
      " NOUN  |  [4, 11)   | sample2.txt\n",
      " NOUN  |  [15, 23)  | sample3.txt\n",
      " PROPN |   [0, 7)   | sample3.txt\n",
      " PROPN |  [26, 46)  | sample3.txt\n",
      " PROPN |  [47, 55)  | sample3.txt\n",
      " PROPN |  [67, 75)  | sample3.txt\n",
      " PROPN |  [8, 12)   | sample3.txt\n",
      " NOUN  |  [13, 22)  | sample4.txt\n",
      " PROPN |   [0, 3)   | sample4.txt\n",
      " PROPN |  [4, 12)   | sample4.txt\n",
      "  ADJ  |   [0, 8)   | sample5.txt\n",
      " NOUN  |  [18, 28)  | sample5.txt\n",
<<<<<<< HEAD
=======
      " PROPN |  [9, 17)   | sample5.txt\n",
      "  ADJ  |  [26, 34)  | sample6.txt\n",
      " NOUN  |  [4, 11)   | sample6.txt\n",
      " NOUN  |  [44, 52)  | sample6.txt\n",
      " PROPN |  [35, 43)  | sample6.txt\n",
      "  ADJ  |   [0, 8)   | sample7.txt\n",
      "  ADJ  |  [36, 43)  | sample7.txt\n",
      " NOUN  |  [21, 27)  | sample7.txt\n",
      " NOUN  |  [44, 54)  | sample7.txt\n",
      " NOUN  |  [59, 68)  | sample7.txt\n",
      " NOUN  |  [69, 80)  | sample7.txt\n",
      " NOUN  |  [9, 20)   | sample7.txt\n",
>>>>>>> defea06 (fixing comments)
      "\n",
      "printing results for query 'POSMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 53) | sample1.txt\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, Path)':\n",
      "  Label  |   Span   |    Path\n",
      "---------+----------+-------------\n",
      " family  | [49, 53) | sample1.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "POSTable(POS, Span, Path) <- FilesContent(Path, Content), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "?POSTable(POS, Span, Path)\n",
    "\n",
    "POSMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?POSMatches(Label, Span, Path)\n",
    "\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n",
    "?POSRuleMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                               Content\n",
      "-------------+-------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His wife recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                     The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                             Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                      neg COVID-19 education .\n",
      " sample5.txt |                                                   positive COVID-19 precaution .\n",
      " sample6.txt |                                       The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                         Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'POSRuleMatches(Label, Span, \"sample1.txt\")':\n",
      "  Label  |   Span\n",
      "---------+----------\n",
      " family  | [49, 53)\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'POSRuleMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the matches with the correct label\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 37,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-rules'></a>\n",
    "### [Target Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/target_rules.py):\n",
    "These rules used the label that was assigned through the concept tagger, to capture some more complex patterns and assign a label for them inorder to decremnt the cases of false positive.\n",
    "Each rule look like this:\n",
    "\n",
    "        TargetRule(\n",
    "            literal=\"coronavirus screening\",\n",
    "            category=\"IGNORE\",\n",
    "            pattern=[\n",
    "                {\"_\": {\"concept_tag\": \"COVID-19\"}},\n",
    "                {\"LOWER\": {\"IN\": [\"screen\", \"screening\", \"screenings\"]}},\n",
    "            ],\n",
    "        ),\n",
    "Since we replaced the spans we found with the corresponding label we didn't need the concept_tag attribute of the token/span.\n",
    "To ease the patterns we have devided them into two groups PreTargetRules and TargetRules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreTargetRules: \n",
    "To ease the process, we have implemented preTarget rules aimed at squash consecutive identical labels assigned through the concept tagger into a single label"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:COVID-19(?: COVID-19)+),COVID-19\n",
      "(?i)(?:positive(?: positive)+),positive\n",
      "(?i)(?:patient(?: patient)+),patient\n",
      "(?i)(?:other_experiencer(?: other_experiencer)+),other_experiencer\n",
      "(?i)(?:screening(?: screening)+),screening"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat pre_target_rules.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"pre_target_rules.csv\", relation_name=\"PreTargetTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PreTargetMatches(Label, Span, Path)':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "PreTargetMatches(Label, Span, Path) <- lemma_texts(Path, Text), PreTargetTagRules(Pattern, Label), py_rgx_span(Text,Pattern) -> (Span)\n",
    "?PreTargetMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PreTargetMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'PreTargetMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'PreTargetMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'PreTargetMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'PreTargetMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"PreTargetMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
=======
   "execution_count": 38,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:COVID-19 positive (?:unit|floor)|positive COVID-19 (?:unit|floor|exposure)),COVID-19\n",
      "(?i)(?:known(?: positive)? COVID-19(?: positive)? (?:exposure|contact)),COVID-19\n",
      "(?i)(?:COVID-19 positive screening|positive COVID-19 screening|screening COVID-19 positive|screening positive COVID-19),positive coronavirus screening\n",
      "(?i)(?:diagnosis : COVID-19 (?:test|screening)),COVID-19\n",
      "(?i)(?:COVID-19 screening),coronavirus screening\n",
      "(?i)(?:active COVID-19 precaution|droplet isolation precaution|positive for (?:flu|influenza)|(?:the|a) positive case|results are confirm),1 2 3\n",
      "(?i)(?:exposed to positive|[ ] COVID-19|age like_num(?: )?\\+|(?:return|back) to work|COVID-19 infection rate),1 2 3\n",
      "(?i)(?:COVID-19 (?:restriction|emergency|epidemic|outbreak|crisis|breakout|pandemic|spread|screening)|droplet precaution),1 2\n",
      "(?i)(?:contact precautions|positive (?:flu|influenza)|positive (?:patient|person)|confirm (?:with|w/(?:/)?|w)|(?:the|positive) case),1 2\n",
      "(?i)(?:results confirm|(?:neg|pos)\\S+ pressure|positive (?:attitude|feedback|serology)|COVID-19 (guidelines|rate)),1 2\n",
      "(?i)(?:has the patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5 6\n",
      "(?i)(?:has patient been diagnosed (?:with|w/(?:/)?|w)),1 2 3 4 5\n",
      "(?i)((?:person|patient) with confirm COVID-19),1 2 3 4\n",
      "(?i)(?:COVID-19 positive (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(?:in order to decrease the spread of the COVID-19 infection),1 2 3 4 5 6 7 8 9 10\n",
      "(?i)(?:COVID-19 positive (?:patient|person|people|veteran)),OTHER_PERSON\n",
      "(?i)(?:positive COVID-19 (?:tested )?other_experiencer),COVID-19\n",
      "(?i)(?:(?:(?:contact|exposure) (?:with|to)? )?positive COVID-19 (?:patient|person|veteran)),OTHER_PERSON\n",
      "(?i)(?:(?:patient|person) (?:who|that) test (?:positive|confirm) for COVID-19),OTHER_PERSON\n",
      "(?i)(ref : not detected|history of present illness|does not know|but|therefore|flu|metapneumovirus|;),<IGNORE>"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat target_rules.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 39,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 40,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "%%rgxlog\n",
    "TargetTagMatches(Label, Span, Path) <- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content, Pattern) -> (Span)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                        The patient have reported positive COVID-19 exposure .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'TargetTagMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample3.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
<<<<<<< HEAD
=======
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample6.txt\")':\n",
      "  Label   |   Span\n",
      "----------+----------\n",
      " COVID-19 | [26, 52)\n",
      "\n",
      "printing results for query 'TargetTagMatches(Label, Span, \"sample7.txt\")':\n",
      "[]\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 43,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section-rules'></a>\n",
    "### [Section Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/section_rules.py):\n",
    "Here, we'll add a section detection component that defines rules for detecting sections titles, which usually appear before a semicolon.\n",
    "Section rules are utilized to identify specific section names, enabling the separation of text into different parts. Entities occurring in certain sections are considered positive.\n",
    "\n",
    "In the original project, the SectionRule class was used to define rules for identifying specific section text. Each SectionRule has the following structure\n",
    "\n",
    "      SectionRule(category=\"problem_list\", literal=\"Active Problem List:\"),\n",
    "      SectionRule(category=\"problem_list\", literal=\"Current Problems:\"),\n",
    "    \n",
    "    \n",
    "**Literal** : This specifies the literal section text or word that this rule is targeting.\n",
    "\n",
    "**Category** : This specifies the section category associated with the identified section.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Similar to the approach used in the concept tagger stage, regex patterns were derived from these literals, and these patterns are stored in the 'section_target_rules.csv' file and are used to match section texts and replace them with their appropriate category.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, sectionLabel"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 44,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:Lab results :),labs :\n",
      "(?i)(?:Addendum :),addendum :\n",
      "(?i)(?:(?:ALLERGIC REACTIONS|ALLERGIES) :),allergies :\n",
      "(?i)(?:(?:CC|Chief Complaint) :),chief_complaint :\n",
      "(?i)(?:COMMENTS :),comments :\n",
      "(?i)(?:(?:(?:ADMISSION )?DIAGNOSES|Diagnosis|Primary Diagnosis|Primary|Secondary(?: (?:Diagnoses|Diagnosis))) :),diagnoses :\n",
      "(?i)(?:(?:Brief Hospital Course|CONCISE SUMMARY OF HOSPITAL COURSE BY ISSUE/SYSTEM|HOSPITAL COURSE|SUMMARY OF HOSPITAL COURSE) :),hospital_course :\n",
      "(?i)(?:(?:Imaging|MRI|INTERPRETATION|Radiology) :),imaging :\n",
      "(?i)(?:(?:ADMISSION LABS|Discharge Labs|ECHO|Findings|INDICATION|Labs|Micro|Microbiology|Studies|Pertinent Results) :),labs_and_studies :\n",
      "(?i)(?:(?:ACTIVE MEDICATIONS(?: LIST)|ADMISSION MEDICATIONS|CURRENT MEDICATIONS|DISCHARGE MEDICATIONS|HOME MEDICATIONS|MEDICATIONS) :),medications :\n",
      "(?i)(?:(?:MEDICATIONS AT HOME|MEDICATIONS LIST|MEDICATIONS ON ADMISSION|MEDICATIONS ON DISCHARGE|MEDICATIONS ON TRANSFER|MEDICATIONS PRIOR TO ADMISSION) :),medications :\n",
      "(?i)(?:Neuro :),neurological :\n",
      "(?i)(?:(?:A/P|MEDICATIONS LIST|ASSESSMENT/PLAN|ASSESSMENT|Clinical Impression|DISCHARGE DIAGNOSES|DISCHARGE DIAGNOSIS) :),observation_and_plan :\n",
      "(?i)(?:(?:Discharge Condition|Discharge Disposition|FINAL DIAGNOSES|FINAL DIAGNOSIS|IMPRESSION|Impression and Plan|Impression and Recommendation) :),observation_and_plan :\n",
      "(?i)(?:(?:Facility|Service) :),other :\n",
      "(?i)(?:(?:Current Medical Problems|History of Chronic Illness|MHx|PAST HISTORY|PAST MEDICAL Hx|PAST SURGICAL HISTORY|PMH|PMHx|PAST MEDICAL HISTORY|UNDERLYING MEDICAL CONDITION) :),past_medical_history :\n",
      "(?i)(?:(?:Education|Patient Education|DISCHARGE INSTRUCTIONS/FOLLOWUP|DISCHARGE INSTRUCTIONS|Followup Instructions) :),patient_education :\n",
      "(?i)(?:(?:PE|PHYSICAL EXAM|PHYSICAL EXAMINATION) :),physical_exam :\n",
      "(?i)(?:(?:Active Problem List|Current Problems|Medical Problems|PROBLEM LIST) :),problem_list :\n",
      "(?i)(?:REASON FOR THIS EXAMINATION :),reason_for_examination :\n",
      "(?i)(?:(?:Electronic Signature|Signed electronically by) :),signature :\n",
      "(?i)(?:(?:PMHSx|PSH|SH|Sexual History:|Social History) :),social_history :"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat section_rules.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 45,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_span_contained(span1, span2):\n",
    "    \"\"\"\n",
    "    Checks if one span is contained within the other span and returns the smaller span if yes.\n",
    "\n",
    "    Parameters:\n",
    "        span1 (span)\n",
    "        span2 (span)\n",
    "\n",
    "    Returns:\n",
    "        span: span1 if contained within span2 or vice versa, or None if not contained.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield span1\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield span2\n",
    "\n",
    "magic_session.register(is_span_contained, \"is_span_contained\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
=======
   "execution_count": 46,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, Path)':\n",
      "     Label      |  Span   |    Path\n",
      "----------------+---------+-------------\n",
      " problem_list : | [0, 14) | sample3.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "SectionRulesMatches(Label, Span, Path) <- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?SectionRulesMatches(Label, Span, Path)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              Problem List : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      " sample6.txt |                                                 The patient have reported COVID-19 .\n",
      " sample7.txt |                          Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample1.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample2.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample3.txt\")':\n",
      "     Label      |  Span\n",
      "----------------+---------\n",
      " problem_list : | [0, 14)\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample4.txt\")':\n",
      "[]\n",
      "\n",
      "printing results for query 'SectionRulesMatches(Label, Span, \"sample5.txt\")':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_spans(\"SectionRulesMatches\", \"FilesPaths\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 49,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'FilesContent(Path, Content)':\n",
      "    Path     |                                                                Content\n",
      "-------------+---------------------------------------------------------------------------------------------------------------------------------------\n",
      " sample1.txt | patient presents to be tested for COVID-19 . His family recently tested positive for COVID-19 . COVID-19 results came back positive .\n",
      " sample2.txt |                                      The patient be tested for COVID-19 . Results be positive .\n",
      " sample3.txt |                              problem_list : like_num . associated_diagnosis like_num . COVID-19 like_num\n",
      " sample4.txt |                                                       neg COVID-19 education .\n",
      " sample5.txt |                                                    positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "?FilesContent(Path, Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Assertion:\n",
    "\n",
    " Next, we will explore how to assert attributes indicating whether a mention of COVID-19 is positive or not. In our project, we have created a table     named 'CovidAttributes' that contains all attributes for each COVID-19 mention. This table will be used for classifying documents."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 50,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionMatches(Path, Span, CovidAttribute)':\n",
      "    Path     |  Span   |  CovidAttribute\n",
      "-------------+---------+------------------\n",
      " sample3.txt | [0, 76) |     positive\n",
      "\n",
      "printing results for query 'CovidMatches(Path, Span)':\n",
      "    Path     |   Span\n",
      "-------------+-----------\n",
      " sample1.txt | [34, 42)\n",
      " sample1.txt | [85, 93)\n",
      " sample1.txt | [96, 104)\n",
      " sample2.txt | [26, 34)\n",
      " sample3.txt | [58, 66)\n",
      " sample4.txt |  [4, 12)\n",
      " sample5.txt |  [9, 17)\n",
<<<<<<< HEAD
=======
      " sample6.txt | [26, 34)\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#Here, we employ a pattern to identify entities present in specific sections and mark them as positive,\n",
    "#and adding them to the 'CovidAttributes' table.\n",
    "\n",
    "pattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n",
    "new SectionRulesAttribute(str, str)\n",
    "SectionRulesAttribute(pattern, \"positive\")\n",
    "\n",
    "SectionMatches(Path, Span, CovidAttribute) <- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -> (Span)\n",
    "?SectionMatches(Path, Span, CovidAttribute)\n",
    "\n",
    "CovidMatches(Path, Span) <- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -> (Span)\n",
    "?CovidMatches(Path, Span)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 51,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'SectionCovidAttributes(Path, CovidSpan, CovidAttribute)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute\n",
      "-------------+-------------+------------------\n",
      " sample3.txt |  [58, 66)   |     positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "SectionCovidAttributes(Path, CovidSpan, CovidAttribute) <- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?SectionCovidAttributes(Path, CovidSpan, CovidAttribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text into Sentences:\n",
    "\n",
    "In the subsequent stages, where attributes are assigned to COVID-19 mentions, a departure from the previous stages occurs. Here, patterns are no longer applied to the entire text, instead, they are applied at the sentence level, since the attributes of COVID-19 mentions are typically determined by the context of the sentence in which they appear. This means the text is processed and tokenized into sentences using spaCy's English language model. This process is accomplished through the use of  ie functions and relations."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenization(text_path):\n",
    "    \"\"\"\n",
    "    This function reads a text file, processes its content using spaCy's English language model,\n",
    "    tokenizing it into sentences and returns each individual sentence in the processed text using a generator.\n",
    "    \n",
    "    Parameters:\n",
    "        text_path (str): The path to the text file to be annotated.\n",
    "\n",
    "    Returns:\n",
    "        str: Individual sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        yield sentence.text\n",
    "\n",
    "magic_session.register(ie_function=sent_tokenization, ie_function_name = \"sent_tokenization\", in_rel=[DataTypes.string], out_rel=[DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
=======
   "execution_count": 52,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'Sents(Path, Sent)':\n",
<<<<<<< HEAD
      "    Path     |                        Sent\n",
      "-------------+----------------------------------------------------\n",
      " sample1.txt |       COVID-19 results came back positive .\n",
      " sample1.txt | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |    patient presents to be tested for COVID-19 .\n",
      " sample2.txt |               Results be positive .\n",
      " sample2.txt |        The patient be tested for COVID-19 .\n",
      " sample3.txt |                 COVID-19 like_num\n",
      " sample3.txt |          associated_diagnosis like_num .\n",
      " sample3.txt |             problem_list : like_num .\n",
      " sample4.txt |              neg COVID-19 education .\n",
      " sample5.txt |           positive COVID-19 precaution .\n",
      "\n",
      "printing results for query 'SentSpans(Path, Sent, SentSpan)':\n",
      "    Path     |                        Sent                        |  SentSpan\n",
      "-------------+----------------------------------------------------+------------\n",
      " sample1.txt |       COVID-19 results came back positive .        | [96, 133)\n",
      " sample1.txt | His family recently tested positive for COVID-19 . |  [45, 95)\n",
      " sample1.txt |    patient presents to be tested for COVID-19 .    |  [0, 44)\n",
      " sample2.txt |               Results be positive .                |  [37, 58)\n",
      " sample2.txt |        The patient be tested for COVID-19 .        |  [0, 36)\n",
      " sample3.txt |                 COVID-19 like_num                  |  [58, 75)\n",
      " sample3.txt |          associated_diagnosis like_num .           |  [26, 57)\n",
      " sample3.txt |             problem_list : like_num .              |  [0, 25)\n",
      " sample4.txt |              neg COVID-19 education .              |  [0, 24)\n",
      " sample5.txt |           positive COVID-19 precaution .           |  [0, 30)\n",
=======
      "    Path     |                                        Sent\n",
      "-------------+------------------------------------------------------------------------------------\n",
      " sample1.txt |                       COVID-19 results came back positive .\n",
      " sample1.txt |                 His family recently tested positive for COVID-19 .\n",
      " sample1.txt |                    patient presents to be tested for COVID-19 .\n",
      " sample2.txt |                               Results be positive .\n",
      " sample2.txt |                        The patient be tested for COVID-19 .\n",
      " sample3.txt |                                 COVID-19 like_num\n",
      " sample3.txt |                                associated_diagnosis\n",
      " sample3.txt |                                     like_num .\n",
      " sample3.txt |                             problem_list : like_num .\n",
      " sample4.txt |                              neg COVID-19 education .\n",
      " sample5.txt |                           positive COVID-19 precaution .\n",
      " sample6.txt |                        The patient have reported COVID-19 .\n",
      " sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments .\n",
      "\n",
      "printing results for query 'SentSpans(Path, Sent, SentSpan)':\n",
      "    Path     |                                        Sent                                        |  SentSpan\n",
      "-------------+------------------------------------------------------------------------------------+------------\n",
      " sample1.txt |                       COVID-19 results came back positive .                        | [96, 133)\n",
      " sample1.txt |                 His family recently tested positive for COVID-19 .                 |  [45, 95)\n",
      " sample1.txt |                    patient presents to be tested for COVID-19 .                    |  [0, 44)\n",
      " sample2.txt |                               Results be positive .                                |  [37, 58)\n",
      " sample2.txt |                        The patient be tested for COVID-19 .                        |  [0, 36)\n",
      " sample3.txt |                                 COVID-19 like_num                                  |  [58, 75)\n",
      " sample3.txt |                                associated_diagnosis                                |  [26, 46)\n",
      " sample3.txt |                                     like_num .                                     |  [15, 25)\n",
      " sample3.txt |                                     like_num .                                     |  [47, 57)\n",
      " sample3.txt |                             problem_list : like_num .                              |  [0, 25)\n",
      " sample4.txt |                              neg COVID-19 education .                              |  [0, 24)\n",
      " sample5.txt |                           positive COVID-19 precaution .                           |  [0, 30)\n",
      " sample6.txt |                        The patient have reported COVID-19 .                        |  [0, 36)\n",
      " sample7.txt | Elevated cholesterol levels require further assessment and lifestyle adjustments . |  [0, 82)\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#Sentences of the text\n",
    "Sents(Path, Sent) <- FilesPaths(Path), sent_tokenization(Path) -> (Sent)\n",
    "?Sents(Path, Sent)\n",
    "\n",
    "#SentSpan is the span of the sentence in the text\n",
    "SentSpans(Path, Sent, SentSpan) <- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -> (SentSpan)\n",
    "?SentSpans(Path, Sent, SentSpan)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_span(span1, span2):\n",
    "    \"\"\"\n",
    "    Computes the relative position of the conatined span within the other span.\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        span1 (Span): The first span object.\n",
    "        span2 (Span): The second span object.\n",
    "\n",
    "    Yields:\n",
    "        Span: The new relative span of the contained one.\n",
    "        None: If there's no span contained within the other.\n",
    "    \"\"\"\n",
    "    start1, end1 = span1.span_start, span1.span_end\n",
    "    start2, end2 = span2.span_start, span2.span_end\n",
    "    \n",
    "    if start2 <= start1 and end1 <= end2:\n",
    "        yield Span(span1.span_start - span2.span_start, span1.span_end - span2.span_start)\n",
    "        \n",
    "    elif start1 <= start2 and end2 <= end1:\n",
    "        yield Span(span2.span_start - span1.span_start, span2.span_end - span1.span_start)\n",
    "\n",
    "magic_session.register(get_relative_span, \"get_relative_span\", in_rel=[DataTypes.span, DataTypes.span], out_rel=[DataTypes.span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
=======
   "execution_count": 53,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
      "    Path     |  CovidSpan  |  CovidAttribute  |       Sent\n",
      "-------------+-------------+------------------+-------------------\n",
      " sample3.txt |   [0, 8)    |     positive     | COVID-19 like_num\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\n",
    "SentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='context-rules'></a>\n",
    "### [Context Rules](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/context_rules.py):\n",
    "These rules assign an attribute for each COVID-19 label based on the context, these attributes will be used later to classify each text.\n",
    "\n",
    "Example for this rule is: \n",
    "\n",
    "    ConTextRule(\n",
    "        literal=\"Not Detected\",\n",
    "        category=\"NEGATED_EXISTENCE\",\n",
    "        direction=\"BACKWARD\",\n",
    "        pattern=[\n",
    "            {\"LOWER\": {\"IN\": [\"not\", \"non\"]}},\n",
    "            {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},\n",
    "            {\"LOWER\": {\"REGEX\": \"detecte?d\"}},\n",
    "        ],\n",
    "        allowed_types={\"COVID-19\"},\n",
    "    ),\n",
    "   **direction** specify if the allowed_types should be before or after the pattern,\n",
    "   **allowed_types** specify on what labels should this rule be applied on "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 54,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?i)(?:positive COVID-19|COVID-19 (?:\\([^)]*\\)) (?:positive|detected)|COVID-19(?: positive)? associated_diagnosis)#positive\n",
      "(?i)(?:COVID-19 status : positive)#positive\n",
      "(?i)(?:associated_diagnosis COVID-19|associated_diagnosis (?:with|w|w//|from) (?:associated_diagnosis )?COVID-19)#positive\n",
      "(?i)(?:COVID-19 positive(?: patient| precaution)?|associated_diagnosis (?:due|secondary) to COVID-19)#positive\n",
      "(?i)(?:(?:current|recent) COVID-19 diagnosis)#positive\n",
      "(?i)(?:COVID-19 (?:- )?related (?:admission|associated_diagnosis)|admitted (?:due to|(?:with|w|w/)) COVID-19)#positive\n",
      "(?i)(?:COVID-19 infection|b34(?:\\.)?2|b97.29|u07.1)#positive\n",
      "(?i)(?:COVID-19 eval(?:uation)?|(?:positive )? COVID-19 symptoms|rule out COVID-19)#uncertain\n",
      "(?i)(?:patient (?:do )?have COVID-19)#positive\n",
      "(?i)(?:diagnosis : COVID-19(?: (?:test|screen)(?:ing|ed|s)? positive)?(?: positive)?)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:not|non) (?:- )?detecte?d)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} negative screening|negative screening(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} : negative)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:not be|none) detected)#negated\n",
      "(?i)(?:free from(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? not (?:be )?tested)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} not indicated)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? NEGATIVE NEG)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} negative test)#negated\n",
      "(?i)(?:negative test(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:without any(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:denie(?:s|d)(?: any| travel)?(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#negated\n",
      "(?i)(?:no (?:evidence(?: of)?|(?:hx|-hx|history) of|diagnosis (?:of)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:no(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:no (?:positive|one|residents|confirm case|contact(?: w/?(?:ith)?$))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? no confirm case)#negated\n",
      "(?i)(?:(?:no|n't) (?:be )? confirm(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:no known|not have)(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:answer(?:ed|s|ing)? (?:no|negative|neg)|negative))#negated\n",
      "(?i)(?:(?:answer(?:ed|s|ing)? (?:no|negative|neg)|(?:neg|negative)(?: for)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:not positive(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? not positive)#negated\n",
      "(?i)(?:excluded(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} excluded)#negated\n",
      "(?i)(?:no risk factor for(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:negative screening(?: for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? screening (?:negative|neg))#negated\n",
      "(?i)(?:(?:screening (?:negative|neg) for|do (?:not|n't) have (?:any )?(?:signs|symptoms|ss|s/s))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? do not screening positive)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:be negative|not test positive))#negated\n",
      "(?i)(?:(?:be negative|not test positive|not? screening(?: for)|no signs of|no (?:sign|symptom|indication(?:of|for)?)|not? test(?:\\S+)? for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no exposure|(?:without|w/o) (?:signs|symptoms)(?:or (?:signs|symptoms))|do)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} not have)#negated\n",
      "(?i)(?:(?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria|no concern (?:for|of)|not? (?:at )risk)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:not|n't) have a (?:positive )?diagnosis|do not meet criteria))#negated\n",
      "(?i)(?:(?:no suspicion(?: for)|not suspect|ruled out for|no(?: recent) travel|not be in|clear(?:ed|s|ing) (?:of|for|from))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:not(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:be ruled out|be not likely|not have contact with))#negated\n",
      "(?i)(?:(?:no (?:hx|history) (?:of )travel|not have contact with|no symptoms of|no risk factors|no (?:confirm case|report))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:no (?:exposure|contact) (?:to|with)|not test(?:\\S+)? positive))#negated\n",
      "(?i)(?:(?:no (?:exposure|contact) (?:to|with)|do (?:not|n't) meet(?: screening)(?: criteria)(?: for)|not test(?:\\S+)? positive(?: for)|not tested(?: or diagnosis))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:no|any)(?: known) contact(?: with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} : no)#negated\n",
      "(?i)(?:(?:(?:not|never) diagnosis with|not been tested (?:for )?or diagnosis with)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} confirm)#positive\n",
      "(?i)(?:(?:confirm|known)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#positive\n",
      "(?i)(?:(?:(?:test(?:\\S+)?)?positive(?: for)?|notif(?:y|ied) of positive (?:results?|test(?:\\S+)?|status))(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positiv(?:e|ity)|test(?:\\S+)? positive|(?:test|pcr) remains positive|notif(?:y|ied) of positive (?:results?|test(?:ing)?|status)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:positive status|results be positive))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} results positive)#positive\n",
      "(?i)(?:results positive(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#positive\n",
      "(?i)(?:notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status)(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? notif(?:y|ied) (?:the )? (?:veteran|patient|family) of positive (?:results?|test(?:ing)?|status))#positive\n",
      "(?i)(?:likely secondary to(?: (?!<IGNORE>)\\S+){0,0} COVID-19)#positive\n",
      "(?i)(?:(?:problem(?: list)? (?:of|:)|(?:active|current|acute) problems :|admi(?:t|ssion) diagnosis(?: :)?)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#positive\n",
      "(?i)(?:(?:reason for admission :|treatment of|(?:admitting )diagnosis(?: :)?)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,3} diagnosis like_num)#positive\n",
      "(?i)(?:(?:Reason for admission :|inpatient with|discharged from|in m?icu (?:for|with))(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#admission\n",
      "(?i)(?:(?:admit(?:ted|s|ting) (?:like_num|with|for)|admitted (?:to|on)|Reason for ICU :|admission for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#admission\n",
      "(?i)(?:Reason for ED visit or Hospital Admission :(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#admission\n",
      "(?i)(?:(?:(?:in|to) (?:the )(?:hospital|icu|micu) (?:for|due to)|hospitali(?:zed)?(?: timesx)? (?:for|due to))(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#admission\n",
      "(?i)(?:(?:diagnosis with|found to be positive for)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,5} found to be positive)#positive\n",
      "(?i)(?:(?:positive test|presum(?:e|ed|es|ing) positive|not(?: yet)? recover(?:s|ing|ed)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positive test|presum(?:e|ed|es|ing) positive))#positive\n",
      "(?i)(?:(?:management of|ards(?: (?:from|with|secondary to))?|acute respiratory distress|post - extubation)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#positive\n",
      "(?i)(?:(?:in(?: the)? setting of|in the s / o|found to have|present(?:s|ed|ing)? with)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#positive\n",
      "(?i)(?:resp(?:iratory) failure(?:(?: (?:with|due to))?|like_num|\\( like_num \\))(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#positive\n",
      "(?i)(?:(?:active(?: for)|recovering from)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} recovering from)#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} (?:detected|value : detected|POSITIVEH))#positive\n",
      "(?i)(?:(?:\\d+(?: )?-|like_num )year(?:(?: )?-(?: )?old| old) (?:(?:aa|white|black|hispanic|caucasian) )?(?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:with|w|w/|admitted)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:(?:like_num (?:y[or]|y / o)|[\\d]+yo) (?:\\b(?!family\\b|other_experiencer\\b)\\S+\\b )?(?:patient |veteran )?(?:with|w|w/)(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#patient_experiencer\n",
      "(?i)(?:the (?:veteran|vet|patient) have(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#patient_experiencer\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} precaution)#future\n",
      "(?i)(?:(?:(?:precaution|protection|protect) (?:for|against)|concern about|reports of|vaccine|protect yourself|prevent(?:ed|ion|s|ing)|avoid)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:prevent(?:ed|ion|s|ing)|vaccine|educat\\S*|instruction))#future\n",
      "(?i)(?:(?:questions (?:about|regarding|re|concerning|on|for)|(?:anxiety|ask(?:ing|ed|es|ed)?) about|educat(?:ion|ed|ing|ed)?|instruction)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:information(?: )?(?:on|about|regarding|re)?|protocols?)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} protocols?)#future\n",
      "(?i)(?:(?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?|(?:anxious|worr(?:ied|ies|y|ying)) (?:about|re|regarding))(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:materials|fact(?: )?sheet|literature|(?:informat(?:ion|ed|ing) )?handouts?))#future\n",
      "(?i)(?:if(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#future\n",
      "(?i)(?:(?:advisor(?:y|ies)|travel screen(?: :)?|Travel History Questionnaire|prescreen|front gate)(?: (?!<IGNORE>)\\S+)*? COVID-19)#screening\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:questionnaire :|questionn?aire|question\\S*|prescreen|front gate))#screening\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,9} screen\\S*)#screening\n",
      "(?i)(?:screen\\S*(?: (?!<IGNORE>)\\S+){0,9} COVID-19)#screening\n",
      "(?i)(?:have you(?: (?!<IGNORE>)\\S+)*? COVID-19)#not relevant\n",
      "(?i)(?:(?:mers)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:This patient was screened for the following suspected travel related illness(?:es)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? This patient was screened for the following suspected travel related illness(?:es)?)#future\n",
      "(?i)(?:(?:will(?: be) travel|travel plans|if you need|plan to travel)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:(?:limit|reduce|lower|minimize)(?: the)? (?:risk|chance|possibility) of|if you)(?: (?!<IGNORE>)\\S+)*? COVID-19)#future\n",
      "(?i)(?:(?:(?:(?:-)?hx|history|) of)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#negated\n",
      "(?i)(?:(?:^(?:check|test|retest|eval)(?: for)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:(?:work(?:-|\\s)up)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:(?:evaluation)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:evaluation))#test\n",
      "(?i)(?:(?:swab|PCR|specimen sent)(?: (?!<IGNORE>)\\S+)*? COVID-19)#test\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:swab|PCR|specimen sent))#test\n",
      "(?i)(?:(?:awaiting results|at risk for|risk for|currently being ruled out or has tested positive for|to exclude)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:awaiting results|currently being ruled out or has tested positive for|(?:patient|person) of interest))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:risk))#uncertain\n",
      "(?i)(?:(?:investigation of)(?: (?!<IGNORE>)\\S+){0,0} COVID-19)#uncertain\n",
      "(?i)(?:(?:question of|differential diagnosis :|ddx :)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:awaiting|questionnaire|r(?:/)?o(?:\\.)?)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:awaiting|questionnaire|r(?:/)?o(?:\\.)?))#uncertain\n",
      "(?i)(?:(?:under investigation|(?:may|might) be positive(?: for)?|flew|tarvel(?:ed)?|travelled)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:under investigation|(?:may|might) be positive))#uncertain\n",
      "(?i)(?:(?:facility (?:with|has)(?: a)?|known to have|(?:same )?room|patients with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:area|county|community|city) (?:with|of)|in the building|(?:several|multiple|one)(?:of )?(?:the )? other_experiencer)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:in the building))#negated\n",
      "(?i)(?:(?:(?:he|she) thinks (?:he|she) (?:have|had|has)|\\S+ would like)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|unlikely to be)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:positive (?:screen|criteria|triage)|(?:^test )?pending|screen positive|possible positive))#uncertain\n",
      "(?i)(?:(?:(?:possible|potential)? exposure|possibly|possible positive)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:risk of|likely|probable|probably)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:suspicion(?: for)?|^suspect|differential diagnosis|ddx(?: :)?|doubt)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:suspicion|^suspect|differential diagnosis|ddx(?: :)?|may have been exposed))#uncertain\n",
      "(?i)(?:(?:(?:positive )?(?:sign|symptom) of)(?: (?!<IGNORE>)\\S+){0,3} COVID-19)#uncertain\n",
      "(?i)(?:(?:sx|s/s|rule (?:- )out|be ruled out(?: for)?|^(?:vs\\.?|versus)$)(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} (?:sx|s/s|rule (?:- )out|^(?:vs\\.?|versus)$))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:possible|potential)? exposure|may have been exposed))#uncertain\n",
      "(?i)(?:(?:concern(:?s)?(?: for| of)?|if (?:negative|positive)|c/f|assess(?:ed)? for|concerning for)(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:(?:unlikely(?: to be positive)?|low (?:suspicion|probability|risk (?:for|in|of)))(?: (?!<IGNORE>)\\S+)*? COVID-19)#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:unlikely(?: to be positive)?|low (?:suspicion|probability)|is unlikely))#uncertain\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,2} (?:extremely low))#uncertain\n",
      "(?i)(?:(?:low risk of)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#uncertain\n",
      "(?i)(?:(?:(?:other_experiencer|family) ^test positive(?: for)?|any one|contact with(?: known))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:other_experiencer|family)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:other_experiencer|any one|contact with(?: known)))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:(?:a|an|another) \\S+ tested positive))#negated\n",
      "(?i)(?:(?:had contact|same (?:building|floor)|care for|clean)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:had contact|same (?:building|floor)|care for|clean))#negated\n",
      "(?i)(?:(?:concern(?:ed)? about)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#negated\n",
      "(?i)(?:(?:patient concern (?:for|of)|desire|(?:concerned|prepare) (?:for|about))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:seen in|a (?:positive|confirmed) case of|cases|epidemic|pandemic)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,1} (?:cases|epidemic|pandemic|national emergency|crisis|situation|mandate|\\?))#negated\n",
      "(?i)(?:(?:national emergency|crisis|situation|mandate)(?: (?!<IGNORE>)\\S+){0,1} COVID-19)#negated\n",
      "(?i)(?:(?:seen in(?: the)? setting of)(?: (?!<IGNORE>)\\S+){0,5} COVID-19)#negated\n",
      "(?i)(?:(?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off|goals :)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:^cancel (?:flight|plan|trip|vacation)|supposed to (?:travel|go|visit)|called off))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:in the (?:area|community)|outbreak))#negated\n",
      "(?i)(?:(?:in the (?:area|community)|outbreak)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:news|media|tv|television|broadcast|headline(?:s)?|newspaper(?:s)?|clinic cancellation))#negated\n",
      "(?i)(?:(?:^read about|deploy|(?:come|been) in close contact(?: with)?)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:^read about|deploy|(?:come|been) in close contact(?: with)?|error))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:have you had close contact|web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases))#negated\n",
      "(?i)(?:(?:have you had close contact|the group|session|(?:nurse(?:s)?|rn) notes)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:web(?:\\s)?site|internet|world(?:\\s|-)?wide|countries with cases|error)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:(?:(?:person|patients) with(?: confirmed)?(?: or)?(?: suspected)?|cases of)(?: (?!<IGNORE>)\\S+){0,2} COVID-19)#negated\n",
      "(?i)(?:elective(?: (?!<IGNORE>)\\S+){0,4} COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,4} elective)#negated\n",
      "(?i)(?:(?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?))(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:reschedule|barrier to travel|positive (?:individual(?:s)?|contact(?:s)?|patient(?:s)?)))#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:(?:someone|person) who (?:has|have) tested positive|contact with))#negated\n",
      "(?i)(?:(?:(?:someone|person) who (?:has|have) tested positive|contact with)(?: (?!<IGNORE>)\\S+)*? COVID-19)#negated\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+){0,0} (?:\\(resolved\\)))#positive\n",
      "(?i)(?:COVID-19(?: (?!<IGNORE>)\\S+)*? (?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :))#IGNORE\n",
      "(?i)(?:(?:social worker|initially negative|likely recovered|not aware|positive (?:case|symptom|sign)|client history|emergency contact|several positive|special instructions :)(?: (?!<IGNORE>)\\S+)*? COVID-19)#IGNORE"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat context_rules.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": 55,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 56,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'ContextMatches(CovidAttribute, Span, Path, Sent)':\n",
<<<<<<< HEAD
      "  CovidAttribute  |   Span   |    Path     |                        Sent\n",
      "------------------+----------+-------------+----------------------------------------------------\n",
      "     positive     | [0, 35)  | sample1.txt |       COVID-19 results came back positive .\n",
      "     positive     | [27, 48) | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "     negated      | [4, 48)  | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "     negated      | [0, 12)  | sample4.txt |              neg COVID-19 education .\n",
      "      future      | [4, 22)  | sample4.txt |              neg COVID-19 education .\n",
      "     positive     | [0, 17)  | sample5.txt |           positive COVID-19 precaution .\n",
      "      future      | [9, 28)  | sample5.txt |           positive COVID-19 precaution .\n",
=======
      "   CovidAttribute    |   Span   |    Path     |                        Sent\n",
      "---------------------+----------+-------------+----------------------------------------------------\n",
      "      positive       | [0, 17)  | sample5.txt |           positive COVID-19 precaution .\n",
      "       negated       | [0, 12)  | sample4.txt |              neg COVID-19 education .\n",
      "      positive       | [27, 48) | sample1.txt | His family recently tested positive for COVID-19 .\n",
      "      positive       | [0, 35)  | sample1.txt |       COVID-19 results came back positive .\n",
      " patient_experiencer | [0, 34)  | sample6.txt |        The patient have reported COVID-19 .\n",
      "       future        | [9, 28)  | sample5.txt |           positive COVID-19 precaution .\n",
      "       future        | [4, 22)  | sample4.txt |              neg COVID-19 education .\n",
      "       negated       | [4, 48)  | sample1.txt | His family recently tested positive for COVID-19 .\n",
>>>>>>> defea06 (fixing comments)
      "\n",
      "printing results for query 'CovidSpans(Path, Span, Sent)':\n",
      "    Path     |   Span   |                        Sent\n",
      "-------------+----------+----------------------------------------------------\n",
      " sample1.txt |  [0, 8)  |       COVID-19 results came back positive .\n",
      " sample1.txt | [40, 48) | His family recently tested positive for COVID-19 .\n",
      " sample1.txt | [34, 42) |    patient presents to be tested for COVID-19 .\n",
      " sample2.txt | [26, 34) |        The patient be tested for COVID-19 .\n",
      " sample3.txt |  [0, 8)  |                 COVID-19 like_num\n",
      " sample4.txt | [4, 12)  |              neg COVID-19 education .\n",
      " sample5.txt | [9, 17)  |           positive COVID-19 precaution .\n",
<<<<<<< HEAD
=======
      " sample6.txt | [26, 34) |        The patient have reported COVID-19 .\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "#covid_attributes: negated, other_experiencer, is_future, not_relevant, uncertain, positive\n",
    "ContextMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?ContextMatches(CovidAttribute, Span, Path, Sent)\n",
    "\n",
    "CovidSpans(Path, Span, Sent) <- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -> (Span)\n",
    "?CovidSpans(Path, Span, Sent)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
=======
   "execution_count": 57,
>>>>>>> defea06 (fixing comments)
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
<<<<<<< HEAD
      "    Path     |  CovidSpan  |  CovidAttribute  |                        Sent\n",
      "-------------+-------------+------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |     positive     |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |     negated      | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |     positive     | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |     positive     |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |      future      |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |     negated      |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |      future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |     positive     |           positive COVID-19 precaution .\n",
=======
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='postprocess-rules'></a>\n",
    "### [Postprocessor](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/postprocess_rules.py):\n",
    "These rules assign an additional attribute for each COVID-19 mention based on its attributes or the context of the sentences they are part of. This way, we can be flexible and fix problems with the data and make specific improvements. For example, they can be handy for spotting and correcting wrongly tagged positive cases, making our classification more accurate.\n",
    "\n",
    "\n",
    "Example rule in the original project:\n",
    "\n",
    "PostprocessingRule(\n",
    "        patterns=[\n",
    "        \n",
    "            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=({\"deny\", \"denies\", \"denied\"},),\n",
    "            ),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=({\"contact\", \"contacts\", \"confirmed\"},),\n",
    "            ),\n",
    "        ],\n",
    "        action=postprocessing_functions.remove_ent,\n",
    "        description=\"Remove a coronavirus entity if 'denies' and 'contact' are in. This will help get rid of false positives from screening.\",\n",
    "    ),    \n",
    "\n",
    "This rule iterates through each entity and checks a series of conditions which are the \"PostprocessingPattern\". If all conditions evaluate as True, then some action is taken on the entity, which is 'remove' action in this example. Some other actions could include changing attributes.\n",
    "\n",
    "\n",
    "In our case, we assign \"IGNORE\" attribute to the COVID-19 mention causing it to be excluded from consideration during the document classification process.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, Attribute\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
=======
   "execution_count": 58,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*education.*#IGNORE\n",
      ".* \\?#IGNORE\n",
      "(?=.*\\b(?:deny|denies|denied)\\b)(?=.*\\b(?:contact|confirm)\\b).*#IGNORE\n",
      "(?=.*\\b(?:setting of|s/o)\\b)(?!.*\\b(?:COVID-19 infection|COVID-19 ards)\\b).*#no_positive\n",
      "(?i)(.*benign.*)#uncertain\n",
      "admitted to COVID-19 unit#positive"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat postprocess_pattern_rules.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": 59,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 60,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PostprocessMatches(CovidAttribute, Span, Path, Sent)':\n",
      "  CovidAttribute  |  Span   |    Path     |           Sent\n",
      "------------------+---------+-------------+--------------------------\n",
      "      IGNORE      | [0, 24) | sample4.txt | neg COVID-19 education .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "PostprocessMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?PostprocessMatches(CovidAttribute, Span, Path, Sent)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
=======
   "execution_count": 61,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
<<<<<<< HEAD
      "    Path     |  CovidSpan  |  CovidAttribute  |                        Sent\n",
      "-------------+-------------+------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |     positive     |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |     negated      | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |     positive     | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |     positive     |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |      IGNORE      |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |      future      |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |     negated      |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |      future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |     positive     |           positive COVID-19 precaution .\n",
=======
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess rules with attributes example:\n",
    "\n",
    "PostprocessingRule(\n",
    "        patterns=[\n",
    "        \n",
    "            PostprocessingPattern(lambda ent: ent.label_ == \"COVID-19\"),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.is_modified_by_category,\n",
    "                condition_args=(\"DEFINITE_POSITIVE_EXISTENCE\",),\n",
    "            ),\n",
    "            # PostprocessingPattern(postprocessing_functions.is_modified_by_category, condition_args=(\"TEST\",)),\n",
    "            PostprocessingPattern(\n",
    "                postprocessing_functions.sentence_contains,\n",
    "                condition_args=(\n",
    "                    {\n",
    "                        \"should\",\n",
    "                        \"unless\",\n",
    "                        \"either\",\n",
    "                        \"if comes back\",\n",
    "                        \"if returns\",\n",
    "                        \"if s?he tests positive\",\n",
    "                    },\n",
    "                    True,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        action=set_is_uncertain,\n",
    "        action_args=(True,),\n",
    "        description=\"Subjunctive of test returning positive. 'Will contact patient should his covid-19 test return positive.'\",\n",
    "    ),\n",
    "\n",
    "This rule examines whether a COVID-19 mention possesses a positive attribute and if the sentence containing it includes any of the words specified in 'condition_args' If these conditions are met, the uncertain attribute is set to true.\n",
    "\n",
    "\n",
    "In our case, we check for each COVID-19 mention in the 'CovidAttributes' table if it's labeled as 'positive', also, we check if any of the specified words in 'condition_args' are present in the same sentence using a regex search. If the conditions are met, then we simply assign it an 'uncertain' attribute.\n",
    "\n",
    "Each rule in the CSV file follows this format: regexPattern, ExistingAttribute, NewAttribute\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
=======
   "execution_count": 62,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*pending.*#negated#no_negated\n",
      ".*(?:should|unless|either|if comes back|if returns|if s?he tests positive).*#positive#uncertain\n",
      ".*precaution.*#positive#no_future\n",
      ".*(?:re[ -]?test|second test|repeat).*#negated#no_negated\n",
      ".*(?:sign|symptom|s/s).*#positive#uncertain"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat postprocess_attributes_rules.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
=======
   "execution_count": 63,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "session.import_relation_from_csv(\"postprocess_attributes_rules.csv\", relation_name=\"PostprocessRulesWithAttributes\", delimiter=\"#\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
=======
   "execution_count": 64,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)':\n",
      "  CovidAttribute  |  NewAttribute  |  Span   |    Path     |              Sent\n",
      "------------------+----------------+---------+-------------+--------------------------------\n",
      "     positive     |   no_future    | [0, 30) | sample5.txt | positive COVID-19 precaution .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRulesWithAttributes(Pattern, CovidAttribute, NewAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "?PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 65,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, NewAttribute, Sent)':\n",
<<<<<<< HEAD
      "    Path     |  CovidSpan  |  NewAttribute  |                        Sent\n",
      "-------------+-------------+----------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |    positive    |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |    negated     | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |    positive    | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |    positive    |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |     IGNORE     |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |     future     |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |    negated     |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |     future     |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |   no_future    |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |    positive    |           positive COVID-19 precaution .\n",
=======
      "    Path     |  CovidSpan  |    NewAttribute     |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "CovidAttributes(Path, CovidSpan, NewAttribute, Sent) <- CovidAttributes(Path, CovidSpan, CovidAttribute, Sent), PostprocessWithAttributesMatches(CovidAttribute, NewAttribute, Span, Path, Sent)\n",
    "?CovidAttributes(Path, CovidSpan, NewAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess rules with next_sentence:\n",
    "\n",
    "There's a rule that checks if the following sentence contains positive mentions. If it does, the COVID-19 mentions in the current sentence are also\n",
    "marked as positive. To Implement this rule in our project, we defined a new relation that pairs each sentence with its subsequent sentence.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
=======
   "execution_count": 66,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sent(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = list(doc.sents)\n",
    "    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n",
    "        yield(sentences[i].text, sentences[i + 1].text)\n",
    "\n",
    "magic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
=======
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: patient presents to be tested for COVID-19 . next sentence: His family recently tested positive for COVID-19 .\n",
      "sentence: His family recently tested positive for COVID-19 . next sentence: COVID-19 results came back positive .\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "for first_sent, second_sent in next_sent(\"sample1.txt\"):\n",
    "    print(f\"sentence: {first_sent}\", f\"next sentence: {second_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'NextSent(Path, Sent1, Sent2)':\n",
      "    Path     |                       Sent1                        |                       Sent2\n",
      "-------------+----------------------------------------------------+----------------------------------------------------\n",
      " sample1.txt | His family recently tested positive for COVID-19 . |       COVID-19 results came back positive .\n",
      " sample1.txt |    patient presents to be tested for COVID-19 .    | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |        The patient be tested for COVID-19 .        |               Results be positive .\n",
      " sample3.txt |          associated_diagnosis like_num .           |                 COVID-19 like_num\n",
      " sample3.txt |             problem_list : like_num .              |          associated_diagnosis like_num .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "NextSent(Path, Sent1, Sent2) <- FilesPaths(Path), next_sent(Path) -> (Sent1, Sent2)\n",
    "?NextSent(Path, Sent1, Sent2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
=======
   "execution_count": 69,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)':\n",
<<<<<<< HEAD
      "    Path     |  CovidSpan  |  CovidAttribute  |                        Sent\n",
      "-------------+-------------+------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |     positive     |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |     negated      | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |     positive     | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |  [26, 34)   |     positive     |        The patient be tested for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |     positive     |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |      IGNORE      |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |      future      |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |     negated      |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |      future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |    no_future     |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |     positive     |           positive COVID-19 precaution .\n",
=======
      "    Path     |  CovidSpan  |   CovidAttribute    |                        Sent\n",
      "-------------+-------------+---------------------+----------------------------------------------------\n",
      " sample1.txt |   [0, 8)    |      positive       |       COVID-19 results came back positive .\n",
      " sample1.txt |  [40, 48)   |       negated       | His family recently tested positive for COVID-19 .\n",
      " sample1.txt |  [40, 48)   |      positive       | His family recently tested positive for COVID-19 .\n",
      " sample2.txt |  [26, 34)   |      positive       |        The patient be tested for COVID-19 .\n",
      " sample3.txt |   [0, 8)    |      positive       |                 COVID-19 like_num\n",
      " sample4.txt |   [4, 12)   |       IGNORE        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       future        |              neg COVID-19 education .\n",
      " sample4.txt |   [4, 12)   |       negated       |              neg COVID-19 education .\n",
      " sample5.txt |   [9, 17)   |       future        |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      no_future      |           positive COVID-19 precaution .\n",
      " sample5.txt |   [9, 17)   |      positive       |           positive COVID-19 precaution .\n",
      " sample6.txt |  [26, 34)   | patient_experiencer |        The patient have reported COVID-19 .\n",
>>>>>>> defea06 (fixing comments)
      "\n"
     ]
    }
   ],
   "source": [
    "%%rgxlog\n",
    "new PostProcessWithNextSentenceRules(str, str)\n",
    "PostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\n",
    "PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) <- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n",
    "?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='document_classifier'></a>\n",
    "### [Document Classifier](https://github.com/abchapman93/VA_COVID-19_NLP_BSV/blob/master/cov_bsv/knowledge_base/document_classifier.py):\n",
    "Now we have the basic pieces in place to make our document classification. Each document is classified as either 'POS', 'UNK', or 'NEG' determined by the attributes of its COVID-19 mentions. The Results are stored in a DataFrame.\n",
    "\n",
    "Document Classifier stage has 2 parts:\n",
    " 1) **Attribute filtering**: Our pipeline assigns various attributes to each COVID-19 mention. However, during this stage, each COVID-19 case is refined to possess only one attribute. This filtering process operates based on specific conditions outlined in the 'attribute_filter' function.\n",
    " 2) **Document classification**: Documents are classified based on distinct conditions, as detailed in the 'classify_doc_helper' function. This step ensures the accurate categorization of each document according to the specified criteria.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
=======
   "execution_count": 70,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_filter(group):\n",
    "    \"\"\"\n",
    "    Filters attributes within each \"CovidSpan\" of a DataFrame table based on specific conditions.\n",
    "\n",
    "    Parameters:\n",
    "        group (pandas.Series): A pandas Series representing attributes for each \"CovidSpan\" within a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        str: Filtered \"CovidSpan\" attribute determined by the following rules:\n",
    "            - If 'IGNORE' is present, returns 'IGNORE'.\n",
    "            - If 'negated' is present (and 'no_negated' is not present), returns 'negated'.\n",
    "            - If 'future' is present (and 'no_future' is not present), returns 'negated'.\n",
    "            - If 'other experiencer' or 'not relevant' is present, returns 'negated'.\n",
    "            - If 'positive' is present (and 'uncertain' and 'no_positive' are not present), returns 'positive'.\n",
    "            - Otherwise, returns 'uncertain'.\n",
    "    \"\"\"\n",
    "    if 'IGNORE' in group.values:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group.values and not 'no_negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'future' in group.values and not 'no_future' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "          Path Attribute\n",
      "0  sample1.txt    IGNORE\n",
      "1  sample1.txt   negated\n",
      "2  sample1.txt  positive\n",
      "3  sample2.txt  positive\n",
      "\n",
      "After:\n",
      "          Path Attribute\n",
      "0  sample1.txt    IGNORE\n",
      "1  sample2.txt  positive\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "data = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n",
    "        'Attribute': ['IGNORE', 'negated', 'positive', 'positive']}\n",
    "df_example = pd.DataFrame(data)\n",
    "print(\"Before:\")\n",
    "print(df_example)\n",
    "\n",
    "df_example['Attribute'] = df_example.groupby(['Path'])['Attribute'].transform(attribute_filter)\n",
    "df_example = df_example.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nAfter:\")\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>CovidSpan</th>\n",
       "      <th>CovidAttribute</th>\n",
       "      <th>Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[0, 8)</td>\n",
       "      <td>positive</td>\n",
       "      <td>COVID-19 results came back positive .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>[40, 48)</td>\n",
       "      <td>negated</td>\n",
       "      <td>His family recently tested positive for COVID-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>[26, 34)</td>\n",
       "      <td>positive</td>\n",
       "      <td>The patient be tested for COVID-19 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>[0, 8)</td>\n",
       "      <td>positive</td>\n",
       "      <td>COVID-19 like_num</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>[4, 12)</td>\n",
       "      <td>IGNORE</td>\n",
       "      <td>neg COVID-19 education .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>[9, 17)</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive COVID-19 precaution .</td>\n",
       "    </tr>\n",
<<<<<<< HEAD
=======
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sample6.txt</td>\n",
       "      <td>[26, 34)</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>The patient have reported COVID-19 .</td>\n",
       "    </tr>\n",
>>>>>>> defea06 (fixing comments)
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path CovidSpan CovidAttribute  \\\n",
       "0  sample1.txt    [0, 8)       positive   \n",
       "1  sample1.txt  [40, 48)        negated   \n",
       "2  sample2.txt  [26, 34)       positive   \n",
       "3  sample3.txt    [0, 8)       positive   \n",
       "4  sample4.txt   [4, 12)         IGNORE   \n",
       "5  sample5.txt   [9, 17)       positive   \n",
<<<<<<< HEAD
=======
       "6  sample6.txt  [26, 34)      uncertain   \n",
>>>>>>> defea06 (fixing comments)
       "\n",
       "                                                Sent  \n",
       "0              COVID-19 results came back positive .  \n",
       "1  His family recently tested positive for COVID-...  \n",
       "2               The patient be tested for COVID-19 .  \n",
       "3                                  COVID-19 like_num  \n",
       "4                           neg COVID-19 education .  \n",
<<<<<<< HEAD
       "5                     positive COVID-19 precaution .  "
      ]
     },
     "execution_count": 58,
=======
       "5                     positive COVID-19 precaution .  \n",
       "6               The patient have reported COVID-19 .  "
      ]
     },
     "execution_count": 72,
>>>>>>> defea06 (fixing comments)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
=======
   "execution_count": 73,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_doc_helper(group):\n",
    "    \"\"\"\n",
    "Classifies a document as 'POS', 'UNK', or 'NEG' based on COVID-19 attributes.\n",
    "\n",
    "Parameters:\n",
    "    group (pandas.Series): A pandas Series representing COVID-19 attributes for each document within a DataFrame.\n",
    "    \n",
    "Returns:\n",
    "    str: Document classification determined as follows:\n",
    "         - 'POS': If at least one COVID-19 attribute with \"positive\" is present in the group.\n",
    "         - 'UNK': If at least one COVID-19 attribute with \"uncertain\" is present in the group and no \"positive\" attributes,\n",
    "                  or there's at least one COVID-19 attribute with 'IGNORE' and no other COVID-19 attributes exist.\n",
    "         - 'NEG': Otherwise.\n",
    "\"\"\"\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group.values:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "          Path  Attribute\n",
      "0  sample1.txt  uncertain\n",
      "1  sample1.txt    negated\n",
      "2  sample1.txt   positive\n",
      "3  sample2.txt   positive\n",
      "\n",
      "After:\n",
      "          Path DocResult\n",
      "0  sample1.txt       POS\n",
      "1  sample2.txt       POS\n"
     ]
    }
   ],
   "source": [
    "# usage example\n",
    "data = {'Path': [\"sample1.txt\", \"sample1.txt\", \"sample1.txt\", \"sample2.txt\"],\n",
    "        'Attribute': ['uncertain', 'negated', 'positive', 'positive']}\n",
    "df_example = pd.DataFrame(data)\n",
    "print(\"Before:\")\n",
    "print(df_example)\n",
    "\n",
    "df_example['DocResult'] = df_example.groupby(['Path'])['Attribute'].transform(classify_doc_helper)\n",
    "df_example = df_example[['Path', 'DocResult']]\n",
    "df_example = df_example.drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nAfter:\")\n",
    "print(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       POS\n",
       "2  sample3.txt       POS\n",
       "3  sample4.txt       UNK\n",
       "4  sample5.txt       POS"
      ]
     },
<<<<<<< HEAD
     "execution_count": 60,
=======
     "execution_count": 75,
>>>>>>> defea06 (fixing comments)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 76,
>>>>>>> defea06 (fixing comments)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>DocResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample1.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample2.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample3.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample4.txt</td>\n",
       "      <td>UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample5.txt</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Path DocResult\n",
       "0  sample1.txt       POS\n",
       "1  sample2.txt       POS\n",
       "2  sample3.txt       POS\n",
       "3  sample4.txt       UNK\n",
       "4  sample5.txt       POS"
      ]
     },
<<<<<<< HEAD
     "execution_count": 61,
=======
     "execution_count": 76,
>>>>>>> defea06 (fixing comments)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = (session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": []
=======
   "source": [
    "<a id='bringing-all-together'></a>\n",
    "### Bringing It All Together\n",
    "\n",
    "In this section, we will directly compare the original Python Spacy pipeline project with its rgxlog counterpart. Our emphasis is on showcasing the overall brevity of the rgxlog implementation in contrast to the Python Spacy pipeline.\n",
    "\n",
    "#### Code Metrics\n",
    "\n",
    "Let's commence by providing an estimated count of total lines in each implementation:\n",
    "\n",
    "- **Total Number of Lines in the original Python implementation:** **4435**\n",
    "- **Total Number of Lines in our rgxlog implementation:** **596** (7 times smaller!)<br>\n",
    "\n",
    "And here's a detailed comparison:\n",
    "\n",
    "![Screenshot%20%2825%29.png](attachment:Screenshot%20%2825%29.png)\n",
    "\n",
    "Now, we will present the combined rgxlog and python code (excluding \"generic ie\" functions and excluding queries) to visually illustrate the compactness of the implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept tagger:\n",
    "```python\n",
    "def lemmatize_text(text_path, lemma_words_path):\n",
    "    # Define a list of words to be lemmatized\n",
    "    lemma_words = [line.strip() for line in open(f\"{lemma_words_path}\") if line.strip()]\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    lemmatized_text = \"\"\n",
    "    for token in doc:\n",
    "        if token.lemma_ in lemma_words:\n",
    "            lemmatized_text += token.lemma_\n",
    "        elif token.like_num:\n",
    "            lemmatized_text += \"like_num\"\n",
    "        else:\n",
    "            lemmatized_text += token.text\n",
    "        lemmatized_text += \" \"\n",
    "\n",
    "    # Write the lemmatized text back to the same file\n",
    "    with open(text_path, 'w') as file:\n",
    "        file.writelines(lemmatized_text)\n",
    "\n",
    "    yield lemmatized_text\n",
    "\n",
    "def annotate_text_with_pos(text_path):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"PRON\", \"ADJ\"]:\n",
    "            yield token.pos_, Span(token.idx, token.idx + len(token.text))\n",
    "        else:\n",
    "            yield tuple()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "session.import_relation_from_csv(\"concept_tags_rules.csv\", relation_name=\"ConceptTagRules\", delimiter=\",\")\n",
    "\n",
    "%%rgxlog\n",
    "LemmaMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"lemma\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "replace_spans(\"LemmaMatches\", \"FilesPaths\")\n",
    "POSTable(POS, Span, Path) <- FilesContent(Path, Content), annotate_text_with_pos(Path) -> (POS, Span)\n",
    "POSMatches(Label, Span, Path) <- FilesContent(Path, Content), ConceptTagRules(Pattern, Label, \"pos\"), py_rgx_span(Content, Pattern) -> (Span)\n",
    "POSRuleMatches(Label, Span, Path) <- POSTable(POS, Span, Path), POSMatches(Label, Span, Path)\n",
    "replace_spans(\"POSRuleMatches\", \"FilesPaths\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target matcher:\n",
    "```python\n",
    "magic_session.import_relation_from_csv(\"target_rules.csv\", relation_name=\"TargetTagRules\", delimiter=\",\")\n",
    "\n",
    "%%rgxlog\n",
    "TargetTagMatches(Label, Span, Path) <- FilesContent(Path, Content), TargetTagRules(Pattern, Label), py_rgx_span(Content,Pattern) -> (Span)\n",
    "replace_spans(\"TargetTagMatches\", \"FilesPaths\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sectionizer:\n",
    "```python\n",
    "\n",
    "magic_session.import_relation_from_csv(\"section_rules.csv\", relation_name=\"SectionRules\", delimiter=\",\")\n",
    "\n",
    "%%rgxlog\n",
    "SectionRulesMatches(Label, Span, Path) <- FilesContent(Path, Content), SectionRules(Pattern, Label), py_rgx_span(Content,Pattern) -> (Span)\n",
    "replace_spans(\"SectionRulesMatches\", \"FilesPaths\")\n",
    "\n",
    "pattern = \"(?i)(?:diagnoses :|observation_and_plan :|past_medical_history :|problem_list :)(?:(?!labs :|addendum :|allergies :|chief_complaint :|comments :|family_history :|hospital_course :|imaging :|labs_and_studies :|medications :|neurological :|other :|patient_education :|physical_exam :|reason_for_examination :|signature :|social_history :).)*\"\n",
    "\n",
    "new SectionRulesAttribute(str, str)\n",
    "SectionRulesAttribute(pattern, \"positive\")\n",
    "SectionMatches(Path, Span, CovidAttribute) <- FilesContent(Path, Content), SectionRulesAttribute(Pattern, CovidAttribute), py_rgx_span(Content, Pattern) -> (Span)\n",
    "CovidMatches(Path, Span) <- FilesContent(Path, Content), py_rgx_span(Content, \"COVID-19\") -> (Span)\n",
    "SectionCovidAttributes(Path, CovidSpan, CovidAttribute) <- SectionMatches(Path, Span1, CovidAttribute), CovidMatches(Path, Span2), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "\n",
    "Sents(Path, Sent) <- FilesPaths(Path), sent_tokenization(Path) -> (Sent)\n",
    "SentSpans(Path, Sent, SentSpan) <- FilesContent(Path, Content), Sents(Path, Sent), py_rgx_span(Content, Sent) -> (SentSpan)\n",
    "\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- SectionCovidAttributes(Path, AbsCovidSpan, CovidAttribute),\\\n",
    "SentSpans(Path, Sent, SentSpan) ,get_relative_span(AbsCovidSpan, SentSpan) -> (CovidSpan)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context matcher:\n",
    "```python\n",
    "magic_session.import_relation_from_csv(\"context_rules.csv\", relation_name=\"ContextRules\", delimiter=\"#\")\n",
    "\n",
    "%%rgxlog\n",
    "ContextMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), ContextRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidSpans(Path, Span, Sent) <- Sents(Path, Sent), py_rgx_span(Sent, \"COVID-19\") -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- ContextMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessor:\n",
    "```python\n",
    "def next_sent(text_path):\n",
    "    with open(text_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(contents)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = list(doc.sents)\n",
    "    for i in range(len(sentences) - 1):  # Iterate until the second-to-last sentence\n",
    "        yield(sentences[i].text, sentences[i + 1].text)\n",
    "\n",
    "magic_session.register(ie_function=next_sent, ie_function_name = \"next_sent\", in_rel=[DataTypes.string], out_rel=[DataTypes.string,DataTypes.string])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "magic_session.import_relation_from_csv(\"postprocess_pattern_rules.csv\", relation_name=\"PostprocessRules\", delimiter=\"#\")\n",
    "\n",
    "%%rgxlog\n",
    "PostprocessMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostprocessRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent) <- PostprocessMatches(CovidAttribute, Span1, Path, Sent), CovidSpans(Path, Span2, Sent), is_span_contained(Span1, Span2) -> (CovidSpan)\n",
    "\n",
    "NextSent(Path, Sent1, Sent2) <- FilesPaths(Path), next_sent(Path) -> (Sent1, Sent2)\n",
    "new PostProcessWithNextSentenceRules(str, str)\n",
    "PostProcessWithNextSentenceRules(\"(?i)(?:^(?:positive|detected)|results?(?: be)? positive)\", \"positive\")\n",
    "PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent) <- Sents(Path, Sent), PostProcessWithNextSentenceRules(Pattern, CovidAttribute),\\\n",
    "py_rgx_span(Sent, Pattern) -> (Span)\n",
    "CovidAttributes(Path, CovidSpan, CovidAttribute, Sent1) <- CovidSpans(Path, CovidSpan, Sent1), NextSent(Path, Sent1, Sent2), PostProcessWithNextSentenceMatches(CovidAttribute, Span, Path, Sent2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Classifier:\n",
    "```python\n",
    "def attribute_filter(group):\n",
    "    if 'IGNORE' in group.values:\n",
    "        return 'IGNORE'\n",
    "    elif 'negated' in group.values and not 'no_negated' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'future' in group.values and not 'no_future' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'other experiencer' in group.values or 'not relevant' in group.values:\n",
    "        return 'negated'\n",
    "    elif 'positive' in group.values and not 'uncertain' in group.values and not 'no_positive' in group.values:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'uncertain'\n",
    "\n",
    "df = (magic_session.run_commands(\"?CovidAttributes(Path, CovidSpan, CovidAttribute, Sent)\", print_results=False, format_results=True))[0]\n",
    "if len(df) == 0:\n",
    "    df = DataFrame(columns=[\"Path\",\"CovidSpan\",\"CovidAttribute\"])\n",
    "df['CovidAttribute'] = df.groupby(['CovidSpan', 'Sent'])['CovidAttribute'].transform(attribute_filter)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def classify_doc_helper(group):\n",
    "    if 'positive' in group.values:\n",
    "        return 'POS'\n",
    "    elif 'uncertain' in group.values:\n",
    "        return 'UNK'\n",
    "    elif 'negated' in group.values:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'UNK'\n",
    "        \n",
    "df['DocResult'] = df.groupby('Path')['CovidAttribute'].transform(classify_doc_helper)\n",
    "df = df[['Path', 'DocResult']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_path = (magic_session.run_commands(\"?FilesPaths(Path)\", print_results=False, format_results=True))[0]\n",
    "df = (pd.merge(df, df_path, on='Path', how='outer'))\n",
    "df['DocResult'] = df['DocResult'].fillna(\"UNK\")\n",
    "df\n",
    "```"
   ]
>>>>>>> defea06 (fixing comments)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
